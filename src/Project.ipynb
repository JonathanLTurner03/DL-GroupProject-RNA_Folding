{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l37bn3-3nP5S"
      },
      "source": [
        "CS 4277: Deep Learning Group Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoeTYxZjnP5U"
      },
      "source": [
        "## CS 4277: *Deep Learning* Group Project\n",
        "### Members:\n",
        "- Nicholas Hodge\n",
        "- Joshua Peeples\n",
        "- Jonathan Turner\n",
        "\n",
        "### This project is our attempt at the Stanford RNA 3D Folding Challenge, found at:\n",
        "\n",
        "https://www.kaggle.com/competitions/stanford-rna-3d-folding\n",
        "\n",
        "**For this project to run:**\n",
        "\n",
        "1. Install matplotlib in your Jupyter Kernel: Block [1]\n",
        "2. Setup correct path files to your train dataset: Block [7] (there is a comment)\n",
        "\n",
        "**Future work:**\n",
        "\n",
        "1. Setup validation correctly\n",
        "2. Test\n",
        "3. Return Submission.csv as per requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BGEUeleSnP5U"
      },
      "outputs": [],
      "source": [
        "# Uncomment and run if matplotlib not installed\n",
        "# !  python -m pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Z_qewxm9nP5V"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# import matplotlib\n",
        "# import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "198NcZ8bnP5V"
      },
      "outputs": [],
      "source": [
        "NUC_TO_IDX = {\n",
        "    \"A\": 0,\n",
        "    \"U\": 1,\n",
        "    \"C\": 2,\n",
        "    \"G\": 3,\n",
        "    \"N\": 4 # There are characters *other* than the above 4 sometimes. 'N' is standard for \"unknown\" (apparently)\n",
        "}\n",
        "PAD_IDX = 5\n",
        "VOCAB_SIZE = len(NUC_TO_IDX) + 1\n",
        "\n",
        "# Annotated to avoid later confusion - Nick\n",
        "# This is inherits the Dataset class from pytorch to allow for the dataset to be an Iterable (i.e. work a LOT faster)\n",
        "class RNADataset(Dataset):\n",
        "    def __init__(self, seq_csv_path, coords_csv_path):\n",
        "        # Read both train CSVs (the 'labels' csv -> 'coords')\n",
        "        self.sequences_df = pd.read_csv(seq_csv_path)\n",
        "        coords_df_raw = pd.read_csv(coords_csv_path)\n",
        "\n",
        "        # We are going to get the base_id from each row in coords to associate them with the correct sequence.\n",
        "        # 1SCL_A_5 becomes 1SCL_A\n",
        "        coords_df_raw[\"base_id\"] = coords_df_raw[\"ID\"].apply(lambda x: \"_\".join(x.split(\"_\")[:2]))\n",
        "\n",
        "        # Now we are going to create groups of coords, where each group corresponds with the same sequence\n",
        "        # Unfortunately some sequences have missing coord values, but I am going to assume that there are potential\n",
        "        # sequences that have some missing and some not. So:\n",
        "\n",
        "        # Method to remove entire groups where *any* row has missing coords\n",
        "        def is_group_valid(group):\n",
        "            return group[[\"x_1\", \"y_1\", \"z_1\"]].notna().all().all() # returns only rows where all columns are good\n",
        "\n",
        "        valid_groups = [\n",
        "            group for _, group in coords_df_raw.groupby(\"base_id\") if is_group_valid(group)\n",
        "        ]\n",
        "\n",
        "        # Concatenate all valid groups into a new coords_df\n",
        "        self.coords_df = pd.concat(valid_groups, ignore_index=True)\n",
        "\n",
        "        # Build groups and valid sequence IDs list\n",
        "        self.coord_groups = self.coords_df.groupby(\"base_id\")\n",
        "        self.valid_ids = set(self.coord_groups.groups.keys())\n",
        "\n",
        "        # Filter sequences to only include those with clean coordinate groups (prevents later tensors from being mishaped)\n",
        "        self.sequences_df = self.sequences_df[self.sequences_df[\"target_id\"].isin(self.valid_ids)]\n",
        "\n",
        "    # Optional but Pytorch docs suggest this for 'Sampler' implmentations (might need that?)\n",
        "    def __len__(self):\n",
        "        return len(self.sequences_df)\n",
        "\n",
        "    # Optional but Pytorch docs suggest this for speedup batched samples loading\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.sequences_df.iloc[idx]\n",
        "        seq_id = row[\"target_id\"]\n",
        "        sequence = row[\"sequence\"]\n",
        "\n",
        "        token_ids = [NUC_TO_IDX.get(nuc, NUC_TO_IDX[\"N\"]) for nuc in sequence]\n",
        "        token_ids = torch.tensor(token_ids, dtype=torch.long)\n",
        "\n",
        "        # Here we introduce standardization to the coordinates\n",
        "\n",
        "        # TODO: calculate the following values somewhere in the document in case the dataset changes:\n",
        "        # Currently precalculated values\n",
        "        mean_x = 80.44731529117061\n",
        "        std_x = 147.42231938515297\n",
        "        mean_y = 84.04072703411182\n",
        "        std_y = 114.92890150429712\n",
        "        mean_z = 98.61122565112208\n",
        "        std_z = 119.41066506340083\n",
        "\n",
        "        coords_standardized = self.coord_groups.get_group(seq_id)[[\"x_1\", \"y_1\", \"z_1\"]].values\n",
        "        coords_standardized[:, 0] = (coords_standardized[:, 0] - mean_x) / std_x\n",
        "        coords_standardized[:, 1] = (coords_standardized[:, 1] - mean_y) / std_y\n",
        "        coords_standardized[:, 2] = (coords_standardized[:, 2] - mean_z) / std_z\n",
        "\n",
        "        coords = torch.tensor(coords_standardized, dtype=torch.float32)\n",
        "\n",
        "        return token_ids, coords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ac683bn5nP5W"
      },
      "outputs": [],
      "source": [
        "# Pad sequences in collate_fn\n",
        "def collate_fn(batch):\n",
        "    sequences, coords = zip(*batch)\n",
        "\n",
        "    # Pad sequences with PAD_IDX\n",
        "    seq_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=PAD_IDX)\n",
        "    coord_padded = torch.nn.utils.rnn.pad_sequence(coords, batch_first=True, padding_value=0.0)\n",
        "\n",
        "    # Mask should check against PAD_IDX\n",
        "    mask = (seq_padded != PAD_IDX).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    return seq_padded, coord_padded, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iFKtgnu9nP5W"
      },
      "outputs": [],
      "source": [
        "# Source: Aladdin Persson on YouTube (then modified to have an encoder-only architecture)\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads, dropout):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        assert (self.head_dim * heads == embed_size), \"Embed size needs to be divisible by heads\"\n",
        "\n",
        "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, values, keys, query, mask):\n",
        "        N, value_len, _ = values.shape\n",
        "        _, key_len, _ = keys.shape\n",
        "        _, query_len, _ = query.shape\n",
        "\n",
        "        # Split embedding into self.heads pieces\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "        values = self.values(values)\n",
        "        keys = self.keys(keys)\n",
        "        queries = self.queries(queries)\n",
        "\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
        "        # queries shape: (N, query_len, heads, heads_dim)\n",
        "        # keys shape: (N, key_len, heads, heads_dim)\n",
        "        # energy shape: (N, heads, query_len, key_len)\n",
        "\n",
        "        if mask is not None:\n",
        "            # mask: (batch, 1, 1, seq_len) -> broadcastable to (batch, heads, query_len, key_len)\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e9\"))\n",
        "\n",
        "        attention = torch.softmax(energy / (self.embed_size ** 0.5), dim=3)\n",
        "        attention = self.dropout(attention)\n",
        "\n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
        "            N, query_len, self.heads * self.head_dim\n",
        "        )\n",
        "        # attention shape: (N, heads, query_len, key_len)\n",
        "        # values shape: (N, value_len, heads, heads_dim)\n",
        "        # after einsum (N, query_len, heads, head_dim) then flatten last two dimensions\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        return out\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = SelfAttention(embed_size, heads, dropout)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attention = self.attention(x, x, x, mask)\n",
        "\n",
        "        x = self.dropout(self.norm1(attention + x))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm2(forward + x))\n",
        "        return out\n",
        "\n",
        "class RNA3DFoldPredictor(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 embed_size,\n",
        "                 num_layers,\n",
        "                 heads,\n",
        "                 forward_expansion,\n",
        "                 dropout,\n",
        "                 max_length):\n",
        "        super().__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.fc_out = nn.Linear(embed_size, 3)  # Predict (x, y, z)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        N, seq_len = x.shape\n",
        "\n",
        "        positions = torch.arange(0, seq_len).unsqueeze(0).expand(N, seq_len).to(x.device)\n",
        "\n",
        "        out = self.token_embedding(x) + self.position_embedding(positions)\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, mask)\n",
        "\n",
        "        coords = self.fc_out(out)\n",
        "        return coords\n",
        "\n",
        "    def predict_multiple(self, x, n_samples=5):\n",
        "        self.train()  # Activate dropout during inference\n",
        "        with torch.no_grad():\n",
        "            outputs = [self(x) for _ in range(n_samples)]\n",
        "        return torch.stack(outputs)  # Shape: (n_samples, batch_size, seq_len, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ujvnX5UknP5W",
        "outputId": "2641e091-4a6d-4a1e-8543-93ae4b932e58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print (device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KC9qjEnrnP5X",
        "outputId": "bff6bf07-5464-4b40-ff2f-37e495518d2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max token ID: tensor(5, device='cuda:0')\n",
            "Embedding size: 6\n",
            "Output shape: torch.Size([4, 83, 3])\n",
            "Batch 1 Loss: 1.566165\n",
            "Batch 2 Loss: 1.290069\n",
            "Batch 3 Loss: 1.052013\n",
            "Batch 4 Loss: 1.720773\n",
            "Batch 5 Loss: 0.938640\n",
            "Batch 6 Loss: 2.446501\n",
            "Batch 7 Loss: 0.952919\n",
            "Batch 8 Loss: 1.117447\n",
            "Batch 9 Loss: 0.887389\n",
            "Batch 10 Loss: 1.058887\n",
            "Batch 11 Loss: 0.812275\n",
            "Batch 12 Loss: 1.206711\n",
            "Batch 13 Loss: 1.071779\n",
            "Batch 14 Loss: 0.884889\n",
            "Batch 15 Loss: 0.714097\n",
            "Batch 16 Loss: 1.022797\n",
            "Batch 17 Loss: 0.647281\n",
            "Batch 18 Loss: 0.751234\n",
            "Batch 19 Loss: 1.065351\n",
            "Batch 20 Loss: 1.141871\n",
            "Batch 21 Loss: 0.685920\n",
            "Batch 22 Loss: 0.795845\n",
            "Batch 23 Loss: 0.883020\n",
            "Batch 24 Loss: 0.921442\n",
            "Batch 25 Loss: 0.752520\n",
            "Batch 26 Loss: 0.817975\n",
            "Batch 27 Loss: 1.390630\n",
            "Batch 28 Loss: 0.882561\n",
            "Batch 29 Loss: 1.038605\n",
            "Batch 30 Loss: 0.905442\n",
            "Batch 31 Loss: 0.769019\n",
            "Batch 32 Loss: 1.445550\n",
            "Batch 33 Loss: 1.103188\n",
            "Batch 34 Loss: 1.154603\n",
            "Batch 35 Loss: 1.811391\n",
            "Batch 36 Loss: 1.692756\n",
            "Batch 37 Loss: 0.934070\n",
            "Batch 38 Loss: 6.222395\n",
            "Batch 39 Loss: 0.890561\n",
            "Batch 40 Loss: 0.759583\n",
            "Batch 41 Loss: 1.012269\n",
            "Batch 42 Loss: 0.681415\n",
            "Batch 43 Loss: 1.058124\n",
            "Batch 44 Loss: 1.371932\n",
            "Batch 45 Loss: 0.666101\n",
            "Batch 46 Loss: 0.907600\n",
            "Batch 47 Loss: 0.787986\n",
            "Batch 48 Loss: 1.619979\n",
            "Batch 49 Loss: 0.892334\n",
            "Batch 50 Loss: 0.793907\n",
            "Batch 51 Loss: 0.794726\n",
            "Batch 52 Loss: 0.822682\n",
            "Batch 53 Loss: 0.827487\n",
            "Batch 54 Loss: 0.729152\n",
            "Batch 55 Loss: 1.813161\n",
            "Batch 56 Loss: 0.644877\n",
            "Batch 57 Loss: 0.818921\n",
            "Batch 58 Loss: 1.054454\n",
            "Batch 59 Loss: 1.026981\n",
            "Batch 60 Loss: 1.322836\n",
            "Batch 61 Loss: 1.046162\n",
            "Batch 62 Loss: 1.146752\n",
            "Batch 63 Loss: 0.670376\n",
            "Batch 64 Loss: 1.577085\n",
            "Batch 65 Loss: 2.111937\n",
            "Batch 66 Loss: 0.991400\n",
            "Batch 67 Loss: 0.586029\n",
            "Batch 68 Loss: 0.956932\n",
            "Batch 69 Loss: 0.468508\n",
            "Batch 70 Loss: 0.679010\n",
            "Batch 71 Loss: 1.100284\n",
            "Batch 72 Loss: 0.608218\n",
            "Batch 73 Loss: 0.756855\n",
            "Batch 74 Loss: 1.367512\n",
            "Batch 75 Loss: 1.782314\n",
            "Batch 76 Loss: 0.996086\n",
            "Batch 77 Loss: 0.838650\n",
            "Batch 78 Loss: 0.605357\n",
            "Batch 79 Loss: 1.454154\n",
            "Batch 80 Loss: 0.876061\n",
            "Batch 81 Loss: 0.732440\n",
            "Batch 82 Loss: 1.658720\n",
            "Batch 83 Loss: 0.630848\n",
            "Batch 84 Loss: 0.738647\n",
            "Batch 85 Loss: 0.725887\n",
            "Batch 86 Loss: 0.731757\n",
            "Batch 87 Loss: 2.257036\n",
            "Batch 88 Loss: 1.775633\n",
            "Batch 89 Loss: 0.943060\n",
            "Batch 90 Loss: 0.931959\n",
            "Batch 91 Loss: 0.628069\n",
            "Batch 92 Loss: 1.392184\n",
            "Batch 93 Loss: 1.290322\n",
            "Batch 94 Loss: 0.753947\n",
            "Batch 95 Loss: 1.342960\n",
            "Batch 96 Loss: 2.412056\n",
            "Batch 97 Loss: 1.153888\n",
            "Batch 98 Loss: 2.944202\n",
            "Batch 99 Loss: 1.452747\n",
            "Batch 100 Loss: 0.653758\n",
            "Batch 101 Loss: 0.937926\n",
            "Batch 102 Loss: 0.556484\n",
            "Batch 103 Loss: 0.851653\n",
            "Batch 104 Loss: 1.117125\n",
            "Batch 105 Loss: 1.706470\n",
            "Batch 106 Loss: 0.938159\n",
            "Batch 107 Loss: 15.215324\n",
            "Batch 108 Loss: 1.172212\n",
            "Batch 109 Loss: 1.141328\n",
            "Batch 110 Loss: 0.868874\n",
            "Batch 111 Loss: 0.599145\n",
            "Batch 112 Loss: 1.709463\n",
            "Batch 113 Loss: 0.864749\n",
            "Batch 114 Loss: 0.660019\n",
            "Batch 115 Loss: 0.651536\n",
            "Batch 116 Loss: 0.907315\n",
            "Batch 117 Loss: 0.752787\n",
            "Batch 118 Loss: 0.674154\n",
            "Batch 119 Loss: 0.709603\n",
            "Batch 120 Loss: 0.912383\n",
            "Batch 121 Loss: 0.714348\n",
            "Batch 122 Loss: 0.696165\n",
            "Batch 123 Loss: 1.372509\n",
            "Batch 124 Loss: 1.768351\n",
            "Batch 125 Loss: 1.131443\n",
            "Batch 126 Loss: 0.530777\n",
            "Batch 127 Loss: 1.607899\n",
            "Batch 128 Loss: 1.082074\n",
            "Batch 129 Loss: 0.791850\n",
            "Batch 130 Loss: 0.766663\n",
            "Batch 131 Loss: 0.904033\n",
            "Batch 132 Loss: 0.917905\n",
            "Batch 133 Loss: 0.722741\n",
            "Batch 134 Loss: 0.846602\n",
            "Batch 135 Loss: 0.872630\n",
            "Batch 136 Loss: 0.696360\n",
            "Batch 137 Loss: 0.864202\n",
            "Batch 138 Loss: 1.430603\n",
            "Batch 139 Loss: 0.536865\n",
            "Batch 140 Loss: 0.676880\n",
            "Batch 141 Loss: 1.210266\n",
            "Batch 142 Loss: 0.789970\n",
            "Batch 143 Loss: 1.327566\n",
            "Batch 144 Loss: 0.770485\n",
            "Batch 145 Loss: 0.773287\n",
            "Batch 146 Loss: 1.031134\n",
            "Batch 147 Loss: 0.887884\n",
            "Batch 148 Loss: 0.757806\n",
            "Batch 149 Loss: 0.889550\n",
            "Batch 150 Loss: 0.740335\n",
            "Batch 151 Loss: 2.363744\n",
            "Batch 152 Loss: 0.806637\n",
            "Epoch 1 Loss: 1.1701\n",
            "Max token ID: tensor(5, device='cuda:0')\n",
            "Embedding size: 6\n",
            "Output shape: torch.Size([4, 363, 3])\n",
            "Batch 1 Loss: 1.150433\n",
            "Batch 2 Loss: 1.391546\n",
            "Batch 3 Loss: 0.618127\n",
            "Batch 4 Loss: 0.997331\n",
            "Batch 5 Loss: 0.729777\n",
            "Batch 6 Loss: 0.426019\n",
            "Batch 7 Loss: 1.082423\n",
            "Batch 8 Loss: 1.014470\n",
            "Batch 9 Loss: 0.689847\n",
            "Batch 10 Loss: 0.846221\n",
            "Batch 11 Loss: 0.654630\n",
            "Batch 12 Loss: 2.062942\n",
            "Batch 13 Loss: 0.637512\n",
            "Batch 14 Loss: 1.775495\n",
            "Batch 15 Loss: 0.636045\n",
            "Batch 16 Loss: 0.634680\n",
            "Batch 17 Loss: 0.751858\n",
            "Batch 18 Loss: 0.847766\n",
            "Batch 19 Loss: 0.619397\n",
            "Batch 20 Loss: 0.451695\n",
            "Batch 21 Loss: 0.854215\n",
            "Batch 22 Loss: 0.745638\n",
            "Batch 23 Loss: 0.679690\n",
            "Batch 24 Loss: 0.976277\n",
            "Batch 25 Loss: 0.662605\n",
            "Batch 26 Loss: 0.699719\n",
            "Batch 27 Loss: 0.638942\n",
            "Batch 28 Loss: 1.546349\n",
            "Batch 29 Loss: 1.226900\n",
            "Batch 30 Loss: 2.332476\n",
            "Batch 31 Loss: 0.562054\n",
            "Batch 32 Loss: 0.701616\n",
            "Batch 33 Loss: 0.725069\n",
            "Batch 34 Loss: 1.363895\n",
            "Batch 35 Loss: 0.687853\n",
            "Batch 36 Loss: 0.641672\n",
            "Batch 37 Loss: 2.445929\n",
            "Batch 38 Loss: 1.749834\n",
            "Batch 39 Loss: 14.126242\n",
            "Batch 40 Loss: 0.609773\n",
            "Batch 41 Loss: 1.111722\n",
            "Batch 42 Loss: 0.546421\n",
            "Batch 43 Loss: 0.740236\n",
            "Batch 44 Loss: 0.609750\n",
            "Batch 45 Loss: 0.718408\n",
            "Batch 46 Loss: 0.708932\n",
            "Batch 47 Loss: 1.806366\n",
            "Batch 48 Loss: 0.565090\n",
            "Batch 49 Loss: 0.604695\n",
            "Batch 50 Loss: 0.662066\n",
            "Batch 51 Loss: 1.839755\n",
            "Batch 52 Loss: 0.909826\n",
            "Batch 53 Loss: 0.799920\n",
            "Batch 54 Loss: 0.721024\n",
            "Batch 55 Loss: 1.003997\n",
            "Batch 56 Loss: 0.792537\n",
            "Batch 57 Loss: 0.719038\n",
            "Batch 58 Loss: 0.489716\n",
            "Batch 59 Loss: 2.213825\n",
            "Batch 60 Loss: 0.585899\n",
            "Batch 61 Loss: 0.642884\n",
            "Batch 62 Loss: 1.160963\n",
            "Batch 63 Loss: 1.586015\n",
            "Batch 64 Loss: 0.595365\n",
            "Batch 65 Loss: 1.623724\n",
            "Batch 66 Loss: 0.671650\n",
            "Batch 67 Loss: 0.580283\n",
            "Batch 68 Loss: 0.569044\n",
            "Batch 69 Loss: 1.649372\n",
            "Batch 70 Loss: 2.668753\n",
            "Batch 71 Loss: 0.667470\n",
            "Batch 72 Loss: 0.611073\n",
            "Batch 73 Loss: 0.541327\n",
            "Batch 74 Loss: 1.176980\n",
            "Batch 75 Loss: 0.674957\n",
            "Batch 76 Loss: 0.499235\n",
            "Batch 77 Loss: 0.595293\n",
            "Batch 78 Loss: 1.416858\n",
            "Batch 79 Loss: 1.159687\n",
            "Batch 80 Loss: 0.918492\n",
            "Batch 81 Loss: 1.478427\n",
            "Batch 82 Loss: 0.466179\n",
            "Batch 83 Loss: 0.961846\n",
            "Batch 84 Loss: 0.482488\n",
            "Batch 85 Loss: 1.030331\n",
            "Batch 86 Loss: 1.577187\n",
            "Batch 87 Loss: 2.108552\n",
            "Batch 88 Loss: 0.768711\n",
            "Batch 89 Loss: 1.542090\n",
            "Batch 90 Loss: 0.895262\n",
            "Batch 91 Loss: 1.544727\n",
            "Batch 92 Loss: 1.730715\n",
            "Batch 93 Loss: 0.628828\n",
            "Batch 94 Loss: 1.346386\n",
            "Batch 95 Loss: 1.807492\n",
            "Batch 96 Loss: 0.728277\n",
            "Batch 97 Loss: 0.724944\n",
            "Batch 98 Loss: 0.722378\n",
            "Batch 99 Loss: 0.821900\n",
            "Batch 100 Loss: 1.486614\n",
            "Batch 101 Loss: 0.631828\n",
            "Batch 102 Loss: 0.821995\n",
            "Batch 103 Loss: 0.505140\n",
            "Batch 104 Loss: 0.545684\n",
            "Batch 105 Loss: 0.824257\n",
            "Batch 106 Loss: 1.131253\n",
            "Batch 107 Loss: 1.129168\n",
            "Batch 108 Loss: 0.597008\n",
            "Batch 109 Loss: 0.737339\n",
            "Batch 110 Loss: 0.630473\n",
            "Batch 111 Loss: 0.473111\n",
            "Batch 112 Loss: 0.616712\n",
            "Batch 113 Loss: 1.373467\n",
            "Batch 114 Loss: 0.662167\n",
            "Batch 115 Loss: 1.607766\n",
            "Batch 116 Loss: 0.757918\n",
            "Batch 117 Loss: 1.188581\n",
            "Batch 118 Loss: 0.692662\n",
            "Batch 119 Loss: 0.747387\n",
            "Batch 120 Loss: 0.533902\n",
            "Batch 121 Loss: 0.865124\n",
            "Batch 122 Loss: 1.985132\n",
            "Batch 123 Loss: 0.436822\n",
            "Batch 124 Loss: 0.924581\n",
            "Batch 125 Loss: 0.517145\n",
            "Batch 126 Loss: 0.881789\n",
            "Batch 127 Loss: 1.921920\n",
            "Batch 128 Loss: 0.622890\n",
            "Batch 129 Loss: 0.561788\n",
            "Batch 130 Loss: 0.745142\n",
            "Batch 131 Loss: 0.567401\n",
            "Batch 132 Loss: 0.753723\n",
            "Batch 133 Loss: 2.302448\n",
            "Batch 134 Loss: 0.682831\n",
            "Batch 135 Loss: 1.060487\n",
            "Batch 136 Loss: 0.564618\n",
            "Batch 137 Loss: 0.732273\n",
            "Batch 138 Loss: 2.501758\n",
            "Batch 139 Loss: 1.729347\n",
            "Batch 140 Loss: 1.561569\n",
            "Batch 141 Loss: 0.895849\n",
            "Batch 142 Loss: 0.517643\n",
            "Batch 143 Loss: 3.831396\n",
            "Batch 144 Loss: 1.111101\n",
            "Batch 145 Loss: 0.851959\n",
            "Batch 146 Loss: 0.467700\n",
            "Batch 147 Loss: 0.875428\n",
            "Batch 148 Loss: 1.549198\n",
            "Batch 149 Loss: 0.937717\n",
            "Batch 150 Loss: 0.509081\n",
            "Batch 151 Loss: 0.564048\n",
            "Batch 152 Loss: 1.634227\n",
            "Epoch 2 Loss: 1.0847\n",
            "Max token ID: tensor(5, device='cuda:0')\n",
            "Embedding size: 6\n",
            "Output shape: torch.Size([4, 363, 3])\n",
            "Batch 1 Loss: 0.713237\n",
            "Batch 2 Loss: 0.703534\n",
            "Batch 3 Loss: 0.564512\n",
            "Batch 4 Loss: 0.607619\n",
            "Batch 5 Loss: 1.149000\n",
            "Batch 6 Loss: 0.551397\n",
            "Batch 7 Loss: 1.688608\n",
            "Batch 8 Loss: 0.589503\n",
            "Batch 9 Loss: 0.975923\n",
            "Batch 10 Loss: 1.288322\n",
            "Batch 11 Loss: 1.766706\n",
            "Batch 12 Loss: 1.484319\n",
            "Batch 13 Loss: 1.047638\n",
            "Batch 14 Loss: 1.501421\n",
            "Batch 15 Loss: 1.007491\n",
            "Batch 16 Loss: 0.508828\n",
            "Batch 17 Loss: 0.618594\n",
            "Batch 18 Loss: 0.876973\n",
            "Batch 19 Loss: 0.826218\n",
            "Batch 20 Loss: 0.553674\n",
            "Batch 21 Loss: 2.697750\n",
            "Batch 22 Loss: 1.165403\n",
            "Batch 23 Loss: 1.583352\n",
            "Batch 24 Loss: 0.558842\n",
            "Batch 25 Loss: 0.457469\n",
            "Batch 26 Loss: 0.611552\n",
            "Batch 27 Loss: 0.771716\n",
            "Batch 28 Loss: 0.945158\n",
            "Batch 29 Loss: 0.585102\n",
            "Batch 30 Loss: 1.575267\n",
            "Batch 31 Loss: 2.140813\n",
            "Batch 32 Loss: 1.497268\n",
            "Batch 33 Loss: 0.532112\n",
            "Batch 34 Loss: 0.543578\n",
            "Batch 35 Loss: 0.571023\n",
            "Batch 36 Loss: 0.455813\n",
            "Batch 37 Loss: 0.656670\n",
            "Batch 38 Loss: 0.542850\n",
            "Batch 39 Loss: 0.879519\n",
            "Batch 40 Loss: 0.465118\n",
            "Batch 41 Loss: 0.932356\n",
            "Batch 42 Loss: 0.673867\n",
            "Batch 43 Loss: 2.532959\n",
            "Batch 44 Loss: 0.526510\n",
            "Batch 45 Loss: 0.822532\n",
            "Batch 46 Loss: 1.701250\n",
            "Batch 47 Loss: 0.708021\n",
            "Batch 48 Loss: 0.747199\n",
            "Batch 49 Loss: 0.635626\n",
            "Batch 50 Loss: 1.308170\n",
            "Batch 51 Loss: 0.803995\n",
            "Batch 52 Loss: 0.972797\n",
            "Batch 53 Loss: 0.933832\n",
            "Batch 54 Loss: 0.986563\n",
            "Batch 55 Loss: 0.834131\n",
            "Batch 56 Loss: 1.187422\n",
            "Batch 57 Loss: 1.467470\n",
            "Batch 58 Loss: 0.477819\n",
            "Batch 59 Loss: 0.483140\n",
            "Batch 60 Loss: 1.189950\n",
            "Batch 61 Loss: 0.595732\n",
            "Batch 62 Loss: 0.558973\n",
            "Batch 63 Loss: 6.482706\n",
            "Batch 64 Loss: 0.839711\n",
            "Batch 65 Loss: 1.417455\n",
            "Batch 66 Loss: 0.875588\n",
            "Batch 67 Loss: 1.227368\n",
            "Batch 68 Loss: 0.778967\n",
            "Batch 69 Loss: 0.578596\n",
            "Batch 70 Loss: 0.700317\n",
            "Batch 71 Loss: 0.398543\n",
            "Batch 72 Loss: 0.425047\n",
            "Batch 73 Loss: 1.465821\n",
            "Batch 74 Loss: 0.721372\n",
            "Batch 75 Loss: 0.482890\n",
            "Batch 76 Loss: 0.672348\n",
            "Batch 77 Loss: 0.805443\n",
            "Batch 78 Loss: 1.920516\n",
            "Batch 79 Loss: 0.907098\n",
            "Batch 80 Loss: 1.524967\n",
            "Batch 81 Loss: 0.859141\n",
            "Batch 82 Loss: 0.414501\n",
            "Batch 83 Loss: 0.544557\n",
            "Batch 84 Loss: 0.468262\n",
            "Batch 85 Loss: 0.564957\n",
            "Batch 86 Loss: 1.160292\n",
            "Batch 87 Loss: 1.563723\n",
            "Batch 88 Loss: 0.618968\n",
            "Batch 89 Loss: 0.541988\n",
            "Batch 90 Loss: 0.580503\n",
            "Batch 91 Loss: 0.532775\n",
            "Batch 92 Loss: 2.336632\n",
            "Batch 93 Loss: 0.573687\n",
            "Batch 94 Loss: 0.595918\n",
            "Batch 95 Loss: 2.315206\n",
            "Batch 96 Loss: 0.489159\n",
            "Batch 97 Loss: 1.645177\n",
            "Batch 98 Loss: 0.551878\n",
            "Batch 99 Loss: 1.665047\n",
            "Batch 100 Loss: 0.510731\n",
            "Batch 101 Loss: 0.457984\n",
            "Batch 102 Loss: 0.723501\n",
            "Batch 103 Loss: 1.184706\n",
            "Batch 104 Loss: 0.959815\n",
            "Batch 105 Loss: 1.081115\n",
            "Batch 106 Loss: 0.807647\n",
            "Batch 107 Loss: 0.655329\n",
            "Batch 108 Loss: 0.522060\n",
            "Batch 109 Loss: 0.692065\n",
            "Batch 110 Loss: 0.721641\n",
            "Batch 111 Loss: 0.547453\n",
            "Batch 112 Loss: 1.684753\n",
            "Batch 113 Loss: 0.641113\n",
            "Batch 114 Loss: 0.616080\n",
            "Batch 115 Loss: 0.863449\n",
            "Batch 116 Loss: 1.294773\n",
            "Batch 117 Loss: 0.680836\n",
            "Batch 118 Loss: 1.144149\n",
            "Batch 119 Loss: 0.417061\n",
            "Batch 120 Loss: 0.451167\n",
            "Batch 121 Loss: 0.386066\n",
            "Batch 122 Loss: 0.593658\n",
            "Batch 123 Loss: 0.510750\n",
            "Batch 124 Loss: 0.589918\n",
            "Batch 125 Loss: 0.615867\n",
            "Batch 126 Loss: 0.588455\n",
            "Batch 127 Loss: 1.459550\n",
            "Batch 128 Loss: 0.389556\n",
            "Batch 129 Loss: 0.723930\n",
            "Batch 130 Loss: 2.607329\n",
            "Batch 131 Loss: 1.001377\n",
            "Batch 132 Loss: 0.531135\n",
            "Batch 133 Loss: 0.444066\n",
            "Batch 134 Loss: 0.481414\n",
            "Batch 135 Loss: 1.191025\n",
            "Batch 136 Loss: 14.077140\n",
            "Batch 137 Loss: 0.378381\n",
            "Batch 138 Loss: 0.702256\n",
            "Batch 139 Loss: 0.521975\n",
            "Batch 140 Loss: 0.568709\n",
            "Batch 141 Loss: 0.671171\n",
            "Batch 142 Loss: 0.732857\n",
            "Batch 143 Loss: 0.812083\n",
            "Batch 144 Loss: 1.096832\n",
            "Batch 145 Loss: 0.535517\n",
            "Batch 146 Loss: 0.948111\n",
            "Batch 147 Loss: 0.735127\n",
            "Batch 148 Loss: 0.669835\n",
            "Batch 149 Loss: 1.622740\n",
            "Batch 150 Loss: 1.518815\n",
            "Batch 151 Loss: 1.675503\n",
            "Batch 152 Loss: 0.792970\n",
            "Epoch 3 Loss: 1.0291\n",
            "Max token ID: tensor(5, device='cuda:0')\n",
            "Embedding size: 6\n",
            "Output shape: torch.Size([4, 38, 3])\n",
            "Batch 1 Loss: 2.187524\n",
            "Batch 2 Loss: 0.527901\n",
            "Batch 3 Loss: 0.475002\n",
            "Batch 4 Loss: 1.594993\n",
            "Batch 5 Loss: 2.156082\n",
            "Batch 6 Loss: 0.540762\n",
            "Batch 7 Loss: 0.663608\n",
            "Batch 8 Loss: 0.910994\n",
            "Batch 9 Loss: 1.280048\n",
            "Batch 10 Loss: 0.535024\n",
            "Batch 11 Loss: 0.685799\n",
            "Batch 12 Loss: 0.960214\n",
            "Batch 13 Loss: 0.409072\n",
            "Batch 14 Loss: 0.506020\n",
            "Batch 15 Loss: 0.641410\n",
            "Batch 16 Loss: 1.489487\n",
            "Batch 17 Loss: 0.534480\n",
            "Batch 18 Loss: 0.746494\n",
            "Batch 19 Loss: 0.882620\n",
            "Batch 20 Loss: 0.652122\n",
            "Batch 21 Loss: 1.568582\n",
            "Batch 22 Loss: 0.940960\n",
            "Batch 23 Loss: 0.541588\n",
            "Batch 24 Loss: 1.593599\n",
            "Batch 25 Loss: 0.921537\n",
            "Batch 26 Loss: 0.471154\n",
            "Batch 27 Loss: 1.395812\n",
            "Batch 28 Loss: 0.472469\n",
            "Batch 29 Loss: 1.002197\n",
            "Batch 30 Loss: 0.554365\n",
            "Batch 31 Loss: 0.502267\n",
            "Batch 32 Loss: 1.082106\n",
            "Batch 33 Loss: 0.403708\n",
            "Batch 34 Loss: 0.601121\n",
            "Batch 35 Loss: 1.396203\n",
            "Batch 36 Loss: 1.284759\n",
            "Batch 37 Loss: 0.552176\n",
            "Batch 38 Loss: 0.933245\n",
            "Batch 39 Loss: 1.941232\n",
            "Batch 40 Loss: 0.663625\n",
            "Batch 41 Loss: 0.465238\n",
            "Batch 42 Loss: 2.343667\n",
            "Batch 43 Loss: 0.641612\n",
            "Batch 44 Loss: 0.967140\n",
            "Batch 45 Loss: 2.563534\n",
            "Batch 46 Loss: 1.624157\n",
            "Batch 47 Loss: 1.623206\n",
            "Batch 48 Loss: 0.451079\n",
            "Batch 49 Loss: 0.740517\n",
            "Batch 50 Loss: 0.437353\n",
            "Batch 51 Loss: 0.677465\n",
            "Batch 52 Loss: 0.703620\n",
            "Batch 53 Loss: 1.749274\n",
            "Batch 54 Loss: 0.487625\n",
            "Batch 55 Loss: 0.674316\n",
            "Batch 56 Loss: 0.906459\n",
            "Batch 57 Loss: 0.400577\n",
            "Batch 58 Loss: 3.691002\n",
            "Batch 59 Loss: 1.389337\n",
            "Batch 60 Loss: 0.452388\n",
            "Batch 61 Loss: 0.609892\n",
            "Batch 62 Loss: 0.431069\n",
            "Batch 63 Loss: 0.341339\n",
            "Batch 64 Loss: 0.737860\n",
            "Batch 65 Loss: 0.610732\n",
            "Batch 66 Loss: 0.525653\n",
            "Batch 67 Loss: 1.542735\n",
            "Batch 68 Loss: 0.472769\n",
            "Batch 69 Loss: 0.645102\n",
            "Batch 70 Loss: 0.993280\n",
            "Batch 71 Loss: 0.510655\n",
            "Batch 72 Loss: 0.530764\n",
            "Batch 73 Loss: 0.715842\n",
            "Batch 74 Loss: 0.443827\n",
            "Batch 75 Loss: 14.141770\n",
            "Batch 76 Loss: 0.857622\n",
            "Batch 77 Loss: 0.382827\n",
            "Batch 78 Loss: 0.445632\n",
            "Batch 79 Loss: 1.723131\n",
            "Batch 80 Loss: 0.480052\n",
            "Batch 81 Loss: 0.714073\n",
            "Batch 82 Loss: 0.680202\n",
            "Batch 83 Loss: 3.407864\n",
            "Batch 84 Loss: 1.281567\n",
            "Batch 85 Loss: 0.736871\n",
            "Batch 86 Loss: 0.663343\n",
            "Batch 87 Loss: 0.366197\n",
            "Batch 88 Loss: 0.657166\n",
            "Batch 89 Loss: 1.196733\n",
            "Batch 90 Loss: 1.663101\n",
            "Batch 91 Loss: 0.381547\n",
            "Batch 92 Loss: 0.510184\n",
            "Batch 93 Loss: 0.990715\n",
            "Batch 94 Loss: 0.463263\n",
            "Batch 95 Loss: 1.734542\n",
            "Batch 96 Loss: 0.519263\n",
            "Batch 97 Loss: 0.500367\n",
            "Batch 98 Loss: 0.862923\n",
            "Batch 99 Loss: 2.606160\n",
            "Batch 100 Loss: 0.775891\n",
            "Batch 101 Loss: 1.982438\n",
            "Batch 102 Loss: 0.621107\n",
            "Batch 103 Loss: 0.832141\n",
            "Batch 104 Loss: 0.443380\n",
            "Batch 105 Loss: 0.350047\n",
            "Batch 106 Loss: 0.629023\n",
            "Batch 107 Loss: 1.465218\n",
            "Batch 108 Loss: 0.600990\n",
            "Batch 109 Loss: 0.448195\n",
            "Batch 110 Loss: 0.424986\n",
            "Batch 111 Loss: 1.032956\n",
            "Batch 112 Loss: 1.997757\n",
            "Batch 113 Loss: 0.799531\n",
            "Batch 114 Loss: 0.669139\n",
            "Batch 115 Loss: 0.481513\n",
            "Batch 116 Loss: 0.632215\n",
            "Batch 117 Loss: 0.381795\n",
            "Batch 118 Loss: 0.443394\n",
            "Batch 119 Loss: 1.123053\n",
            "Batch 120 Loss: 1.954157\n",
            "Batch 121 Loss: 1.085521\n",
            "Batch 122 Loss: 2.096778\n",
            "Batch 123 Loss: 0.731999\n",
            "Batch 124 Loss: 0.478949\n",
            "Batch 125 Loss: 0.797889\n",
            "Batch 126 Loss: 0.952215\n",
            "Batch 127 Loss: 2.139719\n",
            "Batch 128 Loss: 0.473581\n",
            "Batch 129 Loss: 0.808808\n",
            "Batch 130 Loss: 0.539902\n",
            "Batch 131 Loss: 0.648633\n",
            "Batch 132 Loss: 0.588377\n",
            "Batch 133 Loss: 0.788995\n",
            "Batch 134 Loss: 0.491903\n",
            "Batch 135 Loss: 0.862890\n",
            "Batch 136 Loss: 1.221653\n",
            "Batch 137 Loss: 0.368677\n",
            "Batch 138 Loss: 0.879114\n",
            "Batch 139 Loss: 0.590371\n",
            "Batch 140 Loss: 0.744055\n",
            "Batch 141 Loss: 0.508101\n",
            "Batch 142 Loss: 0.515969\n",
            "Batch 143 Loss: 0.514868\n",
            "Batch 144 Loss: 0.478746\n",
            "Batch 145 Loss: 2.122194\n",
            "Batch 146 Loss: 0.571368\n",
            "Batch 147 Loss: 0.780881\n",
            "Batch 148 Loss: 0.609929\n",
            "Batch 149 Loss: 3.292393\n",
            "Batch 150 Loss: 1.023803\n",
            "Batch 151 Loss: 2.687903\n",
            "Batch 152 Loss: 0.503815\n",
            "Epoch 4 Loss: 1.0280\n",
            "Max token ID: tensor(5, device='cuda:0')\n",
            "Embedding size: 6\n",
            "Output shape: torch.Size([4, 77, 3])\n",
            "Batch 1 Loss: 0.645290\n",
            "Batch 2 Loss: 0.322144\n",
            "Batch 3 Loss: 0.661691\n",
            "Batch 4 Loss: 1.989308\n",
            "Batch 5 Loss: 0.412417\n",
            "Batch 6 Loss: 0.602563\n",
            "Batch 7 Loss: 1.675865\n",
            "Batch 8 Loss: 0.407823\n",
            "Batch 9 Loss: 1.383931\n",
            "Batch 10 Loss: 1.457067\n",
            "Batch 11 Loss: 0.326106\n",
            "Batch 12 Loss: 0.451521\n",
            "Batch 13 Loss: 0.587204\n",
            "Batch 14 Loss: 0.493343\n",
            "Batch 15 Loss: 0.616820\n",
            "Batch 16 Loss: 0.465060\n",
            "Batch 17 Loss: 0.428224\n",
            "Batch 18 Loss: 2.263674\n",
            "Batch 19 Loss: 0.615080\n",
            "Batch 20 Loss: 0.859367\n",
            "Batch 21 Loss: 1.142356\n",
            "Batch 22 Loss: 0.586726\n",
            "Batch 23 Loss: 1.248043\n",
            "Batch 24 Loss: 0.722262\n",
            "Batch 25 Loss: 0.700425\n",
            "Batch 26 Loss: 0.466611\n",
            "Batch 27 Loss: 0.633196\n",
            "Batch 28 Loss: 0.511473\n",
            "Batch 29 Loss: 0.548934\n",
            "Batch 30 Loss: 1.694372\n",
            "Batch 31 Loss: 0.547406\n",
            "Batch 32 Loss: 0.945514\n",
            "Batch 33 Loss: 0.581296\n",
            "Batch 34 Loss: 0.668798\n",
            "Batch 35 Loss: 0.528091\n",
            "Batch 36 Loss: 1.116022\n",
            "Batch 37 Loss: 1.770908\n",
            "Batch 38 Loss: 0.478033\n",
            "Batch 39 Loss: 0.854266\n",
            "Batch 40 Loss: 3.080107\n",
            "Batch 41 Loss: 1.712174\n",
            "Batch 42 Loss: 1.040920\n",
            "Batch 43 Loss: 0.434703\n",
            "Batch 44 Loss: 1.197406\n",
            "Batch 45 Loss: 1.560437\n",
            "Batch 46 Loss: 0.548079\n",
            "Batch 47 Loss: 0.434160\n",
            "Batch 48 Loss: 0.420456\n",
            "Batch 49 Loss: 0.918387\n",
            "Batch 50 Loss: 2.250452\n",
            "Batch 51 Loss: 1.385589\n",
            "Batch 52 Loss: 0.403679\n",
            "Batch 53 Loss: 0.536310\n",
            "Batch 54 Loss: 0.561217\n",
            "Batch 55 Loss: 0.676320\n",
            "Batch 56 Loss: 0.684796\n",
            "Batch 57 Loss: 0.489822\n",
            "Batch 58 Loss: 0.455143\n",
            "Batch 59 Loss: 1.623157\n",
            "Batch 60 Loss: 1.693356\n",
            "Batch 61 Loss: 0.519989\n",
            "Batch 62 Loss: 0.374802\n",
            "Batch 63 Loss: 0.389159\n",
            "Batch 64 Loss: 1.346831\n",
            "Batch 65 Loss: 0.812541\n",
            "Batch 66 Loss: 0.451197\n",
            "Batch 67 Loss: 0.381073\n",
            "Batch 68 Loss: 1.984286\n",
            "Batch 69 Loss: 0.368579\n",
            "Batch 70 Loss: 1.517351\n",
            "Batch 71 Loss: 0.810472\n",
            "Batch 72 Loss: 6.889083\n",
            "Batch 73 Loss: 1.779577\n",
            "Batch 74 Loss: 1.229749\n",
            "Batch 75 Loss: 2.656873\n",
            "Batch 76 Loss: 0.399862\n",
            "Batch 77 Loss: 0.617761\n",
            "Batch 78 Loss: 0.600994\n",
            "Batch 79 Loss: 0.936829\n",
            "Batch 80 Loss: 0.464540\n",
            "Batch 81 Loss: 0.924301\n",
            "Batch 82 Loss: 1.466535\n",
            "Batch 83 Loss: 0.344162\n",
            "Batch 84 Loss: 0.492272\n",
            "Batch 85 Loss: 2.075869\n",
            "Batch 86 Loss: 0.654644\n",
            "Batch 87 Loss: 0.431567\n",
            "Batch 88 Loss: 0.601493\n",
            "Batch 89 Loss: 0.495419\n",
            "Batch 90 Loss: 0.507229\n",
            "Batch 91 Loss: 13.595536\n",
            "Batch 92 Loss: 0.551466\n",
            "Batch 93 Loss: 1.446510\n",
            "Batch 94 Loss: 1.512930\n",
            "Batch 95 Loss: 0.603290\n",
            "Batch 96 Loss: 0.359845\n",
            "Batch 97 Loss: 1.175295\n",
            "Batch 98 Loss: 0.347705\n",
            "Batch 99 Loss: 2.249389\n",
            "Batch 100 Loss: 0.721624\n",
            "Batch 101 Loss: 1.492839\n",
            "Batch 102 Loss: 0.549168\n",
            "Batch 103 Loss: 0.719978\n",
            "Batch 104 Loss: 1.531711\n",
            "Batch 105 Loss: 0.368952\n",
            "Batch 106 Loss: 0.474991\n",
            "Batch 107 Loss: 1.439446\n",
            "Batch 108 Loss: 0.827120\n",
            "Batch 109 Loss: 0.729411\n",
            "Batch 110 Loss: 1.212681\n",
            "Batch 111 Loss: 0.694183\n",
            "Batch 112 Loss: 0.868244\n",
            "Batch 113 Loss: 2.013221\n",
            "Batch 114 Loss: 1.014138\n",
            "Batch 115 Loss: 0.358300\n",
            "Batch 116 Loss: 1.738672\n",
            "Batch 117 Loss: 0.438108\n",
            "Batch 118 Loss: 0.844887\n",
            "Batch 119 Loss: 0.359575\n",
            "Batch 120 Loss: 0.404349\n",
            "Batch 121 Loss: 1.741751\n",
            "Batch 122 Loss: 0.770189\n",
            "Batch 123 Loss: 0.405322\n",
            "Batch 124 Loss: 0.451547\n",
            "Batch 125 Loss: 0.503894\n",
            "Batch 126 Loss: 1.123731\n",
            "Batch 127 Loss: 0.352285\n",
            "Batch 128 Loss: 1.672220\n",
            "Batch 129 Loss: 1.081872\n",
            "Batch 130 Loss: 1.064509\n",
            "Batch 131 Loss: 0.732819\n",
            "Batch 132 Loss: 0.611389\n",
            "Batch 133 Loss: 2.837610\n",
            "Batch 134 Loss: 0.348701\n",
            "Batch 135 Loss: 1.563552\n",
            "Batch 136 Loss: 0.954649\n",
            "Batch 137 Loss: 0.376645\n",
            "Batch 138 Loss: 0.381977\n",
            "Batch 139 Loss: 0.384008\n",
            "Batch 140 Loss: 0.392420\n",
            "Batch 141 Loss: 0.376851\n",
            "Batch 142 Loss: 0.389071\n",
            "Batch 143 Loss: 0.714462\n",
            "Batch 144 Loss: 1.439193\n",
            "Batch 145 Loss: 1.097537\n",
            "Batch 146 Loss: 0.720880\n",
            "Batch 147 Loss: 0.438110\n",
            "Batch 148 Loss: 0.450827\n",
            "Batch 149 Loss: 1.156309\n",
            "Batch 150 Loss: 2.114595\n",
            "Batch 151 Loss: 0.552297\n",
            "Batch 152 Loss: 0.510627\n",
            "Epoch 5 Loss: 1.0223\n",
            "Max token ID: tensor(5, device='cuda:0')\n",
            "Embedding size: 6\n",
            "Output shape: torch.Size([4, 55, 3])\n",
            "Batch 1 Loss: 0.448724\n",
            "Batch 2 Loss: 0.466278\n",
            "Batch 3 Loss: 0.404155\n",
            "Batch 4 Loss: 0.818712\n",
            "Batch 5 Loss: 1.158263\n",
            "Batch 6 Loss: 0.628894\n",
            "Batch 7 Loss: 0.309683\n",
            "Batch 8 Loss: 0.472360\n",
            "Batch 9 Loss: 0.757348\n",
            "Batch 10 Loss: 0.386560\n",
            "Batch 11 Loss: 0.875525\n",
            "Batch 12 Loss: 0.595149\n",
            "Batch 13 Loss: 0.604455\n",
            "Batch 14 Loss: 0.390442\n",
            "Batch 15 Loss: 0.346233\n",
            "Batch 16 Loss: 0.345139\n",
            "Batch 17 Loss: 0.755861\n",
            "Batch 18 Loss: 2.369869\n",
            "Batch 19 Loss: 0.355378\n",
            "Batch 20 Loss: 1.439321\n",
            "Batch 21 Loss: 0.677435\n",
            "Batch 22 Loss: 0.527623\n",
            "Batch 23 Loss: 0.488863\n",
            "Batch 24 Loss: 0.355460\n",
            "Batch 25 Loss: 0.470576\n",
            "Batch 26 Loss: 0.441026\n",
            "Batch 27 Loss: 0.367854\n",
            "Batch 28 Loss: 0.350290\n",
            "Batch 29 Loss: 1.384111\n",
            "Batch 30 Loss: 0.405835\n",
            "Batch 31 Loss: 0.397678\n",
            "Batch 32 Loss: 2.740412\n",
            "Batch 33 Loss: 0.815428\n",
            "Batch 34 Loss: 1.396802\n",
            "Batch 35 Loss: 1.790809\n",
            "Batch 36 Loss: 0.366808\n",
            "Batch 37 Loss: 1.253187\n",
            "Batch 38 Loss: 0.333127\n",
            "Batch 39 Loss: 0.520525\n",
            "Batch 40 Loss: 0.689507\n",
            "Batch 41 Loss: 0.433883\n",
            "Batch 42 Loss: 2.193586\n",
            "Batch 43 Loss: 0.604383\n",
            "Batch 44 Loss: 5.977759\n",
            "Batch 45 Loss: 1.269756\n",
            "Batch 46 Loss: 0.526931\n",
            "Batch 47 Loss: 0.413809\n",
            "Batch 48 Loss: 0.673281\n",
            "Batch 49 Loss: 0.622086\n",
            "Batch 50 Loss: 0.358920\n",
            "Batch 51 Loss: 1.258108\n",
            "Batch 52 Loss: 1.536933\n",
            "Batch 53 Loss: 0.315278\n",
            "Batch 54 Loss: 0.606365\n",
            "Batch 55 Loss: 1.422175\n",
            "Batch 56 Loss: 0.806341\n",
            "Batch 57 Loss: 0.445026\n",
            "Batch 58 Loss: 0.703613\n",
            "Batch 59 Loss: 0.355637\n",
            "Batch 60 Loss: 1.856969\n",
            "Batch 61 Loss: 3.706144\n",
            "Batch 62 Loss: 3.385655\n",
            "Batch 63 Loss: 1.273005\n",
            "Batch 64 Loss: 1.210963\n",
            "Batch 65 Loss: 1.613959\n",
            "Batch 66 Loss: 0.347725\n",
            "Batch 67 Loss: 0.861987\n",
            "Batch 68 Loss: 1.747329\n",
            "Batch 69 Loss: 0.652344\n",
            "Batch 70 Loss: 0.386992\n",
            "Batch 71 Loss: 0.571641\n",
            "Batch 72 Loss: 0.403577\n",
            "Batch 73 Loss: 1.252847\n",
            "Batch 74 Loss: 0.502220\n",
            "Batch 75 Loss: 0.349224\n",
            "Batch 76 Loss: 0.393684\n",
            "Batch 77 Loss: 0.403431\n",
            "Batch 78 Loss: 0.974891\n",
            "Batch 79 Loss: 0.408403\n",
            "Batch 80 Loss: 0.460908\n",
            "Batch 81 Loss: 0.607889\n",
            "Batch 82 Loss: 3.516016\n",
            "Batch 83 Loss: 0.726802\n",
            "Batch 84 Loss: 0.432821\n",
            "Batch 85 Loss: 0.807485\n",
            "Batch 86 Loss: 0.659261\n",
            "Batch 87 Loss: 1.160718\n",
            "Batch 88 Loss: 0.449798\n",
            "Batch 89 Loss: 1.523786\n",
            "Batch 90 Loss: 1.997996\n",
            "Batch 91 Loss: 0.818064\n",
            "Batch 92 Loss: 0.561954\n",
            "Batch 93 Loss: 0.968551\n",
            "Batch 94 Loss: 0.453301\n",
            "Batch 95 Loss: 0.282498\n",
            "Batch 96 Loss: 0.416128\n",
            "Batch 97 Loss: 1.972538\n",
            "Batch 98 Loss: 0.822188\n",
            "Batch 99 Loss: 0.842898\n",
            "Batch 100 Loss: 0.420400\n",
            "Batch 101 Loss: 2.463225\n",
            "Batch 102 Loss: 0.328947\n",
            "Batch 103 Loss: 2.085135\n",
            "Batch 104 Loss: 0.636358\n",
            "Batch 105 Loss: 0.745785\n",
            "Batch 106 Loss: 1.775184\n",
            "Batch 107 Loss: 0.461464\n",
            "Batch 108 Loss: 1.714638\n",
            "Batch 109 Loss: 0.415128\n",
            "Batch 110 Loss: 0.654119\n",
            "Batch 111 Loss: 0.677471\n",
            "Batch 112 Loss: 1.188063\n",
            "Batch 113 Loss: 0.520452\n",
            "Batch 114 Loss: 0.325487\n",
            "Batch 115 Loss: 0.740516\n",
            "Batch 116 Loss: 1.331982\n",
            "Batch 117 Loss: 2.126843\n",
            "Batch 118 Loss: 0.927121\n",
            "Batch 119 Loss: 1.707093\n",
            "Batch 120 Loss: 0.518397\n",
            "Batch 121 Loss: 0.349278\n",
            "Batch 122 Loss: 0.553004\n",
            "Batch 123 Loss: 0.428543\n",
            "Batch 124 Loss: 0.619157\n",
            "Batch 125 Loss: 0.315221\n",
            "Batch 126 Loss: 1.066689\n",
            "Batch 127 Loss: 1.398974\n",
            "Batch 128 Loss: 0.673481\n",
            "Batch 129 Loss: 0.668328\n",
            "Batch 130 Loss: 0.428339\n",
            "Batch 131 Loss: 0.340374\n",
            "Batch 132 Loss: 0.579243\n",
            "Batch 133 Loss: 0.383878\n",
            "Batch 134 Loss: 1.120530\n",
            "Batch 135 Loss: 0.762546\n",
            "Batch 136 Loss: 2.323326\n",
            "Batch 137 Loss: 1.628732\n",
            "Batch 138 Loss: 0.716306\n",
            "Batch 139 Loss: 2.445265\n",
            "Batch 140 Loss: 0.340204\n",
            "Batch 141 Loss: 0.514732\n",
            "Batch 142 Loss: 1.237907\n",
            "Batch 143 Loss: 0.330624\n",
            "Batch 144 Loss: 0.445801\n",
            "Batch 145 Loss: 0.926548\n",
            "Batch 146 Loss: 1.049308\n",
            "Batch 147 Loss: 0.545991\n",
            "Batch 148 Loss: 1.826376\n",
            "Batch 149 Loss: 0.564880\n",
            "Batch 150 Loss: 0.513745\n",
            "Batch 151 Loss: 1.411067\n",
            "Batch 152 Loss: 2.815481\n",
            "Epoch 6 Loss: 0.9414\n",
            "Max token ID: tensor(5, device='cuda:0')\n",
            "Embedding size: 6\n",
            "Output shape: torch.Size([4, 30, 3])\n",
            "Batch 1 Loss: 0.434245\n",
            "Batch 2 Loss: 0.549006\n",
            "Batch 3 Loss: 0.259067\n",
            "Batch 4 Loss: 0.877135\n",
            "Batch 5 Loss: 0.513883\n",
            "Batch 6 Loss: 2.007840\n",
            "Batch 7 Loss: 0.481735\n",
            "Batch 8 Loss: 14.319219\n",
            "Batch 9 Loss: 0.697086\n",
            "Batch 10 Loss: 1.759038\n",
            "Batch 11 Loss: 0.318521\n",
            "Batch 12 Loss: 0.365488\n",
            "Batch 13 Loss: 1.421082\n",
            "Batch 14 Loss: 0.950040\n",
            "Batch 15 Loss: 0.530499\n",
            "Batch 16 Loss: 0.353212\n",
            "Batch 17 Loss: 0.490143\n",
            "Batch 18 Loss: 0.361412\n",
            "Batch 19 Loss: 0.373244\n",
            "Batch 20 Loss: 2.686723\n",
            "Batch 21 Loss: 0.489128\n",
            "Batch 22 Loss: 1.351024\n",
            "Batch 23 Loss: 1.744329\n",
            "Batch 24 Loss: 0.571710\n",
            "Batch 25 Loss: 3.299345\n",
            "Batch 26 Loss: 0.324891\n",
            "Batch 27 Loss: 0.978626\n",
            "Batch 28 Loss: 1.349147\n",
            "Batch 29 Loss: 3.201297\n",
            "Batch 30 Loss: 0.518101\n",
            "Batch 31 Loss: 0.327478\n",
            "Batch 32 Loss: 1.832422\n",
            "Batch 33 Loss: 1.571439\n",
            "Batch 34 Loss: 0.670346\n",
            "Batch 35 Loss: 0.612534\n",
            "Batch 36 Loss: 0.835181\n",
            "Batch 37 Loss: 0.365154\n",
            "Batch 38 Loss: 0.688156\n",
            "Batch 39 Loss: 0.457195\n",
            "Batch 40 Loss: 0.461302\n",
            "Batch 41 Loss: 0.839202\n",
            "Batch 42 Loss: 1.411727\n",
            "Batch 43 Loss: 1.780624\n",
            "Batch 44 Loss: 0.416887\n",
            "Batch 45 Loss: 1.173777\n",
            "Batch 46 Loss: 0.631170\n",
            "Batch 47 Loss: 0.319746\n",
            "Batch 48 Loss: 0.395348\n",
            "Batch 49 Loss: 0.664850\n",
            "Batch 50 Loss: 0.396810\n",
            "Batch 51 Loss: 2.534256\n",
            "Batch 52 Loss: 1.492159\n",
            "Batch 53 Loss: 0.662097\n",
            "Batch 54 Loss: 0.581022\n",
            "Batch 55 Loss: 0.661713\n",
            "Batch 56 Loss: 1.363095\n",
            "Batch 57 Loss: 0.515944\n",
            "Batch 58 Loss: 1.265743\n",
            "Batch 59 Loss: 1.195777\n",
            "Batch 60 Loss: 0.806466\n",
            "Batch 61 Loss: 2.149759\n",
            "Batch 62 Loss: 0.448020\n",
            "Batch 63 Loss: 0.822051\n",
            "Batch 64 Loss: 0.569136\n",
            "Batch 65 Loss: 0.323254\n",
            "Batch 66 Loss: 0.326160\n",
            "Batch 67 Loss: 0.316133\n",
            "Batch 68 Loss: 0.500356\n",
            "Batch 69 Loss: 0.691995\n",
            "Batch 70 Loss: 0.413572\n",
            "Batch 71 Loss: 0.412288\n",
            "Batch 72 Loss: 1.937060\n",
            "Batch 73 Loss: 1.216863\n",
            "Batch 74 Loss: 0.353197\n",
            "Batch 75 Loss: 0.383852\n",
            "Batch 76 Loss: 0.387636\n",
            "Batch 77 Loss: 0.479988\n",
            "Batch 78 Loss: 0.367082\n",
            "Batch 79 Loss: 0.897773\n",
            "Batch 80 Loss: 1.159965\n",
            "Batch 81 Loss: 8.191973\n",
            "Batch 82 Loss: 0.296394\n",
            "Batch 83 Loss: 0.303050\n",
            "Batch 84 Loss: 2.065438\n",
            "Batch 85 Loss: 0.485476\n",
            "Batch 86 Loss: 2.418877\n",
            "Batch 87 Loss: 0.826452\n",
            "Batch 88 Loss: 0.544875\n",
            "Batch 89 Loss: 0.498408\n",
            "Batch 90 Loss: 0.875773\n",
            "Batch 91 Loss: 0.365884\n",
            "Batch 92 Loss: 0.561952\n",
            "Batch 93 Loss: 0.818578\n",
            "Batch 94 Loss: 0.392489\n",
            "Batch 95 Loss: 0.315798\n",
            "Batch 96 Loss: 0.604392\n",
            "Batch 97 Loss: 2.423221\n",
            "Batch 98 Loss: 1.161119\n",
            "Batch 99 Loss: 0.469580\n",
            "Batch 100 Loss: 0.347977\n",
            "Batch 101 Loss: 0.392556\n",
            "Batch 102 Loss: 0.417049\n",
            "Batch 103 Loss: 0.340579\n",
            "Batch 104 Loss: 0.341504\n",
            "Batch 105 Loss: 2.523415\n",
            "Batch 106 Loss: 0.412749\n",
            "Batch 107 Loss: 1.638508\n",
            "Batch 108 Loss: 0.438457\n",
            "Batch 109 Loss: 1.136260\n",
            "Batch 110 Loss: 0.379788\n",
            "Batch 111 Loss: 0.325912\n",
            "Batch 112 Loss: 0.338646\n",
            "Batch 113 Loss: 0.383962\n",
            "Batch 114 Loss: 0.353691\n",
            "Batch 115 Loss: 1.886356\n",
            "Batch 116 Loss: 0.412685\n",
            "Batch 117 Loss: 1.570101\n",
            "Batch 118 Loss: 1.559609\n",
            "Batch 119 Loss: 0.346500\n",
            "Batch 120 Loss: 0.442097\n",
            "Batch 121 Loss: 0.441903\n",
            "Batch 122 Loss: 2.012433\n",
            "Batch 123 Loss: 0.347189\n",
            "Batch 124 Loss: 0.533466\n",
            "Batch 125 Loss: 1.236189\n",
            "Batch 126 Loss: 0.328910\n",
            "Batch 127 Loss: 0.528511\n",
            "Batch 128 Loss: 0.357634\n",
            "Batch 129 Loss: 0.629557\n",
            "Batch 130 Loss: 0.372443\n",
            "Batch 131 Loss: 0.585248\n",
            "Batch 132 Loss: 0.893928\n",
            "Batch 133 Loss: 1.263428\n",
            "Batch 134 Loss: 0.580702\n",
            "Batch 135 Loss: 0.545184\n",
            "Batch 136 Loss: 0.534909\n",
            "Batch 137 Loss: 0.293162\n",
            "Batch 138 Loss: 1.277240\n",
            "Batch 139 Loss: 0.387734\n",
            "Batch 140 Loss: 1.036166\n",
            "Batch 141 Loss: 1.661239\n",
            "Batch 142 Loss: 0.660101\n",
            "Batch 143 Loss: 1.584451\n",
            "Batch 144 Loss: 0.405104\n",
            "Batch 145 Loss: 0.407271\n",
            "Batch 146 Loss: 0.792350\n",
            "Batch 147 Loss: 1.590703\n",
            "Batch 148 Loss: 0.390645\n",
            "Batch 149 Loss: 0.382190\n",
            "Batch 150 Loss: 0.576577\n",
            "Batch 151 Loss: 0.447891\n",
            "Batch 152 Loss: 0.325357\n",
            "Epoch 7 Loss: 0.9716\n",
            "Max token ID: tensor(5, device='cuda:0')\n",
            "Embedding size: 6\n",
            "Output shape: torch.Size([4, 1539, 3])\n",
            "Batch 1 Loss: 2.539065\n",
            "Batch 2 Loss: 0.641557\n",
            "Batch 3 Loss: 2.657378\n",
            "Batch 4 Loss: 0.383052\n",
            "Batch 5 Loss: 0.503963\n",
            "Batch 6 Loss: 0.732516\n",
            "Batch 7 Loss: 1.392473\n",
            "Batch 8 Loss: 1.919868\n",
            "Batch 9 Loss: 0.617923\n",
            "Batch 10 Loss: 1.598568\n",
            "Batch 11 Loss: 2.002051\n",
            "Batch 12 Loss: 0.972419\n",
            "Batch 13 Loss: 0.295494\n",
            "Batch 14 Loss: 0.903829\n",
            "Batch 15 Loss: 1.582318\n",
            "Batch 16 Loss: 0.550412\n",
            "Batch 17 Loss: 4.391498\n",
            "Batch 18 Loss: 1.015389\n",
            "Batch 19 Loss: 1.120244\n",
            "Batch 20 Loss: 0.374164\n",
            "Batch 21 Loss: 0.496031\n",
            "Batch 22 Loss: 0.440306\n",
            "Batch 23 Loss: 0.312706\n",
            "Batch 24 Loss: 0.659703\n",
            "Batch 25 Loss: 0.357646\n",
            "Batch 26 Loss: 0.509689\n",
            "Batch 27 Loss: 0.736164\n",
            "Batch 28 Loss: 0.415511\n",
            "Batch 29 Loss: 0.790648\n",
            "Batch 30 Loss: 0.670430\n",
            "Batch 31 Loss: 1.404483\n",
            "Batch 32 Loss: 0.433012\n",
            "Batch 33 Loss: 1.718397\n",
            "Batch 34 Loss: 0.289366\n",
            "Batch 35 Loss: 1.749705\n",
            "Batch 36 Loss: 2.179492\n",
            "Batch 37 Loss: 1.264579\n",
            "Batch 38 Loss: 0.966547\n",
            "Batch 39 Loss: 0.505650\n",
            "Batch 40 Loss: 0.339941\n",
            "Batch 41 Loss: 0.615635\n",
            "Batch 42 Loss: 0.599104\n",
            "Batch 43 Loss: 0.560502\n",
            "Batch 44 Loss: 0.294558\n",
            "Batch 45 Loss: 0.313555\n",
            "Batch 46 Loss: 0.405763\n",
            "Batch 47 Loss: 1.871228\n",
            "Batch 48 Loss: 0.849373\n",
            "Batch 49 Loss: 0.539840\n",
            "Batch 50 Loss: 1.704600\n",
            "Batch 51 Loss: 0.491581\n",
            "Batch 52 Loss: 2.392545\n",
            "Batch 53 Loss: 2.345780\n",
            "Batch 54 Loss: 1.923944\n",
            "Batch 55 Loss: 0.553686\n",
            "Batch 56 Loss: 1.916527\n",
            "Batch 57 Loss: 2.264654\n",
            "Batch 58 Loss: 0.502744\n",
            "Batch 59 Loss: 0.358405\n",
            "Batch 60 Loss: 0.306121\n",
            "Batch 61 Loss: 0.932324\n",
            "Batch 62 Loss: 0.352162\n",
            "Batch 63 Loss: 1.545651\n",
            "Batch 64 Loss: 0.748834\n",
            "Batch 65 Loss: 0.426651\n",
            "Batch 66 Loss: 0.314733\n",
            "Batch 67 Loss: 0.800162\n",
            "Batch 68 Loss: 0.548396\n",
            "Batch 69 Loss: 0.349057\n",
            "Batch 70 Loss: 1.029542\n",
            "Batch 71 Loss: 1.465194\n",
            "Batch 72 Loss: 0.395753\n",
            "Batch 73 Loss: 0.442578\n",
            "Batch 74 Loss: 0.352651\n",
            "Batch 75 Loss: 1.048321\n",
            "Batch 76 Loss: 5.459475\n",
            "Batch 77 Loss: 1.645525\n",
            "Batch 78 Loss: 0.389416\n",
            "Batch 79 Loss: 2.703384\n",
            "Batch 80 Loss: 0.282435\n",
            "Batch 81 Loss: 0.275344\n",
            "Batch 82 Loss: 0.674961\n",
            "Batch 83 Loss: 0.994429\n",
            "Batch 84 Loss: 0.931380\n",
            "Batch 85 Loss: 0.356103\n",
            "Batch 86 Loss: 0.334412\n",
            "Batch 87 Loss: 1.705207\n",
            "Batch 88 Loss: 0.800173\n",
            "Batch 89 Loss: 0.583263\n",
            "Batch 90 Loss: 0.831637\n",
            "Batch 91 Loss: 0.926855\n",
            "Batch 92 Loss: 0.288037\n",
            "Batch 93 Loss: 0.382379\n",
            "Batch 94 Loss: 0.814922\n",
            "Batch 95 Loss: 0.641651\n",
            "Batch 96 Loss: 1.435979\n",
            "Batch 97 Loss: 0.633501\n",
            "Batch 98 Loss: 0.910968\n",
            "Batch 99 Loss: 0.717506\n",
            "Batch 100 Loss: 0.751282\n",
            "Batch 101 Loss: 0.389645\n",
            "Batch 102 Loss: 0.515566\n",
            "Batch 103 Loss: 1.499463\n",
            "Batch 104 Loss: 1.734524\n",
            "Batch 105 Loss: 0.490655\n",
            "Batch 106 Loss: 0.659442\n",
            "Batch 107 Loss: 1.179871\n",
            "Batch 108 Loss: 1.606427\n",
            "Batch 109 Loss: 0.568048\n",
            "Batch 110 Loss: 0.331883\n",
            "Batch 111 Loss: 1.154094\n",
            "Batch 112 Loss: 0.363592\n",
            "Batch 113 Loss: 1.489113\n",
            "Batch 114 Loss: 0.397810\n",
            "Batch 115 Loss: 0.402491\n",
            "Batch 116 Loss: 1.547884\n",
            "Batch 117 Loss: 1.755180\n",
            "Batch 118 Loss: 0.430639\n",
            "Batch 119 Loss: 2.601925\n",
            "Batch 120 Loss: 0.801451\n",
            "Batch 121 Loss: 0.489310\n",
            "Batch 122 Loss: 0.285596\n",
            "Batch 123 Loss: 0.278997\n",
            "Batch 124 Loss: 0.388896\n",
            "Batch 125 Loss: 0.264763\n",
            "Batch 126 Loss: 0.301067\n",
            "Batch 127 Loss: 0.345418\n",
            "Batch 128 Loss: 0.810034\n",
            "Batch 129 Loss: 0.582369\n",
            "Batch 130 Loss: 1.347905\n",
            "Batch 131 Loss: 0.616504\n",
            "Batch 132 Loss: 0.404826\n",
            "Batch 133 Loss: 0.286312\n",
            "Batch 134 Loss: 0.338782\n",
            "Batch 135 Loss: 0.407887\n",
            "Batch 136 Loss: 0.521342\n",
            "Batch 137 Loss: 0.420539\n",
            "Batch 138 Loss: 0.536942\n",
            "Batch 139 Loss: 0.453378\n",
            "Batch 140 Loss: 0.955527\n",
            "Batch 141 Loss: 0.955363\n",
            "Batch 142 Loss: 0.366873\n",
            "Batch 143 Loss: 0.396220\n",
            "Batch 144 Loss: 0.376078\n",
            "Batch 145 Loss: 0.658384\n",
            "Batch 146 Loss: 0.700947\n",
            "Batch 147 Loss: 0.316066\n",
            "Batch 148 Loss: 0.264850\n",
            "Batch 149 Loss: 0.765234\n",
            "Batch 150 Loss: 0.543352\n",
            "Batch 151 Loss: 0.494216\n",
            "Batch 152 Loss: 1.462573\n",
            "Epoch 8 Loss: 0.8995\n",
            "Max token ID: tensor(5, device='cuda:0')\n",
            "Embedding size: 6\n",
            "Output shape: torch.Size([4, 115, 3])\n",
            "Batch 1 Loss: 0.610726\n",
            "Batch 2 Loss: 0.448821\n",
            "Batch 3 Loss: 0.677856\n",
            "Batch 4 Loss: 0.319144\n",
            "Batch 5 Loss: 0.397860\n",
            "Batch 6 Loss: 0.324495\n",
            "Batch 7 Loss: 0.355812\n",
            "Batch 8 Loss: 0.930486\n",
            "Batch 9 Loss: 1.541739\n",
            "Batch 10 Loss: 0.462396\n",
            "Batch 11 Loss: 0.243561\n",
            "Batch 12 Loss: 0.371563\n",
            "Batch 13 Loss: 0.361653\n",
            "Batch 14 Loss: 0.346104\n",
            "Batch 15 Loss: 1.050500\n",
            "Batch 16 Loss: 0.454227\n",
            "Batch 17 Loss: 1.641741\n",
            "Batch 18 Loss: 0.839323\n",
            "Batch 19 Loss: 0.359076\n",
            "Batch 20 Loss: 0.864069\n",
            "Batch 21 Loss: 0.457334\n",
            "Batch 22 Loss: 0.309438\n",
            "Batch 23 Loss: 0.342034\n",
            "Batch 24 Loss: 0.860480\n",
            "Batch 25 Loss: 0.239383\n",
            "Batch 26 Loss: 0.332891\n",
            "Batch 27 Loss: 0.422144\n",
            "Batch 28 Loss: 1.042804\n",
            "Batch 29 Loss: 0.921672\n",
            "Batch 30 Loss: 0.463307\n",
            "Batch 31 Loss: 2.198242\n",
            "Batch 32 Loss: 0.387051\n",
            "Batch 33 Loss: 0.451564\n",
            "Batch 34 Loss: 0.286531\n",
            "Batch 35 Loss: 0.577801\n",
            "Batch 36 Loss: 0.792256\n",
            "Batch 37 Loss: 1.670037\n",
            "Batch 38 Loss: 1.810768\n",
            "Batch 39 Loss: 1.833704\n",
            "Batch 40 Loss: 0.566062\n",
            "Batch 41 Loss: 0.310517\n",
            "Batch 42 Loss: 1.556448\n",
            "Batch 43 Loss: 1.652941\n",
            "Batch 44 Loss: 0.330377\n",
            "Batch 45 Loss: 1.699671\n",
            "Batch 46 Loss: 1.549878\n",
            "Batch 47 Loss: 0.544046\n",
            "Batch 48 Loss: 3.291528\n",
            "Batch 49 Loss: 0.623181\n",
            "Batch 50 Loss: 0.356812\n",
            "Batch 51 Loss: 1.302301\n",
            "Batch 52 Loss: 0.365501\n",
            "Batch 53 Loss: 0.466762\n",
            "Batch 54 Loss: 1.331616\n",
            "Batch 55 Loss: 0.543809\n",
            "Batch 56 Loss: 1.174344\n",
            "Batch 57 Loss: 0.369354\n",
            "Batch 58 Loss: 1.299525\n",
            "Batch 59 Loss: 0.360466\n",
            "Batch 60 Loss: 0.425404\n",
            "Batch 61 Loss: 0.430571\n",
            "Batch 62 Loss: 0.357202\n",
            "Batch 63 Loss: 0.317037\n",
            "Batch 64 Loss: 0.952138\n",
            "Batch 65 Loss: 1.675949\n",
            "Batch 66 Loss: 0.947100\n",
            "Batch 67 Loss: 0.495009\n",
            "Batch 68 Loss: 0.256841\n",
            "Batch 69 Loss: 0.244105\n",
            "Batch 70 Loss: 0.517795\n",
            "Batch 71 Loss: 1.676680\n",
            "Batch 72 Loss: 1.374403\n",
            "Batch 73 Loss: 0.594649\n",
            "Batch 74 Loss: 1.214464\n",
            "Batch 75 Loss: 0.414260\n",
            "Batch 76 Loss: 0.390038\n",
            "Batch 77 Loss: 1.246129\n",
            "Batch 78 Loss: 0.361039\n",
            "Batch 79 Loss: 0.297088\n",
            "Batch 80 Loss: 1.358184\n",
            "Batch 81 Loss: 0.289041\n",
            "Batch 82 Loss: 0.836082\n",
            "Batch 83 Loss: 0.314761\n",
            "Batch 84 Loss: 0.722799\n",
            "Batch 85 Loss: 0.969279\n",
            "Batch 86 Loss: 0.292587\n",
            "Batch 87 Loss: 0.590939\n",
            "Batch 88 Loss: 1.598885\n",
            "Batch 89 Loss: 0.286006\n",
            "Batch 90 Loss: 0.483916\n",
            "Batch 91 Loss: 1.506445\n",
            "Batch 92 Loss: 0.496844\n",
            "Batch 93 Loss: 0.492349\n",
            "Batch 94 Loss: 2.715987\n",
            "Batch 95 Loss: 2.539334\n",
            "Batch 96 Loss: 0.416558\n",
            "Batch 97 Loss: 0.651382\n",
            "Batch 98 Loss: 0.264071\n",
            "Batch 99 Loss: 0.473457\n",
            "Batch 100 Loss: 0.286085\n",
            "Batch 101 Loss: 3.725429\n",
            "Batch 102 Loss: 2.778694\n",
            "Batch 103 Loss: 0.716440\n",
            "Batch 104 Loss: 0.304165\n",
            "Batch 105 Loss: 1.577028\n",
            "Batch 106 Loss: 0.607981\n",
            "Batch 107 Loss: 2.355633\n",
            "Batch 108 Loss: 0.630450\n",
            "Batch 109 Loss: 0.502757\n",
            "Batch 110 Loss: 1.525735\n",
            "Batch 111 Loss: 0.996049\n",
            "Batch 112 Loss: 8.117127\n",
            "Batch 113 Loss: 0.843714\n",
            "Batch 114 Loss: 0.685564\n",
            "Batch 115 Loss: 2.749759\n",
            "Batch 116 Loss: 0.470877\n",
            "Batch 117 Loss: 0.576086\n",
            "Batch 118 Loss: 1.298751\n",
            "Batch 119 Loss: 0.523933\n",
            "Batch 120 Loss: 1.559368\n",
            "Batch 121 Loss: 0.396237\n",
            "Batch 122 Loss: 0.522252\n",
            "Batch 123 Loss: 0.381442\n",
            "Batch 124 Loss: 0.435710\n",
            "Batch 125 Loss: 2.030817\n",
            "Batch 126 Loss: 0.837204\n",
            "Batch 127 Loss: 0.471194\n",
            "Batch 128 Loss: 0.383549\n",
            "Batch 129 Loss: 0.518587\n",
            "Batch 130 Loss: 0.435390\n",
            "Batch 131 Loss: 0.330881\n",
            "Batch 132 Loss: 2.853214\n",
            "Batch 133 Loss: 0.236297\n",
            "Batch 134 Loss: 0.316081\n",
            "Batch 135 Loss: 0.960427\n",
            "Batch 136 Loss: 1.293554\n",
            "Batch 137 Loss: 0.537759\n",
            "Batch 138 Loss: 0.535879\n",
            "Batch 139 Loss: 0.508863\n",
            "Batch 140 Loss: 0.497027\n",
            "Batch 141 Loss: 0.831827\n",
            "Batch 142 Loss: 0.469662\n",
            "Batch 143 Loss: 1.480493\n",
            "Batch 144 Loss: 0.809685\n",
            "Batch 145 Loss: 0.705676\n",
            "Batch 146 Loss: 2.024123\n",
            "Batch 147 Loss: 0.498093\n",
            "Batch 148 Loss: 0.425271\n",
            "Batch 149 Loss: 0.767198\n",
            "Batch 150 Loss: 1.782298\n",
            "Batch 151 Loss: 0.423590\n",
            "Batch 152 Loss: 0.656193\n",
            "Epoch 9 Loss: 0.9038\n",
            "Max token ID: tensor(5, device='cuda:0')\n",
            "Embedding size: 6\n",
            "Output shape: torch.Size([4, 2685, 3])\n",
            "Batch 1 Loss: 0.682294\n",
            "Batch 2 Loss: 0.411872\n",
            "Batch 3 Loss: 1.788783\n",
            "Batch 4 Loss: 0.338223\n",
            "Batch 5 Loss: 0.700514\n",
            "Batch 6 Loss: 0.295970\n",
            "Batch 7 Loss: 1.939257\n",
            "Batch 8 Loss: 0.287656\n",
            "Batch 9 Loss: 1.727064\n",
            "Batch 10 Loss: 0.599626\n",
            "Batch 11 Loss: 3.533882\n",
            "Batch 12 Loss: 0.956230\n",
            "Batch 13 Loss: 0.536449\n",
            "Batch 14 Loss: 0.262583\n",
            "Batch 15 Loss: 0.875438\n",
            "Batch 16 Loss: 0.391161\n",
            "Batch 17 Loss: 0.298054\n",
            "Batch 18 Loss: 1.451150\n",
            "Batch 19 Loss: 1.043900\n",
            "Batch 20 Loss: 0.420448\n",
            "Batch 21 Loss: 1.031681\n",
            "Batch 22 Loss: 0.389762\n",
            "Batch 23 Loss: 2.069368\n",
            "Batch 24 Loss: 0.329011\n",
            "Batch 25 Loss: 0.696100\n",
            "Batch 26 Loss: 0.733561\n",
            "Batch 27 Loss: 0.311485\n",
            "Batch 28 Loss: 0.310001\n",
            "Batch 29 Loss: 0.368665\n",
            "Batch 30 Loss: 1.532422\n",
            "Batch 31 Loss: 1.201139\n",
            "Batch 32 Loss: 0.279199\n",
            "Batch 33 Loss: 0.764788\n",
            "Batch 34 Loss: 2.671119\n",
            "Batch 35 Loss: 1.620341\n",
            "Batch 36 Loss: 0.674435\n",
            "Batch 37 Loss: 0.289747\n",
            "Batch 38 Loss: 1.341965\n",
            "Batch 39 Loss: 0.506238\n",
            "Batch 40 Loss: 1.121419\n",
            "Batch 41 Loss: 0.490734\n",
            "Batch 42 Loss: 0.510094\n",
            "Batch 43 Loss: 1.583367\n",
            "Batch 44 Loss: 0.473956\n",
            "Batch 45 Loss: 0.512038\n",
            "Batch 46 Loss: 0.334386\n",
            "Batch 47 Loss: 14.600188\n",
            "Batch 48 Loss: 0.397942\n",
            "Batch 49 Loss: 0.313854\n",
            "Batch 50 Loss: 0.656458\n",
            "Batch 51 Loss: 0.621919\n",
            "Batch 52 Loss: 0.617231\n",
            "Batch 53 Loss: 0.289899\n",
            "Batch 54 Loss: 1.214606\n",
            "Batch 55 Loss: 0.869238\n",
            "Batch 56 Loss: 1.654057\n",
            "Batch 57 Loss: 0.558356\n",
            "Batch 58 Loss: 0.408603\n",
            "Batch 59 Loss: 0.577810\n",
            "Batch 60 Loss: 1.641958\n",
            "Batch 61 Loss: 0.286620\n",
            "Batch 62 Loss: 0.281423\n",
            "Batch 63 Loss: 0.267681\n",
            "Batch 64 Loss: 1.144773\n",
            "Batch 65 Loss: 0.254020\n",
            "Batch 66 Loss: 0.475142\n",
            "Batch 67 Loss: 1.294412\n",
            "Batch 68 Loss: 0.331769\n",
            "Batch 69 Loss: 0.303473\n",
            "Batch 70 Loss: 0.410943\n",
            "Batch 71 Loss: 0.313481\n",
            "Batch 72 Loss: 0.790961\n",
            "Batch 73 Loss: 1.620947\n",
            "Batch 74 Loss: 0.296952\n",
            "Batch 75 Loss: 0.339951\n",
            "Batch 76 Loss: 0.271305\n",
            "Batch 77 Loss: 1.791831\n",
            "Batch 78 Loss: 0.497831\n",
            "Batch 79 Loss: 1.202742\n",
            "Batch 80 Loss: 0.633419\n",
            "Batch 81 Loss: 1.549157\n",
            "Batch 82 Loss: 0.291043\n",
            "Batch 83 Loss: 0.348092\n",
            "Batch 84 Loss: 0.619043\n",
            "Batch 85 Loss: 0.484087\n",
            "Batch 86 Loss: 1.020582\n",
            "Batch 87 Loss: 0.397372\n",
            "Batch 88 Loss: 0.362521\n",
            "Batch 89 Loss: 0.420107\n",
            "Batch 90 Loss: 1.616883\n",
            "Batch 91 Loss: 0.349083\n",
            "Batch 92 Loss: 0.310995\n",
            "Batch 93 Loss: 0.731173\n",
            "Batch 94 Loss: 1.267314\n",
            "Batch 95 Loss: 0.300088\n",
            "Batch 96 Loss: 0.293371\n",
            "Batch 97 Loss: 0.880825\n",
            "Batch 98 Loss: 2.026467\n",
            "Batch 99 Loss: 1.175879\n",
            "Batch 100 Loss: 2.313974\n",
            "Batch 101 Loss: 1.113547\n",
            "Batch 102 Loss: 1.797598\n",
            "Batch 103 Loss: 2.603063\n",
            "Batch 104 Loss: 0.303687\n",
            "Batch 105 Loss: 0.307849\n",
            "Batch 106 Loss: 0.387875\n",
            "Batch 107 Loss: 1.479310\n",
            "Batch 108 Loss: 2.886013\n",
            "Batch 109 Loss: 0.478315\n",
            "Batch 110 Loss: 0.768297\n",
            "Batch 111 Loss: 0.363986\n",
            "Batch 112 Loss: 0.403914\n",
            "Batch 113 Loss: 5.279863\n",
            "Batch 114 Loss: 0.447138\n",
            "Batch 115 Loss: 1.513580\n",
            "Batch 116 Loss: 0.286619\n",
            "Batch 117 Loss: 0.630385\n",
            "Batch 118 Loss: 0.399611\n",
            "Batch 119 Loss: 0.582910\n",
            "Batch 120 Loss: 0.273184\n",
            "Batch 121 Loss: 2.109044\n",
            "Batch 122 Loss: 0.326631\n",
            "Batch 123 Loss: 0.377118\n",
            "Batch 124 Loss: 0.671262\n",
            "Batch 125 Loss: 0.338484\n",
            "Batch 126 Loss: 0.523619\n",
            "Batch 127 Loss: 0.328179\n",
            "Batch 128 Loss: 0.565481\n",
            "Batch 129 Loss: 0.655559\n",
            "Batch 130 Loss: 0.742448\n",
            "Batch 131 Loss: 0.370232\n",
            "Batch 132 Loss: 0.458527\n",
            "Batch 133 Loss: 2.079036\n",
            "Batch 134 Loss: 1.058327\n",
            "Batch 135 Loss: 0.338044\n",
            "Batch 136 Loss: 0.437470\n",
            "Batch 137 Loss: 0.290796\n",
            "Batch 138 Loss: 0.496366\n",
            "Batch 139 Loss: 0.826029\n",
            "Batch 140 Loss: 0.463328\n",
            "Batch 141 Loss: 0.339755\n",
            "Batch 142 Loss: 1.668283\n",
            "Batch 143 Loss: 1.399466\n",
            "Batch 144 Loss: 1.576839\n",
            "Batch 145 Loss: 0.397557\n",
            "Batch 146 Loss: 0.568056\n",
            "Batch 147 Loss: 2.486891\n",
            "Batch 148 Loss: 1.610268\n",
            "Batch 149 Loss: 0.800209\n",
            "Batch 150 Loss: 0.603838\n",
            "Batch 151 Loss: 0.449932\n",
            "Batch 152 Loss: 0.283832\n",
            "Epoch 10 Loss: 0.9383\n"
          ]
        }
      ],
      "source": [
        "dataset = RNADataset(\"./data/train_sequences.csv\", \"./data/train_labels.csv\") # replace with your *actual* path\n",
        "loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "model = RNA3DFoldPredictor(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    embed_size=64,\n",
        "    num_layers=4,\n",
        "    heads=4,\n",
        "    forward_expansion=4,\n",
        "    dropout=0.2,\n",
        "    max_length=4298, # nearest multiple of 2 is 8192...actual max is 4298\n",
        ").to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    batch_num = 0\n",
        "\n",
        "    test_batch = next(iter(loader))\n",
        "    seqs, coords, mask = [x.to(device) for x in test_batch]\n",
        "\n",
        "    print(\"Max token ID:\", torch.max(seqs))  # Should be <= 3\n",
        "    print(\"Embedding size:\", model.token_embedding.num_embeddings)  # Should be 4\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(seqs, mask)\n",
        "    print(\"Output shape:\", outputs.shape)\n",
        "\n",
        "    for seqs, coords, mask in loader:\n",
        "        batch_num += 1\n",
        "        # print(\"Max token ID in batch:\", torch.max(seqs))\n",
        "        seqs, coords, mask_attention = seqs.to(device), coords.to(device), mask.to(device)\n",
        "\n",
        "        # check for any NaN values\n",
        "        if torch.isnan(coords).any() or torch.isinf(coords).any():\n",
        "            print(f\"WARNING: NaN/Inf found in target coordinates in batch {batch_num}! Skipping batch.\")\n",
        "            continue\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(seqs, mask_attention)\n",
        "\n",
        "\n",
        "        if torch.isnan(outputs).any() or torch.isinf(outputs).any():\n",
        "             print(f\"WARNING: NaN/Inf found in model outputs BEFORE loss calculation in batch {batch_num}!\")\n",
        "\n",
        "        non_pad_mask = (seqs != PAD_IDX) # Shape: (batch_size, seq_len)\n",
        "\n",
        "        # Flatten outputs and coords, then apply the mask\n",
        "        outputs_flat = outputs.view(-1, 3) # Shape: (batch * seq_len, 3)\n",
        "        coords_flat = coords.view(-1, 3)   # Shape: (batch * seq_len, 3)\n",
        "        non_pad_mask_flat = non_pad_mask.view(-1) # Shape: (batch * seq_len)\n",
        "\n",
        "        outputs_masked = outputs_flat[non_pad_mask_flat]\n",
        "        coords_masked = coords_flat[non_pad_mask_flat]\n",
        "\n",
        "        # Calculate loss ONLY on non-padded elements\n",
        "        if outputs_masked.nelement() > 0: # Check if there are any non-padded elements\n",
        "            loss = criterion(outputs_masked, coords_masked)\n",
        "\n",
        "            if torch.isnan(loss):\n",
        "               print(f\"WARNING: NaN detected in loss for batch {batch_num}!\")\n",
        "               # Add more debugging here if needed: print outputs_masked, coords_masked\n",
        "               continue # Skip optimization step for this batch\n",
        "\n",
        "            print(f\"Batch {batch_num} Loss: {loss.item():.6f}\") # Print loss *before* backward\n",
        "\n",
        "            loss.backward()\n",
        "            # Gradient Clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            print(f\"Skipping batch {batch_num} due to only padding elements.\")\n",
        "\n",
        "    # Avoid division by zero if loader is empty or all batches were skipped\n",
        "    if len(loader) > 0 and total_loss > 0:\n",
        "         print(f\"Epoch {epoch+1} Loss: {total_loss / len(loader):.4f}\") # Or divide by number of valid batches processed\n",
        "    else:\n",
        "         print(f\"Epoch {epoch+1} had no valid batches or zero total loss.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}