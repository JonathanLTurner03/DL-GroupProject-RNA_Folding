{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CS 4277: Deep Learning Group Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS 4277: *Deep Learning* Group Project\n",
    "### Members:\n",
    "- Nicholas Hodge\n",
    "- Joshua Peeples\n",
    "- Jonathan Turner\n",
    "\n",
    "### This project is our attempt at the Stanford RNA 3D Folding Challenge, found at:\n",
    "\n",
    "https://www.kaggle.com/competitions/stanford-rna-3d-folding\n",
    "\n",
    "**For this project to run:**\n",
    "\n",
    "1. Install matplotlib in your Jupyter Kernel: Block [1]\n",
    "2. Setup correct path files to your train dataset: Block [7] (there is a comment)\n",
    "\n",
    "**Future work:**\n",
    "\n",
    "1. Setup validation correctly\n",
    "2. Test\n",
    "3. Return Submission.csv as per requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run if matplotlib not installed\n",
    "# !  python -m pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUC_TO_IDX = {\n",
    "    \"A\": 0,\n",
    "    \"U\": 1,\n",
    "    \"C\": 2,\n",
    "    \"G\": 3,\n",
    "    \"N\": 4 # There are characters *other* than the above 4 sometimes. 'N' is standard for \"unknown\" (apparently)\n",
    "}\n",
    "VOCAB_SIZE = len(NUC_TO_IDX)\n",
    "\n",
    "class RNADataset(Dataset):\n",
    "    def __init__(self, seq_csv_path, coords_csv_path):\n",
    "        self.sequences_df = pd.read_csv(seq_csv_path)\n",
    "        self.coords_df = pd.read_csv(coords_csv_path)\n",
    "\n",
    "        # Extract base ID from \"ID\" column in coords\n",
    "        self.coords_df[\"base_id\"] = self.coords_df[\"ID\"].apply(lambda x: \"_\".join(x.split(\"_\")[:2]))\n",
    "\n",
    "        # Group by base ID and filter sequences that match\n",
    "        self.coord_groups = self.coords_df.groupby(\"base_id\")\n",
    "        self.valid_ids = set(self.coord_groups.groups.keys())\n",
    "\n",
    "        self.sequences_df = self.sequences_df[self.sequences_df[\"target_id\"].isin(self.valid_ids)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.sequences_df.iloc[idx]\n",
    "        seq_id = row[\"target_id\"]\n",
    "        sequence = row[\"sequence\"]\n",
    "\n",
    "        token_ids = []\n",
    "        for nuc in sequence:\n",
    "            token_ids.append(NUC_TO_IDX.get(nuc, NUC_TO_IDX[\"N\"]))  # Use \"N\" for unknown\n",
    "\n",
    "        token_ids = torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "        coords = self.coord_groups.get_group(seq_id)[[\"x_1\", \"y_1\", \"z_1\"]].values\n",
    "        coords = torch.tensor(coords, dtype=torch.float32)\n",
    "\n",
    "        return token_ids, coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences in collate_fn\n",
    "def collate_fn(batch):\n",
    "    sequences, coords = zip(*batch)\n",
    "\n",
    "    # Pad sequences and coordinates\n",
    "    seq_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    coord_padded = torch.nn.utils.rnn.pad_sequence(coords, batch_first=True, padding_value=0.0)\n",
    "\n",
    "    # Create mask (1 for real tokens, 0 for padded)\n",
    "    mask = (seq_padded != 0).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    return seq_padded, coord_padded, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: Aladdin Persson on YouTube (then modified to have an encoder-only architecture)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (self.head_dim * heads == embed_size), \"Embed size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N, value_len, _ = values.shape\n",
    "        _, key_len, _ = keys.shape\n",
    "        _, query_len, _ = query.shape\n",
    "\n",
    "        # Split embedding into self.heads pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        # queries shape: (N, query_len, heads, heads_dim)\n",
    "        # keys shape: (N, key_len, heads, heads_dim)\n",
    "        # energy shape: (N, heads, query_len, key_len)\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(energy / (self.embed_size ** 0.5), dim=3)\n",
    "        attention = self.dropout(attention)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "        # attention shape: (N, heads, query_len, key_len)\n",
    "        # values shape: (N, value_len, heads, heads_dim)\n",
    "        # after einsum (N, query_len, heads, head_dim) then flatten last two dimensions\n",
    "        \n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        attention = self.attention(x, x, x, mask)\n",
    "\n",
    "        x = self.dropout(self.norm1(attention + x))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "    \n",
    "class RNA3DFoldPredictor(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 embed_size,\n",
    "                 num_layers,\n",
    "                 heads,\n",
    "                 forward_expansion,\n",
    "                 dropout,\n",
    "                 max_length):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(embed_size, 3)  # Predict (x, y, z)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        N, seq_len = x.shape\n",
    "\n",
    "        positions = torch.arange(0, seq_len).unsqueeze(0).expand(N, seq_len).to(x.device)\n",
    "\n",
    "        out = self.token_embedding(x) + self.position_embedding(positions)\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, mask)\n",
    "\n",
    "        coords = self.fc_out(out)\n",
    "        return coords\n",
    "    \n",
    "    def predict_multiple(self, x, n_samples=5):\n",
    "        self.train()  # Activate dropout during inference\n",
    "        with torch.no_grad():\n",
    "            outputs = [self(x) for _ in range(n_samples)]\n",
    "        return torch.stack(outputs)  # Shape: (n_samples, batch_size, seq_len, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token ID: tensor(3)\n",
      "Embedding size: 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMax token ID:\u001b[39m\u001b[33m\"\u001b[39m, torch.max(seqs))  \u001b[38;5;66;03m# Should be <= 3\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEmbedding size:\u001b[39m\u001b[33m\"\u001b[39m, model.token_embedding.num_embeddings)  \u001b[38;5;66;03m# Should be 4\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOutput shape:\u001b[39m\u001b[33m\"\u001b[39m, outputs.shape)\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m seqs, coords, mask \u001b[38;5;129;01min\u001b[39;00m loader:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lordn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lordn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 103\u001b[39m, in \u001b[36mRNA3DFoldPredictor.forward\u001b[39m\u001b[34m(self, x, mask)\u001b[39m\n\u001b[32m    101\u001b[39m out = \u001b[38;5;28mself\u001b[39m.token_embedding(x) + \u001b[38;5;28mself\u001b[39m.position_embedding(positions)\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     out = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m coords = \u001b[38;5;28mself\u001b[39m.fc_out(out)\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m coords\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lordn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lordn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, x, mask)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     attention = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.dropout(\u001b[38;5;28mself\u001b[39m.norm1(attention + x))\n\u001b[32m     72\u001b[39m     forward = \u001b[38;5;28mself\u001b[39m.feed_forward(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lordn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lordn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mSelfAttention.forward\u001b[39m\u001b[34m(self, values, keys, query, mask)\u001b[39m\n\u001b[32m     29\u001b[39m keys = \u001b[38;5;28mself\u001b[39m.keys(keys)\n\u001b[32m     30\u001b[39m queries = \u001b[38;5;28mself\u001b[39m.queries(queries)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m energy = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnqhd,nkhd->nhqk\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# queries shape: (N, query_len, heads, heads_dim)\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# keys shape: (N, key_len, heads, heads_dim)\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# energy shape: (N, heads, query_len, key_len)\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lordn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\functional.py:397\u001b[39m, in \u001b[36meinsum\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    394\u001b[39m     _operands = operands[\u001b[32m0\u001b[39m]\n\u001b[32m    395\u001b[39m     \u001b[38;5;66;03m# recurse incase operands contains value that has torch function\u001b[39;00m\n\u001b[32m    396\u001b[39m     \u001b[38;5;66;03m# in the original implementation this line is omitted\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m397\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m_operands\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) <= \u001b[32m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum.enabled:\n\u001b[32m    400\u001b[39m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[32m    401\u001b[39m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _VF.einsum(equation, operands)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lordn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\functional.py:402\u001b[39m, in \u001b[36meinsum\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, *_operands)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) <= \u001b[32m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum.enabled:\n\u001b[32m    400\u001b[39m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[32m    401\u001b[39m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    404\u001b[39m path = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum.is_available():\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "dataset = RNADataset(\"./data/train_sequences.csv\", \"./data/train_labels.csv\") # replace with your *actual* path\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model = RNA3DFoldPredictor(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_size=256,\n",
    "    num_layers=6,\n",
    "    heads=8,\n",
    "    forward_expansion=4,\n",
    "    dropout=0.2,\n",
    "    max_length=8192, # nearest multiple of 2...actual max is 4298\n",
    ").to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    test_batch = next(iter(loader))\n",
    "    seqs, coords, mask = [x.to(device) for x in test_batch]\n",
    "\n",
    "    print(\"Max token ID:\", torch.max(seqs))  # Should be <= 3\n",
    "    print(\"Embedding size:\", model.token_embedding.num_embeddings)  # Should be 4\n",
    "\n",
    "    outputs = model(seqs, mask)\n",
    "    print(\"Output shape:\", outputs.shape)\n",
    "\n",
    "    for seqs, coords, mask in loader:\n",
    "        print(\"Max token ID in batch:\", torch.max(seqs)) # check max value\n",
    "        seqs, coords, mask = seqs.to(device), coords.to(device), mask.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(seqs, mask)\n",
    "\n",
    "        loss = criterion(outputs, coords)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Loss: {total_loss / len(loader):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
