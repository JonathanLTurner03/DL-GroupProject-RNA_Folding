{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l37bn3-3nP5S"
      },
      "source": [
        "CS 4277: Deep Learning Group Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoeTYxZjnP5U"
      },
      "source": [
        "## CS 4277: *Deep Learning* Group Project\n",
        "### Members:\n",
        "- Nicholas Hodge\n",
        "- Joshua Peeples\n",
        "- Jonathan Turner\n",
        "\n",
        "### This project is our attempt at the Stanford RNA 3D Folding Challenge, found at:\n",
        "\n",
        "https://www.kaggle.com/competitions/stanford-rna-3d-folding\n",
        "\n",
        "**For this project to run:**\n",
        "\n",
        "1. Install matplotlib in your Jupyter Kernel: Block [1]\n",
        "2. Setup correct path files to your train dataset: Block [7] (there is a comment)\n",
        "\n",
        "**Future work:**\n",
        "\n",
        "1. Setup validation correctly\n",
        "2. Test\n",
        "3. Return Submission.csv as per requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGEUeleSnP5U"
      },
      "outputs": [],
      "source": [
        "# Uncomment and run if matplotlib not installed\n",
        "# !  python -m pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_qewxm9nP5V"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "198NcZ8bnP5V"
      },
      "outputs": [],
      "source": [
        "NUC_TO_IDX = {\n",
        "    \"A\": 0,\n",
        "    \"U\": 1,\n",
        "    \"C\": 2,\n",
        "    \"G\": 3,\n",
        "    \"N\": 4 # There are characters *other* than the above 4 sometimes. 'N' is standard for \"unknown\" (apparently)\n",
        "}\n",
        "PAD_IDX = 5\n",
        "VOCAB_SIZE = len(NUC_TO_IDX) + 1\n",
        "\n",
        "class RNADataset(Dataset):\n",
        "    def __init__(self, seq_csv_path, coords_csv_path):\n",
        "        self.sequences_df = pd.read_csv(seq_csv_path)\n",
        "        self.coords_df = pd.read_csv(coords_csv_path)\n",
        "\n",
        "        # Extract base ID from \"ID\" column in coords\n",
        "        self.coords_df[\"base_id\"] = self.coords_df[\"ID\"].apply(lambda x: \"_\".join(x.split(\"_\")[:2]))\n",
        "\n",
        "        # Group by base ID and filter sequences that match\n",
        "        self.coord_groups = self.coords_df.groupby(\"base_id\")\n",
        "        self.valid_ids = set(self.coord_groups.groups.keys())\n",
        "\n",
        "        self.sequences_df = self.sequences_df[self.sequences_df[\"target_id\"].isin(self.valid_ids)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.sequences_df.iloc[idx]\n",
        "        seq_id = row[\"target_id\"]\n",
        "        sequence = row[\"sequence\"]\n",
        "\n",
        "        token_ids = []\n",
        "        for nuc in sequence:\n",
        "            token_ids.append(NUC_TO_IDX.get(nuc, NUC_TO_IDX[\"N\"]))  # Use \"N\" for unknown\n",
        "\n",
        "        token_ids = torch.tensor(token_ids, dtype=torch.long)\n",
        "\n",
        "        coords = self.coord_groups.get_group(seq_id)[[\"x_1\", \"y_1\", \"z_1\"]].values\n",
        "        coords = torch.tensor(coords, dtype=torch.float32)\n",
        "\n",
        "        return token_ids, coords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ac683bn5nP5W"
      },
      "outputs": [],
      "source": [
        "# Pad sequences in collate_fn\n",
        "def collate_fn(batch):\n",
        "    sequences, coords = zip(*batch)\n",
        "\n",
        "    # Pad sequences with PAD_IDX\n",
        "    seq_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=PAD_IDX)\n",
        "    coord_padded = torch.nn.utils.rnn.pad_sequence(coords, batch_first=True, padding_value=0.0)\n",
        "\n",
        "    # Mask should check against PAD_IDX\n",
        "    mask = (seq_padded != PAD_IDX).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    return seq_padded, coord_padded, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFKtgnu9nP5W"
      },
      "outputs": [],
      "source": [
        "# Source: Aladdin Persson on YouTube (then modified to have an encoder-only architecture)\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads, dropout):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        assert (self.head_dim * heads == embed_size), \"Embed size needs to be divisible by heads\"\n",
        "\n",
        "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, values, keys, query, mask):\n",
        "        N, value_len, _ = values.shape\n",
        "        _, key_len, _ = keys.shape\n",
        "        _, query_len, _ = query.shape\n",
        "\n",
        "        # Split embedding into self.heads pieces\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "        values = self.values(values)\n",
        "        keys = self.keys(keys)\n",
        "        queries = self.queries(queries)\n",
        "\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
        "        # queries shape: (N, query_len, heads, heads_dim)\n",
        "        # keys shape: (N, key_len, heads, heads_dim)\n",
        "        # energy shape: (N, heads, query_len, key_len)\n",
        "\n",
        "        if mask is not None:\n",
        "            # mask: (batch, 1, 1, seq_len) -> broadcastable to (batch, heads, query_len, key_len)\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e9\"))\n",
        "\n",
        "        attention = torch.softmax(energy / (self.embed_size ** 0.5), dim=3)\n",
        "        attention = self.dropout(attention)\n",
        "\n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
        "            N, query_len, self.heads * self.head_dim\n",
        "        )\n",
        "        # attention shape: (N, heads, query_len, key_len)\n",
        "        # values shape: (N, value_len, heads, heads_dim)\n",
        "        # after einsum (N, query_len, heads, head_dim) then flatten last two dimensions\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        return out\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = SelfAttention(embed_size, heads, dropout)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attention = self.attention(x, x, x, mask)\n",
        "\n",
        "        x = self.dropout(self.norm1(attention + x))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm2(forward + x))\n",
        "        return out\n",
        "\n",
        "class RNA3DFoldPredictor(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 embed_size,\n",
        "                 num_layers,\n",
        "                 heads,\n",
        "                 forward_expansion,\n",
        "                 dropout,\n",
        "                 max_length):\n",
        "        super().__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.fc_out = nn.Linear(embed_size, 3)  # Predict (x, y, z)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        N, seq_len = x.shape\n",
        "\n",
        "        positions = torch.arange(0, seq_len).unsqueeze(0).expand(N, seq_len).to(x.device)\n",
        "\n",
        "        out = self.token_embedding(x) + self.position_embedding(positions)\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, mask)\n",
        "\n",
        "        coords = self.fc_out(out)\n",
        "        return coords\n",
        "\n",
        "    def predict_multiple(self, x, n_samples=5):\n",
        "        self.train()  # Activate dropout during inference\n",
        "        with torch.no_grad():\n",
        "            outputs = [self(x) for _ in range(n_samples)]\n",
        "        return torch.stack(outputs)  # Shape: (n_samples, batch_size, seq_len, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujvnX5UknP5W",
        "outputId": "c8b7a8ca-e870-451e-afef-6d70bc3c9a82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print (device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KC9qjEnrnP5X",
        "outputId": "70d60eba-d503-4909-deb8-4e2b5f8fb425"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max token ID: tensor(5)\n",
            "Embedding size: 6\n",
            "Output shape: torch.Size([8, 134, 3])\n",
            "WARNING: NaN/Inf found in target coordinates in batch 1! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 2! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 3! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 4! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 5! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 6! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 7! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 8! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 9! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 10! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 11! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 12! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 13! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 14! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 15! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 16! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 17! Skipping batch.\n",
            "Batch 18 Loss: 23310.738281\n",
            "WARNING: NaN/Inf found in target coordinates in batch 19! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 20! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 21! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 22! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 23! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 24! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 25! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 26! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 27! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 28! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 29! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 30! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 31! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 32! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 33! Skipping batch.\n",
            "Batch 34 Loss: 14000.553711\n",
            "WARNING: NaN/Inf found in target coordinates in batch 35! Skipping batch.\n",
            "Batch 36 Loss: 32008.906250\n",
            "WARNING: NaN/Inf found in target coordinates in batch 37! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 38! Skipping batch.\n",
            "Batch 39 Loss: 45114.101562\n",
            "WARNING: NaN/Inf found in target coordinates in batch 40! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 41! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 42! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 43! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 44! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 45! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 46! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 47! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 48! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 49! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 50! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 51! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 52! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 53! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 54! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 55! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 56! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 57! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 58! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 59! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 60! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 61! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 62! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 63! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 64! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 65! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 66! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 67! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 68! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 69! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 70! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 71! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 72! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 73! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 74! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 75! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 76! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 77! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 78! Skipping batch.\n",
            "Batch 79 Loss: 45330.460938\n",
            "WARNING: NaN/Inf found in target coordinates in batch 80! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 81! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 82! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 83! Skipping batch.\n",
            "Batch 84 Loss: 32596.687500\n",
            "Batch 85 Loss: 36681.210938\n",
            "WARNING: NaN/Inf found in target coordinates in batch 86! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 87! Skipping batch.\n",
            "Batch 88 Loss: 6102.099609\n",
            "WARNING: NaN/Inf found in target coordinates in batch 89! Skipping batch.\n",
            "WARNING: NaN/Inf found in target coordinates in batch 90! Skipping batch.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     42\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_attention\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.isnan(outputs).any() \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(outputs).any():\n\u001b[32m     47\u001b[39m      \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWARNING: NaN/Inf found in model outputs BEFORE loss calculation in batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m!\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lordn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lordn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 104\u001b[39m, in \u001b[36mRNA3DFoldPredictor.forward\u001b[39m\u001b[34m(self, x, mask)\u001b[39m\n\u001b[32m    102\u001b[39m out = \u001b[38;5;28mself\u001b[39m.token_embedding(x) + \u001b[38;5;28mself\u001b[39m.position_embedding(positions)\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     out = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m coords = \u001b[38;5;28mself\u001b[39m.fc_out(out)\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m coords\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lordn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lordn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, x, mask)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     attention = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.dropout(\u001b[38;5;28mself\u001b[39m.norm1(attention + x))\n\u001b[32m     73\u001b[39m     forward = \u001b[38;5;28mself\u001b[39m.feed_forward(x)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lordn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lordn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mSelfAttention.forward\u001b[39m\u001b[34m(self, values, keys, query, mask)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# mask: (batch, 1, 1, seq_len) -> broadcastable to (batch, heads, query_len, key_len)\u001b[39;00m\n\u001b[32m     39\u001b[39m     energy = energy.masked_fill(mask == \u001b[32m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-1e9\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m attention = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43menergy\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m attention = \u001b[38;5;28mself\u001b[39m.dropout(attention)\n\u001b[32m     44\u001b[39m out = torch.einsum(\u001b[33m\"\u001b[39m\u001b[33mnhql,nlhd->nqhd\u001b[39m\u001b[33m\"\u001b[39m, [attention, values]).reshape(\n\u001b[32m     45\u001b[39m     N, query_len, \u001b[38;5;28mself\u001b[39m.heads * \u001b[38;5;28mself\u001b[39m.head_dim\n\u001b[32m     46\u001b[39m )\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "dataset = RNADataset(\"./data/train_sequences.csv\", \"./data/train_labels.csv\") # replace with your *actual* path\n",
        "loader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "model = RNA3DFoldPredictor(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    embed_size=128,\n",
        "    num_layers=4,\n",
        "    heads=4,\n",
        "    forward_expansion=4,\n",
        "    dropout=0.2,\n",
        "    max_length=4298, # nearest multiple of 2...actual max is 4298\n",
        ").to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    batch_num = 0\n",
        "\n",
        "    test_batch = next(iter(loader))\n",
        "    seqs, coords, mask = [x.to(device) for x in test_batch]\n",
        "\n",
        "    print(\"Max token ID:\", torch.max(seqs))  # Should be <= 3\n",
        "    print(\"Embedding size:\", model.token_embedding.num_embeddings)  # Should be 4\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(seqs, mask)\n",
        "    print(\"Output shape:\", outputs.shape)\n",
        "\n",
        "    for seqs, coords, mask in loader:\n",
        "        batch_num += 1\n",
        "        # print(\"Max token ID in batch:\", torch.max(seqs))\n",
        "        seqs, coords, mask_attention = seqs.to(device), coords.to(device), mask.to(device)\n",
        "\n",
        "        # data check...LOTS of NaN...need to investigate!\n",
        "        if torch.isnan(coords).any() or torch.isinf(coords).any():\n",
        "            print(f\"WARNING: NaN/Inf found in target coordinates in batch {batch_num}! Skipping batch.\")\n",
        "            continue\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(seqs, mask_attention)\n",
        "\n",
        "\n",
        "        if torch.isnan(outputs).any() or torch.isinf(outputs).any():\n",
        "             print(f\"WARNING: NaN/Inf found in model outputs BEFORE loss calculation in batch {batch_num}!\")\n",
        "\n",
        "        non_pad_mask = (seqs != PAD_IDX) # Shape: (batch_size, seq_len)\n",
        "\n",
        "        # Flatten outputs and coords, then apply the mask\n",
        "        outputs_flat = outputs.view(-1, 3) # Shape: (batch * seq_len, 3)\n",
        "        coords_flat = coords.view(-1, 3)   # Shape: (batch * seq_len, 3)\n",
        "        non_pad_mask_flat = non_pad_mask.view(-1) # Shape: (batch * seq_len)\n",
        "\n",
        "        outputs_masked = outputs_flat[non_pad_mask_flat]\n",
        "        coords_masked = coords_flat[non_pad_mask_flat]\n",
        "\n",
        "        # Calculate loss ONLY on non-padded elements\n",
        "        if outputs_masked.nelement() > 0: # Check if there are any non-padded elements\n",
        "            loss = criterion(outputs_masked, coords_masked)\n",
        "\n",
        "            if torch.isnan(loss):\n",
        "               print(f\"WARNING: NaN detected in loss for batch {batch_num}!\")\n",
        "               # Add more debugging here if needed: print outputs_masked, coords_masked\n",
        "               continue # Skip optimization step for this batch\n",
        "\n",
        "            print(f\"Batch {batch_num} Loss: {loss.item():.6f}\") # Print loss *before* backward\n",
        "\n",
        "            loss.backward()\n",
        "            # Gradient Clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            print(f\"Skipping batch {batch_num} due to only padding elements.\")\n",
        "\n",
        "    # Avoid division by zero if loader is empty or all batches were skipped\n",
        "    if len(loader) > 0 and total_loss > 0:\n",
        "         print(f\"Epoch {epoch+1} Loss: {total_loss / len(loader):.4f}\") # Or divide by number of valid batches processed\n",
        "    else:\n",
        "         print(f\"Epoch {epoch+1} had no valid batches or zero total loss.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}