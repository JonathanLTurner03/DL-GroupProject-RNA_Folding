{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l37bn3-3nP5S"
      },
      "source": [
        "CS 4277: Deep Learning Group Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoeTYxZjnP5U"
      },
      "source": [
        "## CS 4277: *Deep Learning* Group Project\n",
        "### Members:\n",
        "- Nicholas Hodge\n",
        "- Joshua Peeples\n",
        "- Jonathan Turner\n",
        "\n",
        "### This project is our attempt at the Stanford RNA 3D Folding Challenge, found at:\n",
        "\n",
        "https://www.kaggle.com/competitions/stanford-rna-3d-folding\n",
        "\n",
        "**For this project to run:**\n",
        "\n",
        "1. Install matplotlib in your Jupyter Kernel: Block [1]\n",
        "2. Setup correct path files to your train dataset: Block [7] (there is a comment)\n",
        "\n",
        "**Future work:**\n",
        "\n",
        "1. Setup validation correctly\n",
        "2. Test\n",
        "3. Return Submission.csv as per requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BGEUeleSnP5U"
      },
      "outputs": [],
      "source": [
        "# Uncomment and run if matplotlib not installed\n",
        "# !  python -m pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Z_qewxm9nP5V"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# import matplotlib\n",
        "# import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujvnX5UknP5W",
        "outputId": "206a48fc-bf03-4930-d5f0-87f7af4a6b96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print (device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WdlPSe6rM19"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "198NcZ8bnP5V"
      },
      "outputs": [],
      "source": [
        "NUC_TO_IDX = {\n",
        "    \"A\": 0,\n",
        "    \"U\": 1,\n",
        "    \"C\": 2,\n",
        "    \"G\": 3,\n",
        "    \"N\": 4 # There are characters *other* than the above 4 sometimes. 'N' is standard for \"unknown\" (apparently)\n",
        "}\n",
        "PAD_IDX = 5\n",
        "VOCAB_SIZE = len(NUC_TO_IDX) + 1\n",
        "\n",
        "# Annotated to avoid later confusion - Nick\n",
        "# This is inherits the Dataset class from pytorch to allow for the dataset to be an Iterable (i.e. work a LOT faster)\n",
        "class RNADataset(Dataset):\n",
        "    def __init__(self, seq_csv_path, coords_csv_path):\n",
        "        # Read both train CSVs (the 'labels' csv -> 'coords')\n",
        "        self.sequences_df = pd.read_csv(seq_csv_path)\n",
        "        coords_df_raw = pd.read_csv(coords_csv_path)\n",
        "\n",
        "        # We are going to get the base_id from each row in coords to associate them with the correct sequence.\n",
        "        # 1SCL_A_5 becomes 1SCL_A\n",
        "        coords_df_raw[\"base_id\"] = coords_df_raw[\"ID\"].apply(lambda x: \"_\".join(x.split(\"_\")[:2]))\n",
        "\n",
        "        # Now we are going to create groups of coords, where each group corresponds with the same sequence\n",
        "        # Unfortunately some sequences have missing coord values, but I am going to assume that there are potential\n",
        "        # sequences that have some missing and some not. So:\n",
        "\n",
        "        # Method to remove entire groups where *any* row has missing coords\n",
        "        def is_group_valid(group):\n",
        "            return group[[\"x_1\", \"y_1\", \"z_1\"]].notna().all().all() # returns only rows where all columns are good\n",
        "\n",
        "        valid_groups = [\n",
        "            group for _, group in coords_df_raw.groupby(\"base_id\") if is_group_valid(group)\n",
        "        ]\n",
        "\n",
        "        # Concatenate all valid groups into a new coords_df\n",
        "        self.coords_df = pd.concat(valid_groups, ignore_index=True)\n",
        "\n",
        "        # Build groups and valid sequence IDs list\n",
        "        self.coord_groups = self.coords_df.groupby(\"base_id\")\n",
        "        self.valid_ids = set(self.coord_groups.groups.keys())\n",
        "\n",
        "        # Filter sequences to only include those with clean coordinate groups (prevents later tensors from being mishaped)\n",
        "        self.sequences_df = self.sequences_df[self.sequences_df[\"target_id\"].isin(self.valid_ids)]\n",
        "\n",
        "    # Optional but Pytorch docs suggest this for 'Sampler' implmentations (might need that?)\n",
        "    def __len__(self):\n",
        "        return len(self.sequences_df)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.sequences_df.iloc[idx]\n",
        "        seq_id = row[\"target_id\"]\n",
        "        sequence = row[\"sequence\"]\n",
        "\n",
        "        token_ids = [NUC_TO_IDX.get(nuc, NUC_TO_IDX[\"N\"]) for nuc in sequence]\n",
        "        token_ids = torch.tensor(token_ids, dtype=torch.long)\n",
        "\n",
        "        # Here we introduce standardization to the coordinates\n",
        "\n",
        "        # TODO: calculate the following values somewhere in the document in case the dataset changes:\n",
        "        # Currently precalculated values\n",
        "        mean_x = 80.44731529117061\n",
        "        std_x = 147.42231938515297\n",
        "        mean_y = 84.04072703411182\n",
        "        std_y = 114.92890150429712\n",
        "        mean_z = 98.61122565112208\n",
        "        std_z = 119.41066506340083\n",
        "\n",
        "        coords_standardized = self.coord_groups.get_group(seq_id)[[\"x_1\", \"y_1\", \"z_1\"]].values\n",
        "        coords_standardized[:, 0] = (coords_standardized[:, 0] - mean_x) / std_x\n",
        "        coords_standardized[:, 1] = (coords_standardized[:, 1] - mean_y) / std_y\n",
        "        coords_standardized[:, 2] = (coords_standardized[:, 2] - mean_z) / std_z\n",
        "\n",
        "        coords = torch.tensor(coords_standardized, dtype=torch.float32)\n",
        "\n",
        "        return token_ids, coords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ac683bn5nP5W"
      },
      "outputs": [],
      "source": [
        "# Pad sequences in train_collate_fn\n",
        "def train_collate_fn(batch):\n",
        "    sequences, coords = zip(*batch)\n",
        "\n",
        "    # Pad sequences with PAD_IDX\n",
        "    seq_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=PAD_IDX)\n",
        "    coord_padded = torch.nn.utils.rnn.pad_sequence(coords, batch_first=True, padding_value=0.0)\n",
        "\n",
        "    # Mask should check against PAD_IDX\n",
        "    mask = (seq_padded != PAD_IDX).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    return seq_padded, coord_padded, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iFKtgnu9nP5W"
      },
      "outputs": [],
      "source": [
        "# Source: Aladdin Persson on YouTube (then modified to have an encoder-only architecture)\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads, dropout):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        assert (self.head_dim * heads == embed_size), \"Embed size needs to be divisible by heads\"\n",
        "\n",
        "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, values, keys, query, mask):\n",
        "        N, value_len, _ = values.shape\n",
        "        _, key_len, _ = keys.shape\n",
        "        _, query_len, _ = query.shape\n",
        "\n",
        "        # Split embedding into self.heads pieces\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "        values = self.values(values)\n",
        "        keys = self.keys(keys)\n",
        "        queries = self.queries(queries)\n",
        "\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
        "        # queries shape: (N, query_len, heads, heads_dim)\n",
        "        # keys shape: (N, key_len, heads, heads_dim)\n",
        "        # energy shape: (N, heads, query_len, key_len)\n",
        "\n",
        "        if mask is not None:\n",
        "            # mask: (batch, 1, 1, seq_len) -> broadcastable to (batch, heads, query_len, key_len)\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e9\"))\n",
        "\n",
        "        attention = torch.softmax(energy / (self.embed_size ** 0.5), dim=3)\n",
        "        attention = self.dropout(attention)\n",
        "\n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
        "            N, query_len, self.heads * self.head_dim\n",
        "        )\n",
        "        # attention shape: (N, heads, query_len, key_len)\n",
        "        # values shape: (N, value_len, heads, heads_dim)\n",
        "        # after einsum (N, query_len, heads, head_dim) then flatten last two dimensions\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        return out\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = SelfAttention(embed_size, heads, dropout)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attention = self.attention(x, x, x, mask)\n",
        "\n",
        "        x = self.dropout(self.norm1(attention + x))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm2(forward + x))\n",
        "        return out\n",
        "\n",
        "class RNA3DFoldPredictor(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 embed_size,\n",
        "                 num_layers,\n",
        "                 heads,\n",
        "                 forward_expansion,\n",
        "                 dropout,\n",
        "                 max_length):\n",
        "        super().__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.fc_out = nn.Linear(embed_size, 3)  # Predict (x, y, z)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        N, seq_len = x.shape\n",
        "\n",
        "        positions = torch.arange(0, seq_len).unsqueeze(0).expand(N, seq_len).to(x.device)\n",
        "\n",
        "        out = self.token_embedding(x) + self.position_embedding(positions)\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, mask)\n",
        "\n",
        "        coords = self.fc_out(out)\n",
        "        return coords\n",
        "\n",
        "    def predict_multiple(self, x, n_samples=5):\n",
        "        self.train()  # Activate dropout during inference\n",
        "        with torch.no_grad():\n",
        "            outputs = [self(x) for _ in range(n_samples)]\n",
        "        return torch.stack(outputs)  # Shape: (n_samples, batch_size, seq_len, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qNMvLJbarM1_"
      },
      "outputs": [],
      "source": [
        "def compute_tm_score(pred_coords, true_coords):\n",
        "    # pred_coords and true_coords: shape (seq_len, 3)\n",
        "    # Mask out invalid coordinates (e.g. from padding or -1e18)\n",
        "    valid_mask = ~(torch.isclose(true_coords, torch.tensor(-1e18)).any(dim=-1))\n",
        "\n",
        "    pred_coords = pred_coords[valid_mask]\n",
        "    true_coords = true_coords[valid_mask]\n",
        "\n",
        "    if len(pred_coords) < 3 or len(true_coords) < 3:\n",
        "        return 0.0  # Not enough points to compare\n",
        "\n",
        "    # Convert to numpy\n",
        "    pred_coords = pred_coords.detach().cpu().numpy()\n",
        "    true_coords = true_coords.detach().cpu().numpy()\n",
        "\n",
        "    # Superimpose using Procrustes\n",
        "    mu_pred = np.mean(pred_coords, axis=0)\n",
        "    mu_true = np.mean(true_coords, axis=0)\n",
        "    pred_centered = pred_coords - mu_pred\n",
        "    true_centered = true_coords - mu_true\n",
        "\n",
        "    H = pred_centered.T @ true_centered\n",
        "    U, S, Vt = np.linalg.svd(H)\n",
        "    R = Vt.T @ U.T\n",
        "    pred_aligned = pred_centered @ R\n",
        "\n",
        "    rmsd = np.sqrt(np.mean(np.sum((pred_aligned - true_centered) ** 2, axis=1)))\n",
        "    tm_score = 1 / (1 + (rmsd / 1.24))  # Simplified TM-score-like metric\n",
        "\n",
        "    return tm_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KC9qjEnrnP5X",
        "outputId": "076c9257-98c2-4719-a6e4-91da764ebd68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max token ID: tensor(5, device='cuda:0')\n",
            "Embedding size: 6\n",
            "Output shape: torch.Size([4, 134, 3])\n",
            "Batch 1 Loss: 1.075750\n",
            "Batch 2 Loss: 1.697220\n",
            "Batch 3 Loss: 1.186286\n",
            "Batch 4 Loss: 0.785351\n",
            "Batch 5 Loss: 1.685617\n",
            "Batch 6 Loss: 1.633016\n",
            "Batch 7 Loss: 0.988838\n",
            "Batch 8 Loss: 1.577247\n",
            "Batch 9 Loss: 1.084945\n",
            "Batch 10 Loss: 0.918133\n",
            "Batch 11 Loss: 0.986582\n",
            "Batch 12 Loss: 0.663105\n",
            "Batch 13 Loss: 0.656975\n",
            "Batch 14 Loss: 1.483961\n",
            "Batch 15 Loss: 1.234468\n",
            "Batch 16 Loss: 1.436586\n",
            "Batch 17 Loss: 1.311145\n",
            "Batch 18 Loss: 0.646610\n",
            "Batch 19 Loss: 0.819395\n",
            "Batch 20 Loss: 1.119095\n",
            "Batch 21 Loss: 1.187753\n",
            "Batch 22 Loss: 1.205928\n",
            "Batch 23 Loss: 1.253905\n",
            "Batch 24 Loss: 1.017939\n",
            "Batch 25 Loss: 1.844937\n",
            "Batch 26 Loss: 0.880586\n",
            "Batch 27 Loss: 1.777552\n",
            "Batch 28 Loss: 0.467045\n",
            "Batch 29 Loss: 0.917429\n",
            "Batch 30 Loss: 0.934313\n",
            "Batch 31 Loss: 1.509476\n",
            "Batch 32 Loss: 1.638286\n",
            "Batch 33 Loss: 1.050958\n",
            "Batch 34 Loss: 0.854604\n",
            "Batch 35 Loss: 1.048206\n",
            "Batch 36 Loss: 0.920672\n",
            "Batch 37 Loss: 0.824061\n",
            "Batch 38 Loss: 1.088969\n",
            "Batch 39 Loss: 1.000744\n",
            "Batch 40 Loss: 1.062917\n",
            "Batch 41 Loss: 0.818052\n",
            "Batch 42 Loss: 1.168112\n",
            "Batch 43 Loss: 1.102562\n",
            "Batch 44 Loss: 1.107108\n",
            "Batch 45 Loss: 0.735803\n",
            "Batch 46 Loss: 0.963253\n",
            "Batch 47 Loss: 0.993083\n",
            "Batch 48 Loss: 1.993700\n",
            "Batch 49 Loss: 0.859932\n",
            "Batch 50 Loss: 0.609888\n",
            "Batch 51 Loss: 0.929154\n",
            "Batch 52 Loss: 0.606004\n",
            "Batch 53 Loss: 1.325969\n",
            "Batch 54 Loss: 0.842841\n",
            "Batch 55 Loss: 1.314230\n",
            "Batch 56 Loss: 0.876266\n",
            "Batch 57 Loss: 1.193102\n",
            "Batch 58 Loss: 1.377966\n",
            "Batch 59 Loss: 1.411014\n",
            "Batch 60 Loss: 0.953521\n",
            "Batch 61 Loss: 1.529313\n",
            "Batch 62 Loss: 0.600741\n",
            "Batch 63 Loss: 0.761271\n",
            "Batch 64 Loss: 0.919296\n",
            "Batch 65 Loss: 0.836751\n",
            "Batch 66 Loss: 0.762485\n",
            "Batch 67 Loss: 1.337787\n",
            "Batch 68 Loss: 1.607143\n",
            "Batch 69 Loss: 0.846276\n",
            "Batch 70 Loss: 0.719964\n",
            "Batch 71 Loss: 1.749915\n",
            "Batch 72 Loss: 0.822885\n",
            "Batch 73 Loss: 0.777593\n",
            "Batch 74 Loss: 0.913908\n",
            "Batch 75 Loss: 0.854725\n",
            "Batch 76 Loss: 0.594445\n",
            "Batch 77 Loss: 0.776190\n",
            "Batch 78 Loss: 2.021286\n",
            "Batch 79 Loss: 1.116009\n",
            "Batch 80 Loss: 0.742474\n",
            "Batch 81 Loss: 0.770316\n",
            "Batch 82 Loss: 0.977974\n",
            "Batch 83 Loss: 1.199286\n",
            "Batch 84 Loss: 6.271273\n",
            "Batch 85 Loss: 1.573248\n",
            "Batch 86 Loss: 0.522164\n",
            "Batch 87 Loss: 0.516652\n",
            "Batch 88 Loss: 0.808446\n",
            "Batch 89 Loss: 0.542774\n",
            "Batch 90 Loss: 0.905837\n",
            "Batch 91 Loss: 0.596008\n",
            "Batch 92 Loss: 1.189417\n",
            "Batch 93 Loss: 1.347947\n",
            "Batch 94 Loss: 1.471344\n",
            "Batch 95 Loss: 0.549877\n",
            "Batch 96 Loss: 1.356849\n",
            "Batch 97 Loss: 1.218790\n",
            "Batch 98 Loss: 0.879444\n",
            "Batch 99 Loss: 0.758938\n",
            "Batch 100 Loss: 2.151298\n",
            "Batch 101 Loss: 0.597194\n",
            "Batch 102 Loss: 0.693662\n",
            "Batch 103 Loss: 0.895034\n",
            "Batch 104 Loss: 2.045518\n",
            "Batch 105 Loss: 0.605019\n",
            "Batch 106 Loss: 0.739244\n",
            "Batch 107 Loss: 0.868927\n",
            "Batch 108 Loss: 1.243481\n",
            "Batch 109 Loss: 0.661518\n",
            "Batch 110 Loss: 1.296675\n",
            "Batch 111 Loss: 0.707978\n",
            "Batch 112 Loss: 0.796073\n",
            "Batch 113 Loss: 1.750363\n",
            "Batch 114 Loss: 0.773084\n",
            "Batch 115 Loss: 0.636774\n",
            "Batch 116 Loss: 0.752003\n",
            "Batch 117 Loss: 1.736152\n",
            "Batch 118 Loss: 1.125524\n",
            "Batch 119 Loss: 1.013950\n",
            "Batch 120 Loss: 1.312479\n",
            "Batch 121 Loss: 0.767025\n",
            "Batch 122 Loss: 0.839358\n",
            "Batch 123 Loss: 1.083377\n",
            "Batch 124 Loss: 0.739803\n",
            "Batch 125 Loss: 0.689071\n",
            "Batch 126 Loss: 0.815464\n",
            "Batch 127 Loss: 0.827800\n",
            "Batch 128 Loss: 0.752253\n",
            "Batch 129 Loss: 1.116991\n",
            "Batch 130 Loss: 0.901696\n",
            "Batch 131 Loss: 0.956678\n",
            "Batch 132 Loss: 0.727739\n",
            "Batch 133 Loss: 0.556053\n",
            "Batch 134 Loss: 1.183758\n",
            "Batch 135 Loss: 0.694563\n",
            "Batch 136 Loss: 0.781720\n",
            "Batch 137 Loss: 1.358737\n",
            "Batch 138 Loss: 0.717952\n",
            "Batch 139 Loss: 0.959375\n",
            "Batch 140 Loss: 0.728103\n",
            "Batch 141 Loss: 1.173982\n",
            "Batch 142 Loss: 0.692162\n",
            "Batch 143 Loss: 1.498252\n",
            "Batch 144 Loss: 2.458324\n",
            "Batch 145 Loss: 0.677022\n",
            "Batch 146 Loss: 2.323028\n",
            "Batch 147 Loss: 0.892590\n",
            "Batch 148 Loss: 0.845837\n",
            "Batch 149 Loss: 0.484434\n",
            "Batch 150 Loss: 1.462726\n",
            "Batch 151 Loss: 0.684442\n",
            "Batch 152 Loss: 15.524038\n",
            "Epoch 1 Loss: 1.1758\n",
            "Max token ID: tensor(5, device='cuda:0')\n",
            "Embedding size: 6\n",
            "Output shape: torch.Size([4, 115, 3])\n",
            "Batch 1 Loss: 0.524373\n",
            "Batch 2 Loss: 0.894403\n",
            "Batch 3 Loss: 1.622102\n",
            "Batch 4 Loss: 14.871303\n",
            "Batch 5 Loss: 0.732641\n",
            "Batch 6 Loss: 0.853484\n",
            "Batch 7 Loss: 1.189172\n",
            "Batch 8 Loss: 0.516037\n",
            "Batch 9 Loss: 1.176197\n",
            "Batch 10 Loss: 0.826303\n",
            "Batch 11 Loss: 1.219860\n",
            "Batch 12 Loss: 0.759293\n",
            "Batch 13 Loss: 5.213121\n",
            "Batch 14 Loss: 1.447593\n",
            "Batch 15 Loss: 0.745789\n",
            "Batch 16 Loss: 1.964069\n",
            "Batch 17 Loss: 1.179732\n",
            "Batch 18 Loss: 0.660710\n",
            "Batch 19 Loss: 0.726719\n",
            "Batch 20 Loss: 3.189852\n",
            "Batch 21 Loss: 0.577102\n",
            "Batch 22 Loss: 0.799163\n",
            "Batch 23 Loss: 0.648532\n",
            "Batch 24 Loss: 0.792593\n",
            "Batch 25 Loss: 0.815291\n",
            "Batch 26 Loss: 0.976570\n",
            "Batch 27 Loss: 0.555884\n",
            "Batch 28 Loss: 0.565830\n",
            "Batch 29 Loss: 0.536659\n",
            "Batch 30 Loss: 1.329610\n",
            "Batch 31 Loss: 0.672535\n",
            "Batch 32 Loss: 1.485019\n",
            "Batch 33 Loss: 1.768785\n",
            "Batch 34 Loss: 1.358482\n",
            "Batch 35 Loss: 1.048364\n",
            "Batch 36 Loss: 1.147084\n",
            "Batch 37 Loss: 0.831869\n",
            "Batch 38 Loss: 1.541516\n",
            "Batch 39 Loss: 0.617144\n",
            "Batch 40 Loss: 0.900807\n",
            "Batch 41 Loss: 0.723332\n",
            "Batch 42 Loss: 2.492928\n",
            "Batch 43 Loss: 0.559539\n",
            "Batch 44 Loss: 1.342091\n",
            "Batch 45 Loss: 0.847190\n",
            "Batch 46 Loss: 0.617407\n",
            "Batch 47 Loss: 0.604670\n",
            "Batch 48 Loss: 0.683328\n",
            "Batch 49 Loss: 0.938803\n",
            "Batch 50 Loss: 0.713451\n",
            "Batch 51 Loss: 0.650263\n",
            "Batch 52 Loss: 0.618027\n",
            "Batch 53 Loss: 1.436265\n",
            "Batch 54 Loss: 0.599386\n",
            "Batch 55 Loss: 1.040724\n",
            "Batch 56 Loss: 1.287943\n",
            "Batch 57 Loss: 0.984924\n",
            "Batch 58 Loss: 0.959814\n",
            "Batch 59 Loss: 0.993128\n",
            "Batch 60 Loss: 0.491781\n",
            "Batch 61 Loss: 0.677458\n",
            "Batch 62 Loss: 0.712441\n",
            "Batch 63 Loss: 0.695941\n",
            "Batch 64 Loss: 1.471342\n",
            "Batch 65 Loss: 0.561213\n",
            "Batch 66 Loss: 0.489882\n",
            "Batch 67 Loss: 1.564235\n",
            "Batch 68 Loss: 0.574873\n",
            "Batch 69 Loss: 0.636726\n",
            "Batch 70 Loss: 0.621635\n",
            "Batch 71 Loss: 0.594495\n",
            "Batch 72 Loss: 1.167694\n",
            "Batch 73 Loss: 0.519736\n",
            "Batch 74 Loss: 0.971592\n",
            "Batch 75 Loss: 0.595668\n",
            "Batch 76 Loss: 0.464324\n",
            "Batch 77 Loss: 0.476801\n",
            "Batch 78 Loss: 1.803160\n",
            "Batch 79 Loss: 0.452070\n",
            "Batch 80 Loss: 2.126482\n",
            "Batch 81 Loss: 0.619046\n",
            "Batch 82 Loss: 1.585517\n",
            "Batch 83 Loss: 1.009929\n",
            "Batch 84 Loss: 1.566259\n",
            "Batch 85 Loss: 0.506191\n",
            "Batch 86 Loss: 0.877980\n",
            "Batch 87 Loss: 0.694065\n",
            "Batch 88 Loss: 0.625268\n",
            "Batch 89 Loss: 0.744515\n",
            "Batch 90 Loss: 0.426449\n",
            "Batch 91 Loss: 0.615736\n",
            "Batch 92 Loss: 0.643231\n",
            "Batch 93 Loss: 0.644232\n",
            "Batch 94 Loss: 1.442500\n",
            "Batch 95 Loss: 0.490966\n",
            "Batch 96 Loss: 0.967642\n",
            "Batch 97 Loss: 0.446474\n",
            "Batch 98 Loss: 0.652108\n",
            "Batch 99 Loss: 0.538496\n",
            "Batch 100 Loss: 1.312918\n",
            "Batch 101 Loss: 1.586125\n",
            "Batch 102 Loss: 0.577197\n",
            "Batch 103 Loss: 0.878190\n",
            "Batch 104 Loss: 2.750791\n",
            "Batch 105 Loss: 0.703073\n",
            "Batch 106 Loss: 0.631310\n",
            "Batch 107 Loss: 1.471772\n",
            "Batch 108 Loss: 0.524607\n",
            "Batch 109 Loss: 0.589014\n",
            "Batch 110 Loss: 1.125598\n",
            "Batch 111 Loss: 1.240937\n",
            "Batch 112 Loss: 1.152987\n",
            "Batch 113 Loss: 0.831914\n",
            "Batch 114 Loss: 0.604279\n",
            "Batch 115 Loss: 1.078516\n",
            "Batch 116 Loss: 0.832899\n",
            "Batch 117 Loss: 0.909496\n",
            "Batch 118 Loss: 1.613066\n",
            "Batch 119 Loss: 1.540166\n",
            "Batch 120 Loss: 0.581962\n",
            "Batch 121 Loss: 0.603942\n",
            "Batch 122 Loss: 0.600887\n",
            "Batch 123 Loss: 1.271253\n",
            "Batch 124 Loss: 0.738120\n",
            "Batch 125 Loss: 0.713256\n",
            "Batch 126 Loss: 0.462918\n",
            "Batch 127 Loss: 0.411787\n",
            "Batch 128 Loss: 0.569204\n",
            "Batch 129 Loss: 1.236358\n",
            "Batch 130 Loss: 0.699576\n",
            "Batch 131 Loss: 0.537272\n",
            "Batch 132 Loss: 1.769863\n",
            "Batch 133 Loss: 1.093980\n",
            "Batch 134 Loss: 1.638439\n",
            "Batch 135 Loss: 0.758167\n",
            "Batch 136 Loss: 0.704380\n",
            "Batch 137 Loss: 1.043046\n",
            "Batch 138 Loss: 0.803539\n",
            "Batch 139 Loss: 0.914369\n",
            "Batch 140 Loss: 0.635648\n",
            "Batch 141 Loss: 0.550543\n",
            "Batch 142 Loss: 1.353954\n",
            "Batch 143 Loss: 2.623985\n",
            "Batch 144 Loss: 1.139233\n",
            "Batch 145 Loss: 0.706373\n",
            "Batch 146 Loss: 0.739521\n",
            "Batch 147 Loss: 2.376122\n",
            "Batch 148 Loss: 1.250700\n",
            "Batch 149 Loss: 0.539881\n",
            "Batch 150 Loss: 2.210298\n",
            "Batch 151 Loss: 1.156417\n",
            "Batch 152 Loss: 0.396165\n",
            "Epoch 2 Loss: 1.0850\n",
            "Max token ID: tensor(5, device='cuda:0')\n",
            "Embedding size: 6\n",
            "Output shape: torch.Size([4, 164, 3])\n",
            "Batch 1 Loss: 0.487611\n",
            "Batch 2 Loss: 0.511679\n",
            "Batch 3 Loss: 0.699201\n",
            "Batch 4 Loss: 0.461450\n",
            "Batch 5 Loss: 0.785698\n",
            "Batch 6 Loss: 0.748056\n",
            "Batch 7 Loss: 0.941097\n",
            "Batch 8 Loss: 2.155007\n",
            "Batch 9 Loss: 1.509345\n",
            "Batch 10 Loss: 0.712654\n",
            "Batch 11 Loss: 0.920780\n",
            "Batch 12 Loss: 0.844058\n",
            "Batch 13 Loss: 1.620674\n",
            "Batch 14 Loss: 1.471383\n",
            "Batch 15 Loss: 0.569409\n",
            "Batch 16 Loss: 0.692383\n",
            "Batch 17 Loss: 0.921422\n",
            "Batch 18 Loss: 0.585829\n",
            "Batch 19 Loss: 0.663045\n",
            "Batch 20 Loss: 0.496538\n",
            "Batch 21 Loss: 1.648574\n",
            "Batch 22 Loss: 0.420860\n",
            "Batch 23 Loss: 0.730318\n",
            "Batch 24 Loss: 0.563049\n",
            "Batch 25 Loss: 0.745648\n",
            "Batch 26 Loss: 0.488867\n",
            "Batch 27 Loss: 1.529101\n",
            "Batch 28 Loss: 0.493652\n",
            "Batch 29 Loss: 0.532641\n",
            "Batch 30 Loss: 1.454076\n",
            "Batch 31 Loss: 0.879107\n",
            "Batch 32 Loss: 0.527856\n",
            "Batch 33 Loss: 1.076779\n",
            "Batch 34 Loss: 0.508234\n",
            "Batch 35 Loss: 0.668868\n",
            "Batch 36 Loss: 0.762027\n",
            "Batch 37 Loss: 2.665534\n",
            "Batch 38 Loss: 0.501743\n",
            "Batch 39 Loss: 0.987398\n",
            "Batch 40 Loss: 0.536670\n",
            "Batch 41 Loss: 0.579406\n",
            "Batch 42 Loss: 0.515757\n",
            "Batch 43 Loss: 1.506842\n",
            "Batch 44 Loss: 1.125868\n",
            "Batch 45 Loss: 2.217238\n",
            "Batch 46 Loss: 0.540068\n",
            "Batch 47 Loss: 0.441879\n",
            "Batch 48 Loss: 0.657664\n",
            "Batch 49 Loss: 2.522467\n",
            "Batch 50 Loss: 0.391285\n",
            "Batch 51 Loss: 0.425529\n",
            "Batch 52 Loss: 0.516541\n",
            "Batch 53 Loss: 0.777801\n",
            "Batch 54 Loss: 0.449396\n",
            "Batch 55 Loss: 0.664076\n",
            "Batch 56 Loss: 0.914482\n",
            "Batch 57 Loss: 1.045548\n",
            "Batch 58 Loss: 0.885000\n",
            "Batch 59 Loss: 0.871952\n",
            "Batch 60 Loss: 1.058755\n",
            "Batch 61 Loss: 0.498314\n",
            "Batch 62 Loss: 0.802893\n",
            "Batch 63 Loss: 0.603313\n",
            "Batch 64 Loss: 0.399250\n",
            "Batch 65 Loss: 0.552245\n",
            "Batch 66 Loss: 0.455283\n",
            "Batch 67 Loss: 1.306397\n",
            "Batch 68 Loss: 1.622064\n",
            "Batch 69 Loss: 0.837453\n",
            "Batch 70 Loss: 0.877887\n",
            "Batch 71 Loss: 2.704222\n",
            "Batch 72 Loss: 1.105237\n",
            "Batch 73 Loss: 0.635948\n",
            "Batch 74 Loss: 1.762513\n",
            "Batch 75 Loss: 0.656642\n",
            "Batch 76 Loss: 0.568520\n",
            "Batch 77 Loss: 0.483046\n",
            "Batch 78 Loss: 0.707487\n",
            "Batch 79 Loss: 0.951418\n",
            "Batch 80 Loss: 1.081334\n",
            "Batch 81 Loss: 0.703417\n",
            "Batch 82 Loss: 0.833779\n",
            "Batch 83 Loss: 1.220331\n",
            "Batch 84 Loss: 0.389574\n",
            "Batch 85 Loss: 0.553313\n",
            "Batch 86 Loss: 0.756149\n",
            "Batch 87 Loss: 0.557858\n",
            "Batch 88 Loss: 1.703637\n",
            "Batch 89 Loss: 2.153603\n",
            "Batch 90 Loss: 0.538688\n",
            "Batch 91 Loss: 1.901988\n",
            "Batch 92 Loss: 0.527320\n",
            "Batch 93 Loss: 0.628467\n",
            "Batch 94 Loss: 1.757842\n",
            "Batch 95 Loss: 0.417997\n",
            "Batch 96 Loss: 0.468061\n",
            "Batch 97 Loss: 0.480409\n",
            "Batch 98 Loss: 0.540919\n",
            "Batch 99 Loss: 1.687744\n",
            "Batch 100 Loss: 0.569372\n",
            "Batch 101 Loss: 0.730670\n",
            "Batch 102 Loss: 0.737419\n",
            "Batch 103 Loss: 0.529228\n",
            "Batch 104 Loss: 0.425303\n",
            "Batch 105 Loss: 0.645858\n",
            "Batch 106 Loss: 3.577767\n",
            "Batch 107 Loss: 0.377436\n",
            "Batch 108 Loss: 6.491652\n",
            "Batch 109 Loss: 0.848071\n",
            "Batch 110 Loss: 0.430001\n",
            "Batch 111 Loss: 1.938668\n",
            "Batch 112 Loss: 0.523227\n",
            "Batch 113 Loss: 0.421999\n",
            "Batch 114 Loss: 0.514179\n",
            "Batch 115 Loss: 1.068516\n",
            "Batch 116 Loss: 1.424876\n",
            "Batch 117 Loss: 2.363813\n",
            "Batch 118 Loss: 0.616062\n",
            "Batch 119 Loss: 0.993784\n",
            "Batch 120 Loss: 0.323593\n",
            "Batch 121 Loss: 0.919721\n",
            "Batch 122 Loss: 1.403184\n",
            "Batch 123 Loss: 1.009610\n",
            "Batch 124 Loss: 0.945525\n",
            "Batch 125 Loss: 0.477784\n",
            "Batch 126 Loss: 0.476970\n",
            "Batch 127 Loss: 0.610493\n",
            "Batch 128 Loss: 0.995544\n",
            "Batch 129 Loss: 0.563258\n",
            "Batch 130 Loss: 1.357557\n",
            "Batch 131 Loss: 0.796524\n",
            "Batch 132 Loss: 0.485123\n",
            "Batch 133 Loss: 0.810566\n",
            "Batch 134 Loss: 2.002604\n",
            "Batch 135 Loss: 0.606165\n",
            "Batch 136 Loss: 0.391043\n",
            "Batch 137 Loss: 1.058428\n",
            "Batch 138 Loss: 1.014519\n",
            "Batch 139 Loss: 0.577502\n",
            "Batch 140 Loss: 0.826701\n",
            "Batch 141 Loss: 0.439975\n",
            "Batch 142 Loss: 0.382308\n",
            "Batch 143 Loss: 0.558921\n",
            "Batch 144 Loss: 2.322216\n",
            "Batch 145 Loss: 0.584887\n",
            "Batch 146 Loss: 0.712986\n",
            "Batch 147 Loss: 0.446112\n",
            "Batch 148 Loss: 1.122599\n",
            "Batch 149 Loss: 1.173716\n",
            "Batch 150 Loss: 2.331569\n",
            "Batch 151 Loss: 7.460277\n",
            "Batch 152 Loss: 1.056986\n",
            "Epoch 3 Loss: 0.9969\n",
            "Max token ID: tensor(5, device='cuda:0')\n",
            "Embedding size: 6\n",
            "Output shape: torch.Size([4, 102, 3])\n",
            "Batch 1 Loss: 1.885601\n",
            "Batch 2 Loss: 0.653283\n",
            "Batch 3 Loss: 0.439007\n",
            "Batch 4 Loss: 1.479999\n",
            "Batch 5 Loss: 0.445742\n",
            "Batch 6 Loss: 0.399908\n",
            "Batch 7 Loss: 1.581406\n",
            "Batch 8 Loss: 1.209153\n",
            "Batch 9 Loss: 0.754734\n",
            "Batch 10 Loss: 1.555745\n",
            "Batch 11 Loss: 0.525016\n",
            "Batch 12 Loss: 0.365712\n",
            "Batch 13 Loss: 0.673796\n",
            "Batch 14 Loss: 1.034966\n",
            "Batch 15 Loss: 1.803944\n",
            "Batch 16 Loss: 0.514099\n",
            "Batch 17 Loss: 0.535120\n",
            "Batch 18 Loss: 0.809795\n",
            "Batch 19 Loss: 1.572437\n",
            "Batch 20 Loss: 0.582461\n",
            "Batch 21 Loss: 0.841255\n",
            "Batch 22 Loss: 14.369146\n",
            "Batch 23 Loss: 0.409127\n",
            "Batch 24 Loss: 1.562431\n",
            "Batch 25 Loss: 0.540691\n",
            "Batch 26 Loss: 0.770476\n",
            "Batch 27 Loss: 0.609733\n",
            "Batch 28 Loss: 0.559029\n",
            "Batch 29 Loss: 0.439983\n",
            "Batch 30 Loss: 0.583222\n",
            "Batch 31 Loss: 2.291628\n",
            "Batch 32 Loss: 1.840911\n",
            "Batch 33 Loss: 0.401864\n",
            "Batch 34 Loss: 0.468046\n",
            "Batch 35 Loss: 0.646248\n",
            "Batch 36 Loss: 1.641123\n",
            "Batch 37 Loss: 0.405098\n",
            "Batch 38 Loss: 1.379164\n",
            "Batch 39 Loss: 1.080104\n",
            "Batch 40 Loss: 0.599883\n",
            "Batch 41 Loss: 1.537313\n",
            "Batch 42 Loss: 1.092378\n",
            "Batch 43 Loss: 0.453331\n",
            "Batch 44 Loss: 0.363451\n",
            "Batch 45 Loss: 1.342381\n",
            "Batch 46 Loss: 0.716783\n",
            "Batch 47 Loss: 0.393764\n",
            "Batch 48 Loss: 1.092680\n",
            "Batch 49 Loss: 0.536112\n",
            "Batch 50 Loss: 0.580732\n",
            "Batch 51 Loss: 1.021924\n",
            "Batch 52 Loss: 0.404108\n",
            "Batch 53 Loss: 0.631129\n",
            "Batch 54 Loss: 0.615430\n",
            "Batch 55 Loss: 0.697525\n",
            "Batch 56 Loss: 0.820776\n",
            "Batch 57 Loss: 0.980610\n",
            "Batch 58 Loss: 1.679913\n",
            "Batch 59 Loss: 0.432831\n",
            "Batch 60 Loss: 0.433435\n",
            "Batch 61 Loss: 0.437860\n",
            "Batch 62 Loss: 2.609632\n",
            "Batch 63 Loss: 0.401136\n",
            "Batch 64 Loss: 0.775132\n",
            "Batch 65 Loss: 0.395505\n",
            "Batch 66 Loss: 0.466998\n",
            "Batch 67 Loss: 3.036431\n",
            "Batch 68 Loss: 0.831517\n",
            "Batch 69 Loss: 1.077730\n",
            "Batch 70 Loss: 0.526857\n",
            "Batch 71 Loss: 1.010734\n",
            "Batch 72 Loss: 2.576194\n",
            "Batch 73 Loss: 1.072787\n",
            "Batch 74 Loss: 0.627865\n",
            "Batch 75 Loss: 1.757540\n",
            "Batch 76 Loss: 0.567013\n",
            "Batch 77 Loss: 0.476126\n",
            "Batch 78 Loss: 0.423062\n",
            "Batch 79 Loss: 0.502271\n",
            "Batch 80 Loss: 0.470820\n",
            "Batch 81 Loss: 0.442853\n",
            "Batch 82 Loss: 1.703652\n",
            "Batch 83 Loss: 0.401559\n",
            "Batch 84 Loss: 1.601509\n",
            "Batch 85 Loss: 0.550731\n",
            "Batch 86 Loss: 0.348674\n",
            "Batch 87 Loss: 0.365671\n",
            "Batch 88 Loss: 0.491354\n",
            "Batch 89 Loss: 1.678737\n",
            "Batch 90 Loss: 0.723972\n",
            "Batch 91 Loss: 0.420162\n",
            "Batch 92 Loss: 0.399114\n",
            "Batch 93 Loss: 0.439968\n",
            "Batch 94 Loss: 3.790935\n",
            "Batch 95 Loss: 1.284840\n",
            "Batch 96 Loss: 0.691820\n",
            "Batch 97 Loss: 2.364804\n",
            "Batch 98 Loss: 0.359871\n",
            "Batch 99 Loss: 0.617267\n",
            "Batch 100 Loss: 0.423341\n",
            "Batch 101 Loss: 0.442100\n",
            "Batch 102 Loss: 0.475046\n",
            "Batch 103 Loss: 0.665457\n",
            "Batch 104 Loss: 0.549013\n",
            "Batch 105 Loss: 0.364943\n",
            "Batch 106 Loss: 1.695071\n",
            "Batch 107 Loss: 0.320249\n",
            "Batch 108 Loss: 0.563386\n",
            "Batch 109 Loss: 0.865175\n",
            "Batch 110 Loss: 0.472246\n",
            "Batch 111 Loss: 1.480947\n",
            "Batch 112 Loss: 0.664584\n",
            "Batch 113 Loss: 1.280071\n",
            "Batch 114 Loss: 0.428644\n",
            "Batch 115 Loss: 3.942665\n",
            "Batch 116 Loss: 0.953914\n",
            "Batch 117 Loss: 2.338627\n",
            "Batch 118 Loss: 0.580055\n",
            "Batch 119 Loss: 0.439795\n",
            "Batch 120 Loss: 0.507723\n",
            "Batch 121 Loss: 0.681498\n",
            "Batch 122 Loss: 0.555881\n",
            "Batch 123 Loss: 1.106429\n",
            "Batch 124 Loss: 0.549298\n",
            "Batch 125 Loss: 0.439182\n",
            "Batch 126 Loss: 0.996690\n",
            "Batch 127 Loss: 1.776033\n",
            "Batch 128 Loss: 0.558456\n",
            "Batch 129 Loss: 2.065661\n",
            "Batch 130 Loss: 0.749376\n",
            "Batch 131 Loss: 2.054997\n",
            "Batch 132 Loss: 0.518241\n",
            "Batch 133 Loss: 0.401639\n",
            "Batch 134 Loss: 0.356114\n",
            "Batch 135 Loss: 0.686591\n",
            "Batch 136 Loss: 0.406598\n",
            "Batch 137 Loss: 1.755429\n",
            "Batch 138 Loss: 0.381464\n",
            "Batch 139 Loss: 0.441177\n",
            "Batch 140 Loss: 0.569256\n",
            "Batch 141 Loss: 0.454106\n",
            "Batch 142 Loss: 0.446976\n",
            "Batch 143 Loss: 0.478934\n",
            "Batch 144 Loss: 1.323563\n",
            "Batch 145 Loss: 1.664652\n",
            "Batch 146 Loss: 1.941422\n",
            "Batch 147 Loss: 0.780049\n",
            "Batch 148 Loss: 0.294464\n",
            "Batch 149 Loss: 0.571342\n",
            "Batch 150 Loss: 1.509714\n",
            "Batch 151 Loss: 0.985100\n",
            "Batch 152 Loss: 1.160032\n",
            "Epoch 4 Loss: 1.0061\n",
            "Max token ID: tensor(5, device='cuda:0')\n",
            "Embedding size: 6\n",
            "Output shape: torch.Size([4, 1443, 3])\n",
            "Batch 1 Loss: 0.468825\n",
            "Batch 2 Loss: 0.652198\n",
            "Batch 3 Loss: 1.597295\n",
            "Batch 4 Loss: 0.379549\n",
            "Batch 5 Loss: 0.319109\n",
            "Batch 6 Loss: 0.377835\n",
            "Batch 7 Loss: 0.415597\n",
            "Batch 8 Loss: 1.771088\n",
            "Batch 9 Loss: 2.794469\n",
            "Batch 10 Loss: 0.331340\n",
            "Batch 11 Loss: 0.894454\n",
            "Batch 12 Loss: 0.424167\n",
            "Batch 13 Loss: 3.744259\n",
            "Batch 14 Loss: 0.486295\n",
            "Batch 15 Loss: 0.657970\n",
            "Batch 16 Loss: 0.462819\n",
            "Batch 17 Loss: 1.559416\n",
            "Batch 18 Loss: 1.802604\n",
            "Batch 19 Loss: 0.531206\n",
            "Batch 20 Loss: 0.375108\n",
            "Batch 21 Loss: 0.369834\n",
            "Batch 22 Loss: 1.507938\n",
            "Batch 23 Loss: 0.832494\n",
            "Batch 24 Loss: 0.555720\n",
            "Batch 25 Loss: 0.896147\n",
            "Batch 26 Loss: 2.257596\n",
            "Batch 27 Loss: 1.756685\n",
            "Batch 28 Loss: 1.005140\n",
            "Batch 29 Loss: 0.327206\n",
            "Batch 30 Loss: 0.434898\n",
            "Batch 31 Loss: 1.629326\n",
            "Batch 32 Loss: 0.414375\n",
            "Batch 33 Loss: 0.484906\n",
            "Batch 34 Loss: 2.144316\n",
            "Batch 35 Loss: 0.395950\n",
            "Batch 36 Loss: 1.742567\n",
            "Batch 37 Loss: 0.547835\n",
            "Batch 38 Loss: 0.536277\n",
            "Batch 39 Loss: 0.461808\n",
            "Batch 40 Loss: 2.137054\n",
            "Batch 41 Loss: 0.365520\n",
            "Batch 42 Loss: 1.235633\n",
            "Batch 43 Loss: 2.578579\n",
            "Batch 44 Loss: 0.475743\n",
            "Batch 45 Loss: 0.971476\n",
            "Batch 46 Loss: 1.469484\n",
            "Batch 47 Loss: 0.631730\n",
            "Batch 48 Loss: 0.369679\n",
            "Batch 49 Loss: 0.362577\n",
            "Batch 50 Loss: 0.533500\n",
            "Batch 51 Loss: 0.422999\n",
            "Batch 52 Loss: 0.374467\n",
            "Batch 53 Loss: 0.462535\n",
            "Batch 54 Loss: 0.796784\n",
            "Batch 55 Loss: 0.684376\n",
            "Batch 56 Loss: 0.766004\n",
            "Batch 57 Loss: 0.437103\n",
            "Batch 58 Loss: 0.962815\n",
            "Batch 59 Loss: 0.648596\n",
            "Batch 60 Loss: 0.628823\n",
            "Batch 61 Loss: 1.548925\n",
            "Batch 62 Loss: 0.335806\n",
            "Batch 63 Loss: 0.364382\n",
            "Batch 64 Loss: 0.391518\n",
            "Batch 65 Loss: 1.618028\n",
            "Batch 66 Loss: 0.316515\n",
            "Batch 67 Loss: 0.967617\n",
            "Batch 68 Loss: 2.684209\n",
            "Batch 69 Loss: 0.641767\n",
            "Batch 70 Loss: 0.632561\n",
            "Batch 71 Loss: 0.980080\n",
            "Batch 72 Loss: 0.352340\n",
            "Batch 73 Loss: 0.648048\n",
            "Batch 74 Loss: 0.558254\n",
            "Batch 75 Loss: 0.736181\n",
            "Batch 76 Loss: 2.034268\n",
            "Batch 77 Loss: 0.740817\n",
            "Batch 78 Loss: 0.540915\n",
            "Batch 79 Loss: 0.606241\n",
            "Batch 80 Loss: 0.421878\n",
            "Batch 81 Loss: 1.586734\n",
            "Batch 82 Loss: 1.809821\n",
            "Batch 83 Loss: 0.408027\n",
            "Batch 84 Loss: 0.681766\n",
            "Batch 85 Loss: 0.578174\n",
            "Batch 86 Loss: 5.275911\n",
            "Batch 87 Loss: 0.666504\n",
            "Batch 88 Loss: 0.421293\n",
            "Batch 89 Loss: 0.637003\n",
            "Batch 90 Loss: 1.604812\n",
            "Batch 91 Loss: 1.539300\n",
            "Batch 92 Loss: 1.311984\n",
            "Batch 93 Loss: 0.809207\n",
            "Batch 94 Loss: 0.691281\n",
            "Batch 95 Loss: 1.183731\n",
            "Batch 96 Loss: 0.675418\n",
            "Batch 97 Loss: 0.329744\n",
            "Batch 98 Loss: 0.595420\n",
            "Batch 99 Loss: 0.381587\n",
            "Batch 100 Loss: 0.577800\n",
            "Batch 101 Loss: 0.363225\n",
            "Batch 102 Loss: 0.551234\n",
            "Batch 103 Loss: 0.884353\n",
            "Batch 104 Loss: 0.583432\n",
            "Batch 105 Loss: 0.849435\n",
            "Batch 106 Loss: 0.429817\n",
            "Batch 107 Loss: 0.420399\n",
            "Batch 108 Loss: 0.368591\n",
            "Batch 109 Loss: 1.754348\n",
            "Batch 110 Loss: 0.417454\n",
            "Batch 111 Loss: 3.710746\n",
            "Batch 112 Loss: 0.405731\n",
            "Batch 113 Loss: 1.487055\n",
            "Batch 114 Loss: 0.630365\n",
            "Batch 115 Loss: 0.666680\n",
            "Batch 116 Loss: 1.394561\n",
            "Batch 117 Loss: 0.285678\n",
            "Batch 118 Loss: 0.339190\n",
            "Batch 119 Loss: 1.239784\n",
            "Batch 120 Loss: 0.357516\n",
            "Batch 121 Loss: 0.448682\n",
            "Batch 122 Loss: 1.834697\n",
            "Batch 123 Loss: 1.734303\n",
            "Batch 124 Loss: 0.941526\n",
            "Batch 125 Loss: 0.978102\n",
            "Batch 126 Loss: 0.368684\n",
            "Batch 127 Loss: 1.109148\n",
            "Batch 128 Loss: 0.394450\n",
            "Batch 129 Loss: 0.467426\n",
            "Batch 130 Loss: 1.037169\n",
            "Batch 131 Loss: 1.499378\n",
            "Batch 132 Loss: 1.357945\n",
            "Batch 133 Loss: 0.439494\n",
            "Batch 134 Loss: 0.992240\n",
            "Batch 135 Loss: 0.386623\n",
            "Batch 136 Loss: 0.401058\n",
            "Batch 137 Loss: 0.482938\n",
            "Batch 138 Loss: 1.169716\n",
            "Batch 139 Loss: 0.909247\n",
            "Batch 140 Loss: 0.388976\n",
            "Batch 141 Loss: 0.283246\n",
            "Batch 142 Loss: 0.428633\n",
            "Batch 143 Loss: 14.623137\n",
            "Batch 144 Loss: 0.572636\n",
            "Batch 145 Loss: 2.003534\n",
            "Batch 146 Loss: 2.067632\n",
            "Batch 147 Loss: 1.626534\n",
            "Batch 148 Loss: 0.348795\n",
            "Batch 149 Loss: 0.412371\n",
            "Batch 150 Loss: 0.531779\n",
            "Batch 151 Loss: 0.525280\n",
            "Batch 152 Loss: 1.014327\n",
            "Epoch 5 Loss: 1.0097\n",
            "Max token ID: tensor(5, device='cuda:0')\n",
            "Embedding size: 6\n",
            "Output shape: torch.Size([4, 121, 3])\n",
            "Batch 1 Loss: 0.349887\n",
            "Batch 2 Loss: 0.309294\n",
            "Batch 3 Loss: 1.847950\n",
            "Batch 4 Loss: 0.325430\n",
            "Batch 5 Loss: 0.779204\n",
            "Batch 6 Loss: 0.328281\n",
            "Batch 7 Loss: 0.396696\n",
            "Batch 8 Loss: 0.933304\n",
            "Batch 9 Loss: 1.538924\n",
            "Batch 10 Loss: 1.707801\n",
            "Batch 11 Loss: 1.132030\n",
            "Batch 12 Loss: 0.707224\n",
            "Batch 13 Loss: 0.577215\n",
            "Batch 14 Loss: 1.638437\n",
            "Batch 15 Loss: 0.423229\n",
            "Batch 16 Loss: 0.460060\n",
            "Batch 17 Loss: 0.255960\n",
            "Batch 18 Loss: 3.700332\n",
            "Batch 19 Loss: 1.653267\n",
            "Batch 20 Loss: 1.716284\n",
            "Batch 21 Loss: 0.663257\n",
            "Batch 22 Loss: 0.348748\n",
            "Batch 23 Loss: 1.447558\n",
            "Batch 24 Loss: 0.376801\n",
            "Batch 25 Loss: 0.365522\n",
            "Batch 26 Loss: 0.685236\n",
            "Batch 27 Loss: 0.669837\n",
            "Batch 28 Loss: 0.417719\n",
            "Batch 29 Loss: 0.336034\n",
            "Batch 30 Loss: 0.898674\n",
            "Batch 31 Loss: 0.308832\n",
            "Batch 32 Loss: 0.764540\n",
            "Batch 33 Loss: 0.456597\n",
            "Batch 34 Loss: 0.531236\n",
            "Batch 35 Loss: 0.443576\n",
            "Batch 36 Loss: 0.602994\n",
            "Batch 37 Loss: 0.502684\n",
            "Batch 38 Loss: 0.422862\n",
            "Batch 39 Loss: 0.603322\n",
            "Batch 40 Loss: 0.732283\n",
            "Batch 41 Loss: 0.758444\n",
            "Batch 42 Loss: 0.429224\n",
            "Batch 43 Loss: 0.944903\n",
            "Batch 44 Loss: 0.901967\n",
            "Batch 45 Loss: 0.394775\n",
            "Batch 46 Loss: 0.503168\n",
            "Batch 47 Loss: 0.498786\n",
            "Batch 48 Loss: 0.332424\n",
            "Batch 49 Loss: 1.921963\n",
            "Batch 50 Loss: 2.841284\n",
            "Batch 51 Loss: 1.060737\n",
            "Batch 52 Loss: 1.681993\n",
            "Batch 53 Loss: 0.381697\n",
            "Batch 54 Loss: 0.429446\n",
            "Batch 55 Loss: 2.102853\n",
            "Batch 56 Loss: 0.909192\n",
            "Batch 57 Loss: 0.449711\n",
            "Batch 58 Loss: 0.451836\n",
            "Batch 59 Loss: 0.331803\n",
            "Batch 60 Loss: 0.514484\n",
            "Batch 61 Loss: 0.842691\n",
            "Batch 62 Loss: 1.596405\n",
            "Batch 63 Loss: 0.493104\n",
            "Batch 64 Loss: 0.308327\n",
            "Batch 65 Loss: 0.439612\n",
            "Batch 66 Loss: 1.033174\n",
            "Batch 67 Loss: 0.446631\n",
            "Batch 68 Loss: 1.375029\n",
            "Batch 69 Loss: 0.498468\n",
            "Batch 70 Loss: 0.357039\n",
            "Batch 71 Loss: 0.331671\n",
            "Batch 72 Loss: 1.721654\n",
            "Batch 73 Loss: 0.567065\n",
            "Batch 74 Loss: 0.399635\n",
            "Batch 75 Loss: 0.624502\n",
            "Batch 76 Loss: 0.410650\n",
            "Batch 77 Loss: 2.565629\n",
            "Batch 78 Loss: 0.414840\n",
            "Batch 79 Loss: 0.319506\n",
            "Batch 80 Loss: 2.626951\n",
            "Batch 81 Loss: 0.531344\n",
            "Batch 82 Loss: 0.415525\n",
            "Batch 83 Loss: 0.350938\n",
            "Batch 84 Loss: 0.487742\n",
            "Batch 85 Loss: 0.522746\n",
            "Batch 86 Loss: 0.970074\n",
            "Batch 87 Loss: 1.017159\n",
            "Batch 88 Loss: 0.494923\n",
            "Batch 89 Loss: 0.586916\n",
            "Batch 90 Loss: 2.788549\n",
            "Batch 91 Loss: 0.313492\n",
            "Batch 92 Loss: 0.711427\n",
            "Batch 93 Loss: 0.316114\n",
            "Batch 94 Loss: 0.374365\n",
            "Batch 95 Loss: 0.280199\n",
            "Batch 96 Loss: 0.312129\n",
            "Batch 97 Loss: 0.455321\n",
            "Batch 98 Loss: 0.561140\n",
            "Batch 99 Loss: 0.625091\n",
            "Batch 100 Loss: 1.876618\n",
            "Batch 101 Loss: 0.532416\n",
            "Batch 102 Loss: 1.758590\n",
            "Batch 103 Loss: 2.717032\n",
            "Batch 104 Loss: 0.545228\n",
            "Batch 105 Loss: 0.884401\n",
            "Batch 106 Loss: 0.230408\n",
            "Batch 107 Loss: 0.342126\n",
            "Batch 108 Loss: 0.498661\n",
            "Batch 109 Loss: 0.917065\n",
            "Batch 110 Loss: 0.555998\n",
            "Batch 111 Loss: 0.347171\n",
            "Batch 112 Loss: 0.304048\n",
            "Batch 113 Loss: 0.353834\n",
            "Batch 114 Loss: 0.580541\n",
            "Batch 115 Loss: 1.301714\n",
            "Batch 116 Loss: 0.274679\n",
            "Batch 117 Loss: 1.145652\n",
            "Batch 118 Loss: 2.456001\n",
            "Batch 119 Loss: 0.303425\n",
            "Batch 120 Loss: 2.065306\n",
            "Batch 121 Loss: 0.586723\n",
            "Batch 122 Loss: 1.101928\n",
            "Batch 123 Loss: 0.770155\n",
            "Batch 124 Loss: 0.712242\n",
            "Batch 125 Loss: 0.353742\n",
            "Batch 126 Loss: 1.571446\n",
            "Batch 127 Loss: 1.896553\n",
            "Batch 128 Loss: 0.624006\n",
            "Batch 129 Loss: 1.699357\n",
            "Batch 130 Loss: 0.443478\n",
            "Batch 131 Loss: 0.961796\n",
            "Batch 132 Loss: 0.365715\n",
            "Batch 133 Loss: 0.366262\n",
            "Batch 134 Loss: 0.597638\n",
            "Batch 135 Loss: 1.754466\n",
            "Batch 136 Loss: 1.250113\n",
            "Batch 137 Loss: 0.432744\n",
            "Batch 138 Loss: 0.869440\n",
            "Batch 139 Loss: 0.420958\n",
            "Batch 140 Loss: 0.449260\n",
            "Batch 141 Loss: 1.571898\n",
            "Batch 142 Loss: 0.548256\n",
            "Batch 143 Loss: 0.374408\n",
            "Batch 144 Loss: 1.829851\n",
            "Batch 145 Loss: 0.951146\n",
            "Batch 146 Loss: 1.232287\n",
            "Batch 147 Loss: 0.603695\n",
            "Batch 148 Loss: 14.542354\n",
            "Batch 149 Loss: 1.459425\n",
            "Batch 150 Loss: 1.359349\n",
            "Batch 151 Loss: 0.329874\n",
            "Batch 152 Loss: 18.315838\n",
            "Epoch 6 Loss: 1.0528\n",
            "Max token ID: tensor(5, device='cuda:0')\n",
            "Embedding size: 6\n",
            "Output shape: torch.Size([4, 1539, 3])\n",
            "Batch 1 Loss: 1.112564\n",
            "Batch 2 Loss: 1.195073\n",
            "Batch 3 Loss: 1.672201\n",
            "Batch 4 Loss: 0.523678\n",
            "Batch 5 Loss: 0.325691\n",
            "Batch 6 Loss: 0.940998\n",
            "Batch 7 Loss: 0.458173\n",
            "Batch 8 Loss: 1.166874\n",
            "Batch 9 Loss: 2.428947\n",
            "Batch 10 Loss: 1.689988\n",
            "Batch 11 Loss: 0.987211\n",
            "Batch 12 Loss: 1.550195\n",
            "Batch 13 Loss: 1.659665\n",
            "Batch 14 Loss: 0.631822\n",
            "Batch 15 Loss: 1.207534\n",
            "Batch 16 Loss: 0.396856\n",
            "Batch 17 Loss: 0.349788\n",
            "Batch 18 Loss: 0.280888\n",
            "Batch 19 Loss: 0.681004\n",
            "Batch 20 Loss: 0.820169\n",
            "Batch 21 Loss: 0.284537\n",
            "Batch 22 Loss: 1.020169\n",
            "Batch 23 Loss: 0.321273\n",
            "Batch 24 Loss: 1.833993\n",
            "Batch 25 Loss: 0.357841\n",
            "Batch 26 Loss: 0.276230\n",
            "Batch 27 Loss: 2.084585\n",
            "Batch 28 Loss: 0.475605\n",
            "Batch 29 Loss: 0.268085\n",
            "Batch 30 Loss: 0.408545\n",
            "Batch 31 Loss: 0.374020\n",
            "Batch 32 Loss: 0.490305\n",
            "Batch 33 Loss: 1.265274\n",
            "Batch 34 Loss: 1.158104\n",
            "Batch 35 Loss: 1.183820\n",
            "Batch 36 Loss: 0.533100\n",
            "Batch 37 Loss: 0.309820\n",
            "Batch 38 Loss: 0.787539\n",
            "Batch 39 Loss: 0.603992\n",
            "Batch 40 Loss: 0.347299\n",
            "Batch 41 Loss: 1.617155\n",
            "Batch 42 Loss: 0.423490\n",
            "Batch 43 Loss: 0.656121\n",
            "Batch 44 Loss: 0.475629\n",
            "Batch 45 Loss: 0.544920\n",
            "Batch 46 Loss: 1.743605\n",
            "Batch 47 Loss: 2.567804\n",
            "Batch 48 Loss: 0.281444\n",
            "Batch 49 Loss: 0.892585\n",
            "Batch 50 Loss: 0.586308\n",
            "Batch 51 Loss: 0.469297\n",
            "Batch 52 Loss: 0.563232\n",
            "Batch 53 Loss: 0.484725\n",
            "Batch 54 Loss: 2.106452\n",
            "Batch 55 Loss: 0.649517\n",
            "Batch 56 Loss: 0.631789\n",
            "Batch 57 Loss: 1.076162\n",
            "Batch 58 Loss: 2.155114\n",
            "Batch 59 Loss: 0.362044\n",
            "Batch 60 Loss: 0.491187\n",
            "Batch 61 Loss: 0.301145\n",
            "Batch 62 Loss: 1.815577\n",
            "Batch 63 Loss: 0.378009\n",
            "Batch 64 Loss: 0.416368\n",
            "Batch 65 Loss: 3.754683\n",
            "Batch 66 Loss: 0.275740\n",
            "Batch 67 Loss: 2.037467\n",
            "Batch 68 Loss: 1.353322\n",
            "Batch 69 Loss: 0.416178\n",
            "Batch 70 Loss: 0.330844\n",
            "Batch 71 Loss: 1.976072\n",
            "Batch 72 Loss: 0.394939\n",
            "Batch 73 Loss: 1.021819\n",
            "Batch 74 Loss: 0.307686\n",
            "Batch 75 Loss: 2.033105\n",
            "Batch 76 Loss: 0.504839\n",
            "Batch 77 Loss: 1.178954\n",
            "Batch 78 Loss: 0.918415\n",
            "Batch 79 Loss: 0.441280\n",
            "Batch 80 Loss: 0.784508\n",
            "Batch 81 Loss: 0.765197\n",
            "Batch 82 Loss: 0.765838\n",
            "Batch 83 Loss: 0.271724\n",
            "Batch 84 Loss: 1.326012\n",
            "Batch 85 Loss: 0.431636\n",
            "Batch 86 Loss: 0.370420\n",
            "Batch 87 Loss: 0.556396\n",
            "Batch 88 Loss: 1.860354\n",
            "Batch 89 Loss: 1.692742\n",
            "Batch 90 Loss: 0.425526\n",
            "Batch 91 Loss: 0.282284\n",
            "Batch 92 Loss: 0.864156\n",
            "Batch 93 Loss: 0.413988\n",
            "Batch 94 Loss: 0.367317\n",
            "Batch 95 Loss: 3.302403\n",
            "Batch 96 Loss: 1.028034\n",
            "Batch 97 Loss: 0.755800\n",
            "Batch 98 Loss: 0.411468\n",
            "Batch 99 Loss: 1.240707\n",
            "Batch 100 Loss: 1.682318\n",
            "Batch 101 Loss: 0.595449\n",
            "Batch 102 Loss: 0.545901\n",
            "Batch 103 Loss: 0.362317\n",
            "Batch 104 Loss: 0.963923\n",
            "Batch 105 Loss: 0.315798\n",
            "Batch 106 Loss: 0.783546\n",
            "Batch 107 Loss: 0.472856\n",
            "Batch 108 Loss: 0.259212\n",
            "Batch 109 Loss: 0.509977\n",
            "Batch 110 Loss: 0.391267\n",
            "Batch 111 Loss: 0.434388\n",
            "Batch 112 Loss: 0.313170\n",
            "Batch 113 Loss: 0.329247\n",
            "Batch 114 Loss: 1.962815\n",
            "Batch 115 Loss: 0.694323\n",
            "Batch 116 Loss: 0.492208\n",
            "Batch 117 Loss: 0.971994\n",
            "Batch 118 Loss: 0.563918\n",
            "Batch 119 Loss: 0.309926\n",
            "Batch 120 Loss: 0.343793\n",
            "Batch 121 Loss: 0.887049\n",
            "Batch 122 Loss: 0.481142\n",
            "Batch 123 Loss: 0.439462\n",
            "Batch 124 Loss: 0.601186\n",
            "Batch 125 Loss: 0.277113\n",
            "Batch 126 Loss: 0.500721\n",
            "Batch 127 Loss: 1.636620\n",
            "Batch 128 Loss: 0.320190\n",
            "Batch 129 Loss: 0.944661\n",
            "Batch 130 Loss: 0.631642\n",
            "Batch 131 Loss: 0.451834\n",
            "Batch 132 Loss: 0.290057\n",
            "Batch 133 Loss: 0.358931\n",
            "Batch 134 Loss: 0.390352\n",
            "Batch 135 Loss: 0.985748\n",
            "Batch 136 Loss: 2.725597\n",
            "Batch 137 Loss: 0.629267\n",
            "Batch 138 Loss: 0.258785\n",
            "Batch 139 Loss: 0.297032\n",
            "Batch 140 Loss: 1.814021\n",
            "Batch 141 Loss: 0.677114\n",
            "Batch 142 Loss: 0.609612\n",
            "Batch 143 Loss: 0.831242\n",
            "Batch 144 Loss: 0.271739\n",
            "Batch 145 Loss: 0.868919\n",
            "Batch 146 Loss: 0.303786\n",
            "Batch 147 Loss: 12.962886\n",
            "Batch 148 Loss: 0.397376\n",
            "Batch 149 Loss: 1.959245\n",
            "Batch 150 Loss: 2.607765\n",
            "Batch 151 Loss: 0.307916\n",
            "Batch 152 Loss: 0.280901\n",
            "Epoch 7 Loss: 0.9297\n",
            "Max token ID: tensor(5, device='cuda:0')\n",
            "Embedding size: 6\n",
            "Output shape: torch.Size([4, 88, 3])\n",
            "Batch 1 Loss: 0.457291\n",
            "Batch 2 Loss: 0.904410\n",
            "Batch 3 Loss: 1.575488\n",
            "Batch 4 Loss: 0.383223\n",
            "Batch 5 Loss: 0.245744\n",
            "Batch 6 Loss: 0.536684\n",
            "Batch 7 Loss: 0.764925\n",
            "Batch 8 Loss: 1.446824\n",
            "Batch 9 Loss: 0.559543\n",
            "Batch 10 Loss: 1.784626\n",
            "Batch 11 Loss: 1.467849\n",
            "Batch 12 Loss: 1.406085\n",
            "Batch 13 Loss: 0.977817\n",
            "Batch 14 Loss: 1.703210\n",
            "Batch 15 Loss: 0.257487\n",
            "Batch 16 Loss: 0.428390\n",
            "Batch 17 Loss: 0.301185\n",
            "Batch 18 Loss: 0.596818\n",
            "Batch 19 Loss: 0.447165\n",
            "Batch 20 Loss: 0.556640\n",
            "Batch 21 Loss: 0.361077\n",
            "Batch 22 Loss: 0.576537\n",
            "Batch 23 Loss: 0.558141\n",
            "Batch 24 Loss: 0.302427\n",
            "Batch 25 Loss: 1.379003\n",
            "Batch 26 Loss: 0.815223\n",
            "Batch 27 Loss: 0.300107\n",
            "Batch 28 Loss: 0.304367\n",
            "Batch 29 Loss: 1.004548\n",
            "Batch 30 Loss: 1.211309\n",
            "Batch 31 Loss: 0.357650\n",
            "Batch 32 Loss: 0.723399\n",
            "Batch 33 Loss: 0.343280\n",
            "Batch 34 Loss: 0.943704\n",
            "Batch 35 Loss: 0.518048\n",
            "Batch 36 Loss: 2.249655\n",
            "Batch 37 Loss: 0.307912\n",
            "Batch 38 Loss: 0.410975\n",
            "Batch 39 Loss: 1.797301\n",
            "Batch 40 Loss: 1.239773\n",
            "Batch 41 Loss: 0.320426\n",
            "Batch 42 Loss: 0.389978\n",
            "Batch 43 Loss: 0.507653\n",
            "Batch 44 Loss: 0.649604\n",
            "Batch 45 Loss: 0.848106\n",
            "Batch 46 Loss: 0.513227\n",
            "Batch 47 Loss: 1.016056\n",
            "Batch 48 Loss: 0.391012\n",
            "Batch 49 Loss: 0.689517\n",
            "Batch 50 Loss: 0.927673\n",
            "Batch 51 Loss: 0.295878\n",
            "Batch 52 Loss: 0.632694\n",
            "Batch 53 Loss: 0.754292\n",
            "Batch 54 Loss: 1.030014\n",
            "Batch 55 Loss: 2.748194\n",
            "Batch 56 Loss: 0.370571\n",
            "Batch 57 Loss: 0.286088\n",
            "Batch 58 Loss: 0.372055\n",
            "Batch 59 Loss: 0.521786\n",
            "Batch 60 Loss: 0.273031\n",
            "Batch 61 Loss: 0.791771\n",
            "Batch 62 Loss: 0.340168\n",
            "Batch 63 Loss: 0.276641\n",
            "Batch 64 Loss: 1.973913\n",
            "Batch 65 Loss: 0.551360\n",
            "Batch 66 Loss: 0.334544\n",
            "Batch 67 Loss: 0.457867\n",
            "Batch 68 Loss: 1.304963\n",
            "Batch 69 Loss: 0.757763\n",
            "Batch 70 Loss: 0.690976\n",
            "Batch 71 Loss: 0.800243\n",
            "Batch 72 Loss: 1.133915\n",
            "Batch 73 Loss: 1.744452\n",
            "Batch 74 Loss: 0.993054\n",
            "Batch 75 Loss: 1.519330\n",
            "Batch 76 Loss: 0.362140\n",
            "Batch 77 Loss: 0.310746\n",
            "Batch 78 Loss: 0.458650\n",
            "Batch 79 Loss: 0.280803\n",
            "Batch 80 Loss: 3.770917\n",
            "Batch 81 Loss: 0.315235\n",
            "Batch 82 Loss: 1.202266\n",
            "Batch 83 Loss: 0.427636\n",
            "Batch 84 Loss: 0.642654\n",
            "Batch 85 Loss: 0.499891\n",
            "Batch 86 Loss: 0.627576\n",
            "Batch 87 Loss: 1.478379\n",
            "Batch 88 Loss: 0.316721\n",
            "Batch 89 Loss: 1.102589\n",
            "Batch 90 Loss: 1.366183\n",
            "Batch 91 Loss: 1.662869\n",
            "Batch 92 Loss: 0.300486\n",
            "Batch 93 Loss: 0.839303\n",
            "Batch 94 Loss: 1.130910\n",
            "Batch 95 Loss: 0.501129\n",
            "Batch 96 Loss: 0.299663\n",
            "Batch 97 Loss: 4.401099\n",
            "Batch 98 Loss: 1.531580\n",
            "Batch 99 Loss: 0.667349\n",
            "Batch 100 Loss: 0.479471\n",
            "Batch 101 Loss: 1.685488\n",
            "Batch 102 Loss: 0.494339\n",
            "Batch 103 Loss: 0.556643\n",
            "Batch 104 Loss: 0.382447\n",
            "Batch 105 Loss: 0.700000\n",
            "Batch 106 Loss: 0.404781\n",
            "Batch 107 Loss: 0.254539\n",
            "Batch 108 Loss: 0.343125\n",
            "Batch 109 Loss: 1.329419\n",
            "Batch 110 Loss: 0.351816\n",
            "Batch 111 Loss: 1.724923\n",
            "Batch 112 Loss: 1.245337\n",
            "Batch 113 Loss: 0.321377\n",
            "Batch 114 Loss: 0.549714\n",
            "Batch 115 Loss: 0.278058\n",
            "Batch 116 Loss: 0.659512\n",
            "Batch 117 Loss: 0.287950\n",
            "Batch 118 Loss: 0.314579\n",
            "Batch 119 Loss: 0.428486\n",
            "Batch 120 Loss: 0.267284\n",
            "Batch 121 Loss: 1.582363\n",
            "Batch 122 Loss: 1.012648\n",
            "Batch 123 Loss: 0.846928\n",
            "Batch 124 Loss: 0.283523\n",
            "Batch 125 Loss: 0.508331\n",
            "Batch 126 Loss: 0.405692\n",
            "Batch 127 Loss: 0.370492\n",
            "Batch 128 Loss: 0.414485\n",
            "Batch 129 Loss: 0.265548\n",
            "Batch 130 Loss: 2.403097\n",
            "Batch 131 Loss: 8.647088\n",
            "Batch 132 Loss: 0.291828\n",
            "Batch 133 Loss: 0.471504\n",
            "Batch 134 Loss: 1.541655\n",
            "Batch 135 Loss: 0.300092\n",
            "Batch 136 Loss: 0.629540\n",
            "Batch 137 Loss: 0.293729\n",
            "Batch 138 Loss: 0.287409\n",
            "Batch 139 Loss: 1.470126\n",
            "Batch 140 Loss: 0.382955\n",
            "Batch 141 Loss: 0.315769\n",
            "Batch 142 Loss: 0.684902\n",
            "Batch 143 Loss: 1.448638\n",
            "Batch 144 Loss: 0.452400\n",
            "Batch 145 Loss: 0.422119\n",
            "Batch 146 Loss: 0.267322\n",
            "Batch 147 Loss: 0.569692\n",
            "Batch 148 Loss: 0.302509\n",
            "Batch 149 Loss: 0.287123\n",
            "Batch 150 Loss: 0.437911\n",
            "Batch 151 Loss: 0.335165\n",
            "Batch 152 Loss: 0.246598\n",
            "Epoch 8 Loss: 0.8253\n",
            "Max token ID: tensor(5, device='cuda:0')\n",
            "Embedding size: 6\n",
            "Output shape: torch.Size([4, 90, 3])\n",
            "Batch 1 Loss: 0.716401\n",
            "Batch 2 Loss: 1.411492\n",
            "Batch 3 Loss: 0.317105\n",
            "Batch 4 Loss: 0.583170\n",
            "Batch 5 Loss: 0.214954\n",
            "Batch 6 Loss: 0.354716\n",
            "Batch 7 Loss: 0.405441\n",
            "Batch 8 Loss: 0.326703\n",
            "Batch 9 Loss: 1.736111\n",
            "Batch 10 Loss: 0.371045\n",
            "Batch 11 Loss: 0.232401\n",
            "Batch 12 Loss: 0.508262\n",
            "Batch 13 Loss: 0.317143\n",
            "Batch 14 Loss: 0.368400\n",
            "Batch 15 Loss: 2.654985\n",
            "Batch 16 Loss: 1.593155\n",
            "Batch 17 Loss: 0.431089\n",
            "Batch 18 Loss: 0.285435\n",
            "Batch 19 Loss: 0.647166\n",
            "Batch 20 Loss: 1.087258\n",
            "Batch 21 Loss: 1.395176\n",
            "Batch 22 Loss: 0.270112\n",
            "Batch 23 Loss: 0.380258\n",
            "Batch 24 Loss: 0.239215\n",
            "Batch 25 Loss: 1.154887\n",
            "Batch 26 Loss: 0.564240\n",
            "Batch 27 Loss: 1.566567\n",
            "Batch 28 Loss: 0.505970\n",
            "Batch 29 Loss: 0.444599\n",
            "Batch 30 Loss: 3.175035\n",
            "Batch 31 Loss: 1.753693\n",
            "Batch 32 Loss: 0.834866\n",
            "Batch 33 Loss: 0.880637\n",
            "Batch 34 Loss: 0.245212\n",
            "Batch 35 Loss: 0.386537\n",
            "Batch 36 Loss: 0.286364\n",
            "Batch 37 Loss: 1.711886\n",
            "Batch 38 Loss: 0.402188\n",
            "Batch 39 Loss: 0.679885\n",
            "Batch 40 Loss: 1.399757\n",
            "Batch 41 Loss: 0.321021\n",
            "Batch 42 Loss: 0.268905\n",
            "Batch 43 Loss: 2.686625\n",
            "Batch 44 Loss: 0.429563\n",
            "Batch 45 Loss: 0.242657\n",
            "Batch 46 Loss: 0.248412\n",
            "Batch 47 Loss: 0.406695\n",
            "Batch 48 Loss: 0.525242\n",
            "Batch 49 Loss: 2.213985\n",
            "Batch 50 Loss: 0.325398\n",
            "Batch 51 Loss: 0.440059\n",
            "Batch 52 Loss: 0.402456\n",
            "Batch 53 Loss: 1.008232\n",
            "Batch 54 Loss: 1.776272\n",
            "Batch 55 Loss: 0.384666\n",
            "Batch 56 Loss: 0.389883\n",
            "Batch 57 Loss: 0.327957\n",
            "Batch 58 Loss: 0.301346\n",
            "Batch 59 Loss: 1.588936\n",
            "Batch 60 Loss: 0.263685\n",
            "Batch 61 Loss: 2.787472\n",
            "Batch 62 Loss: 0.572211\n",
            "Batch 63 Loss: 0.293811\n",
            "Batch 64 Loss: 0.479005\n",
            "Batch 65 Loss: 1.254016\n",
            "Batch 66 Loss: 0.736419\n",
            "Batch 67 Loss: 0.289751\n",
            "Batch 68 Loss: 1.334502\n",
            "Batch 69 Loss: 1.274365\n",
            "Batch 70 Loss: 0.703551\n",
            "Batch 71 Loss: 1.239168\n",
            "Batch 72 Loss: 0.818378\n",
            "Batch 73 Loss: 2.351934\n",
            "Batch 74 Loss: 2.117621\n",
            "Batch 75 Loss: 0.359803\n",
            "Batch 76 Loss: 0.632145\n",
            "Batch 77 Loss: 0.233909\n",
            "Batch 78 Loss: 0.380751\n",
            "Batch 79 Loss: 0.297067\n",
            "Batch 80 Loss: 2.388937\n",
            "Batch 81 Loss: 0.408794\n",
            "Batch 82 Loss: 1.062181\n",
            "Batch 83 Loss: 1.036955\n",
            "Batch 84 Loss: 0.288151\n",
            "Batch 85 Loss: 0.557195\n",
            "Batch 86 Loss: 0.360795\n",
            "Batch 87 Loss: 0.998546\n",
            "Batch 88 Loss: 0.323265\n",
            "Batch 89 Loss: 0.792870\n",
            "Batch 90 Loss: 1.329328\n",
            "Batch 91 Loss: 0.259023\n",
            "Batch 92 Loss: 1.142426\n",
            "Batch 93 Loss: 0.432721\n",
            "Batch 94 Loss: 0.224510\n",
            "Batch 95 Loss: 7.210101\n",
            "Batch 96 Loss: 0.731553\n",
            "Batch 97 Loss: 0.386981\n",
            "Batch 98 Loss: 1.467703\n",
            "Batch 99 Loss: 1.576035\n",
            "Batch 100 Loss: 1.075298\n",
            "Batch 101 Loss: 0.975039\n",
            "Batch 102 Loss: 0.329538\n",
            "Batch 103 Loss: 1.713356\n",
            "Batch 104 Loss: 0.281981\n",
            "Batch 105 Loss: 1.399350\n",
            "Batch 106 Loss: 1.300562\n",
            "Batch 107 Loss: 0.483674\n",
            "Batch 108 Loss: 0.311789\n",
            "Batch 109 Loss: 0.324300\n",
            "Batch 110 Loss: 1.308778\n",
            "Batch 111 Loss: 0.436143\n",
            "Batch 112 Loss: 1.732860\n",
            "Batch 113 Loss: 1.983398\n",
            "Batch 114 Loss: 0.375819\n",
            "Batch 115 Loss: 1.027353\n",
            "Batch 116 Loss: 0.367272\n",
            "Batch 117 Loss: 0.255353\n",
            "Batch 118 Loss: 0.472845\n",
            "Batch 119 Loss: 0.581109\n",
            "Batch 120 Loss: 0.348955\n",
            "Batch 121 Loss: 0.291155\n",
            "Batch 122 Loss: 1.259678\n",
            "Batch 123 Loss: 2.793795\n",
            "Batch 124 Loss: 0.353665\n",
            "Batch 125 Loss: 1.340316\n",
            "Batch 126 Loss: 0.267881\n",
            "Batch 127 Loss: 2.188488\n",
            "Batch 128 Loss: 0.637505\n",
            "Batch 129 Loss: 0.308023\n",
            "Batch 130 Loss: 0.258661\n",
            "Batch 131 Loss: 1.817721\n",
            "Batch 132 Loss: 0.320737\n",
            "Batch 133 Loss: 0.651794\n",
            "Batch 134 Loss: 1.167928\n",
            "Batch 135 Loss: 1.073525\n",
            "Batch 136 Loss: 0.311912\n",
            "Batch 137 Loss: 0.916155\n",
            "Batch 138 Loss: 0.446342\n",
            "Batch 139 Loss: 1.428702\n",
            "Batch 140 Loss: 1.081865\n",
            "Batch 141 Loss: 0.332991\n",
            "Batch 142 Loss: 0.649908\n",
            "Batch 143 Loss: 0.574062\n",
            "Batch 144 Loss: 0.377635\n",
            "Batch 145 Loss: 0.331896\n",
            "Batch 146 Loss: 1.109025\n",
            "Batch 147 Loss: 0.388689\n",
            "Batch 148 Loss: 0.245586\n",
            "Batch 149 Loss: 0.527870\n",
            "Batch 150 Loss: 0.796695\n",
            "Batch 151 Loss: 1.684795\n",
            "Batch 152 Loss: 14.816068\n",
            "Epoch 9 Loss: 0.9629\n",
            "Max token ID: tensor(5, device='cuda:0')\n",
            "Embedding size: 6\n",
            "Output shape: torch.Size([4, 1504, 3])\n",
            "Batch 1 Loss: 2.745303\n",
            "Batch 2 Loss: 0.483219\n",
            "Batch 3 Loss: 0.490042\n",
            "Batch 4 Loss: 0.999076\n",
            "Batch 5 Loss: 0.246365\n",
            "Batch 6 Loss: 1.260065\n",
            "Batch 7 Loss: 2.087857\n",
            "Batch 8 Loss: 0.307125\n",
            "Batch 9 Loss: 0.540540\n",
            "Batch 10 Loss: 0.383502\n",
            "Batch 11 Loss: 2.133991\n",
            "Batch 12 Loss: 0.278497\n",
            "Batch 13 Loss: 0.615695\n",
            "Batch 14 Loss: 0.253056\n",
            "Batch 15 Loss: 0.248462\n",
            "Batch 16 Loss: 0.253039\n",
            "Batch 17 Loss: 0.372236\n",
            "Batch 18 Loss: 0.554501\n",
            "Batch 19 Loss: 1.275032\n",
            "Batch 20 Loss: 0.578726\n",
            "Batch 21 Loss: 1.983268\n",
            "Batch 22 Loss: 0.306162\n",
            "Batch 23 Loss: 0.353540\n",
            "Batch 24 Loss: 0.246335\n",
            "Batch 25 Loss: 0.318200\n",
            "Batch 26 Loss: 0.274187\n",
            "Batch 27 Loss: 0.317527\n",
            "Batch 28 Loss: 0.847139\n",
            "Batch 29 Loss: 0.903817\n",
            "Batch 30 Loss: 0.255120\n",
            "Batch 31 Loss: 1.259838\n",
            "Batch 32 Loss: 0.295277\n",
            "Batch 33 Loss: 0.690293\n",
            "Batch 34 Loss: 0.681514\n",
            "Batch 35 Loss: 0.278368\n",
            "Batch 36 Loss: 0.459564\n",
            "Batch 37 Loss: 0.285050\n",
            "Batch 38 Loss: 0.244744\n",
            "Batch 39 Loss: 0.876829\n",
            "Batch 40 Loss: 0.513347\n",
            "Batch 41 Loss: 0.265523\n",
            "Batch 42 Loss: 0.249306\n",
            "Batch 43 Loss: 0.345200\n",
            "Batch 44 Loss: 0.601742\n",
            "Batch 45 Loss: 1.335668\n",
            "Batch 46 Loss: 0.186733\n",
            "Batch 47 Loss: 13.993780\n",
            "Batch 48 Loss: 3.073291\n",
            "Batch 49 Loss: 0.535814\n",
            "Batch 50 Loss: 0.295669\n",
            "Batch 51 Loss: 0.635590\n",
            "Batch 52 Loss: 2.067282\n",
            "Batch 53 Loss: 1.658272\n",
            "Batch 54 Loss: 0.337065\n",
            "Batch 55 Loss: 0.439668\n",
            "Batch 56 Loss: 0.423774\n",
            "Batch 57 Loss: 0.295891\n",
            "Batch 58 Loss: 1.025352\n",
            "Batch 59 Loss: 1.160523\n",
            "Batch 60 Loss: 1.335407\n",
            "Batch 61 Loss: 1.318026\n",
            "Batch 62 Loss: 0.934539\n",
            "Batch 63 Loss: 0.261511\n",
            "Batch 64 Loss: 0.258744\n",
            "Batch 65 Loss: 0.391352\n",
            "Batch 66 Loss: 1.786487\n",
            "Batch 67 Loss: 0.219224\n",
            "Batch 68 Loss: 0.243717\n",
            "Batch 69 Loss: 0.439129\n",
            "Batch 70 Loss: 0.996225\n",
            "Batch 71 Loss: 1.314643\n",
            "Batch 72 Loss: 1.160486\n",
            "Batch 73 Loss: 0.498558\n",
            "Batch 74 Loss: 1.087961\n",
            "Batch 75 Loss: 0.298198\n",
            "Batch 76 Loss: 3.463282\n",
            "Batch 77 Loss: 0.296743\n",
            "Batch 78 Loss: 0.273621\n",
            "Batch 79 Loss: 0.516925\n",
            "Batch 80 Loss: 0.472923\n",
            "Batch 81 Loss: 0.530437\n",
            "Batch 82 Loss: 0.389170\n",
            "Batch 83 Loss: 0.265471\n",
            "Batch 84 Loss: 0.474003\n",
            "Batch 85 Loss: 1.726469\n",
            "Batch 86 Loss: 1.157802\n",
            "Batch 87 Loss: 1.485408\n",
            "Batch 88 Loss: 0.542564\n",
            "Batch 89 Loss: 1.479373\n",
            "Batch 90 Loss: 1.886991\n",
            "Batch 91 Loss: 0.263355\n",
            "Batch 92 Loss: 0.416636\n",
            "Batch 93 Loss: 0.437826\n",
            "Batch 94 Loss: 0.667321\n",
            "Batch 95 Loss: 1.035414\n",
            "Batch 96 Loss: 1.713053\n",
            "Batch 97 Loss: 0.347596\n",
            "Batch 98 Loss: 0.340877\n",
            "Batch 99 Loss: 0.767107\n",
            "Batch 100 Loss: 0.254750\n",
            "Batch 101 Loss: 1.766215\n",
            "Batch 102 Loss: 0.917605\n",
            "Batch 103 Loss: 1.057988\n",
            "Batch 104 Loss: 1.620319\n",
            "Batch 105 Loss: 0.480238\n",
            "Batch 106 Loss: 0.909299\n",
            "Batch 107 Loss: 2.748950\n",
            "Batch 108 Loss: 0.576985\n",
            "Batch 109 Loss: 0.301371\n",
            "Batch 110 Loss: 0.522516\n",
            "Batch 111 Loss: 0.837817\n",
            "Batch 112 Loss: 2.788607\n",
            "Batch 113 Loss: 0.547173\n",
            "Batch 114 Loss: 0.389339\n",
            "Batch 115 Loss: 0.691393\n",
            "Batch 116 Loss: 0.917885\n",
            "Batch 117 Loss: 1.431400\n",
            "Batch 118 Loss: 0.241155\n",
            "Batch 119 Loss: 0.912698\n",
            "Batch 120 Loss: 0.584587\n",
            "Batch 121 Loss: 0.855112\n",
            "Batch 122 Loss: 0.429349\n",
            "Batch 123 Loss: 2.060787\n",
            "Batch 124 Loss: 0.254858\n",
            "Batch 125 Loss: 0.344886\n",
            "Batch 126 Loss: 3.163950\n",
            "Batch 127 Loss: 0.714781\n",
            "Batch 128 Loss: 1.373721\n",
            "Batch 129 Loss: 0.218865\n",
            "Batch 130 Loss: 0.348649\n",
            "Batch 131 Loss: 0.543327\n",
            "Batch 132 Loss: 0.273735\n",
            "Batch 133 Loss: 0.407522\n",
            "Batch 134 Loss: 0.339423\n",
            "Batch 135 Loss: 0.457840\n",
            "Batch 136 Loss: 0.774410\n",
            "Batch 137 Loss: 0.986566\n",
            "Batch 138 Loss: 0.690138\n",
            "Batch 139 Loss: 1.767175\n",
            "Batch 140 Loss: 0.337948\n",
            "Batch 141 Loss: 1.527215\n",
            "Batch 142 Loss: 0.233516\n",
            "Batch 143 Loss: 1.617473\n",
            "Batch 144 Loss: 0.268981\n",
            "Batch 145 Loss: 0.249826\n",
            "Batch 146 Loss: 1.294198\n",
            "Batch 147 Loss: 0.367057\n",
            "Batch 148 Loss: 2.129516\n",
            "Batch 149 Loss: 0.258895\n",
            "Batch 150 Loss: 0.824350\n",
            "Batch 151 Loss: 1.715351\n",
            "Batch 152 Loss: 0.344024\n",
            "Epoch 10 Loss: 0.9052\n"
          ]
        }
      ],
      "source": [
        "dataset = RNADataset(\"./data/train_sequences.csv\", \"./data/train_labels.csv\") # replace with your *actual* path\n",
        "loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=train_collate_fn)\n",
        "\n",
        "\n",
        "model = RNA3DFoldPredictor(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    embed_size=64,\n",
        "    num_layers=4,\n",
        "    heads=4,\n",
        "    forward_expansion=4,\n",
        "    dropout=0.2,\n",
        "    max_length=4298, # nearest multiple of 2 is 8192...actual max is 4298\n",
        ").to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    batch_num = 0\n",
        "\n",
        "    test_batch = next(iter(loader))\n",
        "    seqs, coords, mask = [x.to(device) for x in test_batch]\n",
        "\n",
        "    print(\"Max token ID:\", torch.max(seqs))  # Should be <= 3\n",
        "    print(\"Embedding size:\", model.token_embedding.num_embeddings)  # Should be 4\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(seqs, mask)\n",
        "    print(\"Output shape:\", outputs.shape)\n",
        "\n",
        "    for seqs, coords, mask in loader:\n",
        "        batch_num += 1\n",
        "        # print(\"Max token ID in batch:\", torch.max(seqs))\n",
        "        seqs, coords, mask_attention = seqs.to(device), coords.to(device), mask.to(device)\n",
        "\n",
        "        # check for any NaN values\n",
        "        if torch.isnan(coords).any() or torch.isinf(coords).any():\n",
        "            print(f\"WARNING: NaN/Inf found in target coordinates in batch {batch_num}! Skipping batch.\")\n",
        "            continue\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(seqs, mask_attention)\n",
        "\n",
        "\n",
        "        if torch.isnan(outputs).any() or torch.isinf(outputs).any():\n",
        "             print(f\"WARNING: NaN/Inf found in model outputs BEFORE loss calculation in batch {batch_num}!\")\n",
        "\n",
        "        non_pad_mask = (seqs != PAD_IDX) # Shape: (batch_size, seq_len)\n",
        "\n",
        "        # Flatten outputs and coords, then apply the mask\n",
        "        outputs_flat = outputs.view(-1, 3) # Shape: (batch * seq_len, 3)\n",
        "        coords_flat = coords.view(-1, 3)   # Shape: (batch * seq_len, 3)\n",
        "        non_pad_mask_flat = non_pad_mask.view(-1) # Shape: (batch * seq_len)\n",
        "\n",
        "        outputs_masked = outputs_flat[non_pad_mask_flat]\n",
        "        coords_masked = coords_flat[non_pad_mask_flat]\n",
        "\n",
        "        # Calculate loss ONLY on non-padded elements\n",
        "        if outputs_masked.nelement() > 0: # Check if there are any non-padded elements\n",
        "            loss = criterion(outputs_masked, coords_masked)\n",
        "\n",
        "            if torch.isnan(loss):\n",
        "               print(f\"WARNING: NaN detected in loss for batch {batch_num}!\")\n",
        "               # Add more debugging here if needed: print outputs_masked, coords_masked\n",
        "               continue # Skip optimization step for this batch\n",
        "\n",
        "            print(f\"Batch {batch_num} Loss: {loss.item():.6f}\") # Print loss *before* backward\n",
        "\n",
        "            loss.backward()\n",
        "            # Gradient Clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            print(f\"Skipping batch {batch_num} due to only padding elements.\")\n",
        "\n",
        "    # Avoid division by zero if loader is empty or all batches were skipped\n",
        "    if len(loader) > 0 and total_loss > 0:\n",
        "         print(f\"Epoch {epoch+1} Loss: {total_loss / len(loader):.4f}\") # Or divide by number of valid batches processed\n",
        "    else:\n",
        "         print(f\"Epoch {epoch+1} had no valid batches or zero total loss.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3leZVKtsrM2A"
      },
      "outputs": [],
      "source": [
        "model_path = './model.pth'\n",
        "torch.save(model.state_dict(), model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1snHuR1orM2A"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qhALdm0GrM2A"
      },
      "outputs": [],
      "source": [
        "IDX_TO_NUC = {v: k for k, v in NUC_TO_IDX.items()}\n",
        "# Testing dataset\n",
        "class RNATestDataset(Dataset):\n",
        "    def __init__(self, test_seq_path):\n",
        "        # Read test csv\n",
        "        self.data = pd.read_csv(test_seq_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        seq_id = row[\"target_id\"]\n",
        "        sequence = row[\"sequence\"]\n",
        "\n",
        "        token_ids = [NUC_TO_IDX.get(nuc, NUC_TO_IDX[\"N\"]) for nuc in sequence]\n",
        "        token_ids = torch.tensor(token_ids, dtype=torch.long)\n",
        "\n",
        "        return seq_id, token_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "w2IlbgfCrM2B"
      },
      "outputs": [],
      "source": [
        "test_dataset = RNATestDataset(test_seq_path='./data/test_sequences.csv')\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "model.load_state_dict(torch.load('model.pth')) # load trained model\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# enable dropout\n",
        "for m in model.modules():\n",
        "        if m.__class__.__name__.startswith('Dropout'):\n",
        "            m.train()\n",
        "\n",
        "submission_rows = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for seq_id, token_ids in test_loader:\n",
        "        seq_id = seq_id[0]  # unpack from list\n",
        "        token_ids = token_ids.to(device).squeeze(0)  # [seq_len]\n",
        "        sequence = [IDX_TO_NUC[i.item()] for i in token_ids]\n",
        "\n",
        "        predictions = []\n",
        "\n",
        "        # Generate 5 predictions\n",
        "        for _ in range(5):\n",
        "            output = model(token_ids.unsqueeze(0))  # [1, seq_len, 3]\n",
        "            coords = output.squeeze(0).cpu().numpy()  # [seq_len, 3]\n",
        "            predictions.append(coords)\n",
        "\n",
        "        predictions = np.stack(predictions, axis=0)  # [5, seq_len, 3]\n",
        "\n",
        "        seq_len = len(sequence)\n",
        "\n",
        "        # Loop over each nucleotide in the sequence\n",
        "        for i in range(seq_len):\n",
        "            row = {\n",
        "                \"ID\": f\"{seq_id}_{i+1}\",\n",
        "                \"resname\": sequence[i],\n",
        "                \"resid\": i+1\n",
        "            }\n",
        "            for j in range(5):  # 5 predictions\n",
        "                row[f\"x_{j+1}\"] = predictions[j, i, 0]\n",
        "                row[f\"y_{j+1}\"] = predictions[j, i, 1]\n",
        "                row[f\"z_{j+1}\"] = predictions[j, i, 2]\n",
        "\n",
        "            submission_rows.append(row)\n",
        "\n",
        "# Convert to DataFrame and save\n",
        "submission_df = pd.DataFrame(submission_rows)\n",
        "submission_df.to_csv(\"submission.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(pd.read_csv('submission.csv'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "tCV4JDZnu9sa",
        "outputId": "4f21abe9-d59f-4350-e441-c1fe193ce368"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "             ID resname  resid       x_1       y_1       z_1       x_2  \\\n",
              "0       R1107_1       G      1 -0.395682  0.026159 -0.460790 -0.494894   \n",
              "1       R1107_2       G      2 -0.391142 -0.348778 -0.625145 -1.039486   \n",
              "2       R1107_3       G      3 -0.406647  0.237234 -0.645078 -0.472471   \n",
              "3       R1107_4       G      4 -0.255734 -0.193740 -0.450975 -1.073963   \n",
              "4       R1107_5       G      5 -0.275882 -0.376889 -0.640550  0.141876   \n",
              "...         ...     ...    ...       ...       ...       ...       ...   \n",
              "2510  R1190_114       U    114  0.373998 -0.722683 -0.175997 -0.131089   \n",
              "2511  R1190_115       U    115 -0.256396 -0.835813 -0.537425 -0.499511   \n",
              "2512  R1190_116       U    116  0.756600 -0.402746 -0.086196 -0.124368   \n",
              "2513  R1190_117       U    117 -0.372658 -0.214195 -0.348880 -0.585410   \n",
              "2514  R1190_118       U    118  0.279265  0.050161 -0.539621  0.003016   \n",
              "\n",
              "           y_2       z_2       x_3       y_3       z_3       x_4       y_4  \\\n",
              "0    -0.650087 -0.418244 -0.416741  0.150414 -0.912710 -0.010037 -0.118914   \n",
              "1    -0.452833 -0.256414 -0.282264 -0.324641 -0.428678  0.153570 -0.090237   \n",
              "2    -0.608401 -0.907962 -0.309058 -0.521215 -0.297594  0.273179 -0.576949   \n",
              "3    -0.451119 -0.545389 -0.265850 -0.288817 -0.768573 -0.674212  0.003626   \n",
              "4    -0.380515 -0.370196 -0.239424 -0.610117 -0.164057 -1.026388  0.407131   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "2510 -0.624439 -0.328461 -0.198958 -0.287816  0.287879  0.217500 -1.008544   \n",
              "2511 -0.407664 -0.626575 -0.321698 -0.102411 -0.289411 -0.508839 -0.105264   \n",
              "2512 -0.887070 -0.260656 -0.721631 -0.328267 -0.258278 -0.448919 -0.096239   \n",
              "2513 -0.147604 -0.384459 -0.355588 -0.096559 -0.569185  0.057323 -0.432629   \n",
              "2514 -0.289835  0.492322  0.356809 -1.528315 -0.244226 -0.796396 -0.094822   \n",
              "\n",
              "           z_4       x_5       y_5       z_5  \n",
              "0    -0.552068 -0.288310 -0.017140 -0.589925  \n",
              "1    -0.447006 -0.015217 -0.057593 -0.473987  \n",
              "2    -0.514920 -0.529903 -0.336794 -0.105902  \n",
              "3    -0.213552  0.183197 -0.327977 -0.753004  \n",
              "4    -0.269336 -0.650812 -1.035031 -0.041577  \n",
              "...        ...       ...       ...       ...  \n",
              "2510 -0.449152 -0.742779 -0.390416 -0.336146  \n",
              "2511  0.337541 -0.595533  0.091612 -0.496264  \n",
              "2512 -0.546463 -1.014522 -0.884052 -0.059010  \n",
              "2513 -0.723334 -0.958681  0.019751 -0.448138  \n",
              "2514 -0.306418  0.193533 -0.684156 -0.279959  \n",
              "\n",
              "[2515 rows x 18 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8051bb49-ac00-45f4-bbdb-e79229d58655\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>resname</th>\n",
              "      <th>resid</th>\n",
              "      <th>x_1</th>\n",
              "      <th>y_1</th>\n",
              "      <th>z_1</th>\n",
              "      <th>x_2</th>\n",
              "      <th>y_2</th>\n",
              "      <th>z_2</th>\n",
              "      <th>x_3</th>\n",
              "      <th>y_3</th>\n",
              "      <th>z_3</th>\n",
              "      <th>x_4</th>\n",
              "      <th>y_4</th>\n",
              "      <th>z_4</th>\n",
              "      <th>x_5</th>\n",
              "      <th>y_5</th>\n",
              "      <th>z_5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>R1107_1</td>\n",
              "      <td>G</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.395682</td>\n",
              "      <td>0.026159</td>\n",
              "      <td>-0.460790</td>\n",
              "      <td>-0.494894</td>\n",
              "      <td>-0.650087</td>\n",
              "      <td>-0.418244</td>\n",
              "      <td>-0.416741</td>\n",
              "      <td>0.150414</td>\n",
              "      <td>-0.912710</td>\n",
              "      <td>-0.010037</td>\n",
              "      <td>-0.118914</td>\n",
              "      <td>-0.552068</td>\n",
              "      <td>-0.288310</td>\n",
              "      <td>-0.017140</td>\n",
              "      <td>-0.589925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>R1107_2</td>\n",
              "      <td>G</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.391142</td>\n",
              "      <td>-0.348778</td>\n",
              "      <td>-0.625145</td>\n",
              "      <td>-1.039486</td>\n",
              "      <td>-0.452833</td>\n",
              "      <td>-0.256414</td>\n",
              "      <td>-0.282264</td>\n",
              "      <td>-0.324641</td>\n",
              "      <td>-0.428678</td>\n",
              "      <td>0.153570</td>\n",
              "      <td>-0.090237</td>\n",
              "      <td>-0.447006</td>\n",
              "      <td>-0.015217</td>\n",
              "      <td>-0.057593</td>\n",
              "      <td>-0.473987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>R1107_3</td>\n",
              "      <td>G</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.406647</td>\n",
              "      <td>0.237234</td>\n",
              "      <td>-0.645078</td>\n",
              "      <td>-0.472471</td>\n",
              "      <td>-0.608401</td>\n",
              "      <td>-0.907962</td>\n",
              "      <td>-0.309058</td>\n",
              "      <td>-0.521215</td>\n",
              "      <td>-0.297594</td>\n",
              "      <td>0.273179</td>\n",
              "      <td>-0.576949</td>\n",
              "      <td>-0.514920</td>\n",
              "      <td>-0.529903</td>\n",
              "      <td>-0.336794</td>\n",
              "      <td>-0.105902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>R1107_4</td>\n",
              "      <td>G</td>\n",
              "      <td>4</td>\n",
              "      <td>-0.255734</td>\n",
              "      <td>-0.193740</td>\n",
              "      <td>-0.450975</td>\n",
              "      <td>-1.073963</td>\n",
              "      <td>-0.451119</td>\n",
              "      <td>-0.545389</td>\n",
              "      <td>-0.265850</td>\n",
              "      <td>-0.288817</td>\n",
              "      <td>-0.768573</td>\n",
              "      <td>-0.674212</td>\n",
              "      <td>0.003626</td>\n",
              "      <td>-0.213552</td>\n",
              "      <td>0.183197</td>\n",
              "      <td>-0.327977</td>\n",
              "      <td>-0.753004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>R1107_5</td>\n",
              "      <td>G</td>\n",
              "      <td>5</td>\n",
              "      <td>-0.275882</td>\n",
              "      <td>-0.376889</td>\n",
              "      <td>-0.640550</td>\n",
              "      <td>0.141876</td>\n",
              "      <td>-0.380515</td>\n",
              "      <td>-0.370196</td>\n",
              "      <td>-0.239424</td>\n",
              "      <td>-0.610117</td>\n",
              "      <td>-0.164057</td>\n",
              "      <td>-1.026388</td>\n",
              "      <td>0.407131</td>\n",
              "      <td>-0.269336</td>\n",
              "      <td>-0.650812</td>\n",
              "      <td>-1.035031</td>\n",
              "      <td>-0.041577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2510</th>\n",
              "      <td>R1190_114</td>\n",
              "      <td>U</td>\n",
              "      <td>114</td>\n",
              "      <td>0.373998</td>\n",
              "      <td>-0.722683</td>\n",
              "      <td>-0.175997</td>\n",
              "      <td>-0.131089</td>\n",
              "      <td>-0.624439</td>\n",
              "      <td>-0.328461</td>\n",
              "      <td>-0.198958</td>\n",
              "      <td>-0.287816</td>\n",
              "      <td>0.287879</td>\n",
              "      <td>0.217500</td>\n",
              "      <td>-1.008544</td>\n",
              "      <td>-0.449152</td>\n",
              "      <td>-0.742779</td>\n",
              "      <td>-0.390416</td>\n",
              "      <td>-0.336146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2511</th>\n",
              "      <td>R1190_115</td>\n",
              "      <td>U</td>\n",
              "      <td>115</td>\n",
              "      <td>-0.256396</td>\n",
              "      <td>-0.835813</td>\n",
              "      <td>-0.537425</td>\n",
              "      <td>-0.499511</td>\n",
              "      <td>-0.407664</td>\n",
              "      <td>-0.626575</td>\n",
              "      <td>-0.321698</td>\n",
              "      <td>-0.102411</td>\n",
              "      <td>-0.289411</td>\n",
              "      <td>-0.508839</td>\n",
              "      <td>-0.105264</td>\n",
              "      <td>0.337541</td>\n",
              "      <td>-0.595533</td>\n",
              "      <td>0.091612</td>\n",
              "      <td>-0.496264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2512</th>\n",
              "      <td>R1190_116</td>\n",
              "      <td>U</td>\n",
              "      <td>116</td>\n",
              "      <td>0.756600</td>\n",
              "      <td>-0.402746</td>\n",
              "      <td>-0.086196</td>\n",
              "      <td>-0.124368</td>\n",
              "      <td>-0.887070</td>\n",
              "      <td>-0.260656</td>\n",
              "      <td>-0.721631</td>\n",
              "      <td>-0.328267</td>\n",
              "      <td>-0.258278</td>\n",
              "      <td>-0.448919</td>\n",
              "      <td>-0.096239</td>\n",
              "      <td>-0.546463</td>\n",
              "      <td>-1.014522</td>\n",
              "      <td>-0.884052</td>\n",
              "      <td>-0.059010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2513</th>\n",
              "      <td>R1190_117</td>\n",
              "      <td>U</td>\n",
              "      <td>117</td>\n",
              "      <td>-0.372658</td>\n",
              "      <td>-0.214195</td>\n",
              "      <td>-0.348880</td>\n",
              "      <td>-0.585410</td>\n",
              "      <td>-0.147604</td>\n",
              "      <td>-0.384459</td>\n",
              "      <td>-0.355588</td>\n",
              "      <td>-0.096559</td>\n",
              "      <td>-0.569185</td>\n",
              "      <td>0.057323</td>\n",
              "      <td>-0.432629</td>\n",
              "      <td>-0.723334</td>\n",
              "      <td>-0.958681</td>\n",
              "      <td>0.019751</td>\n",
              "      <td>-0.448138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2514</th>\n",
              "      <td>R1190_118</td>\n",
              "      <td>U</td>\n",
              "      <td>118</td>\n",
              "      <td>0.279265</td>\n",
              "      <td>0.050161</td>\n",
              "      <td>-0.539621</td>\n",
              "      <td>0.003016</td>\n",
              "      <td>-0.289835</td>\n",
              "      <td>0.492322</td>\n",
              "      <td>0.356809</td>\n",
              "      <td>-1.528315</td>\n",
              "      <td>-0.244226</td>\n",
              "      <td>-0.796396</td>\n",
              "      <td>-0.094822</td>\n",
              "      <td>-0.306418</td>\n",
              "      <td>0.193533</td>\n",
              "      <td>-0.684156</td>\n",
              "      <td>-0.279959</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2515 rows × 18 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8051bb49-ac00-45f4-bbdb-e79229d58655')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8051bb49-ac00-45f4-bbdb-e79229d58655 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8051bb49-ac00-45f4-bbdb-e79229d58655');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-aa88bd67-02cd-4439-9f1c-a631d7638e7d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-aa88bd67-02cd-4439-9f1c-a631d7638e7d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-aa88bd67-02cd-4439-9f1c-a631d7638e7d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(pd\",\n  \"rows\": 2515,\n  \"fields\": [\n    {\n      \"column\": \"ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2515,\n        \"samples\": [\n          \"R1126_293\",\n          \"R1136_2\",\n          \"R1136_17\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"resname\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"C\",\n          \"U\",\n          \"G\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"resid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 176,\n        \"min\": 1,\n        \"max\": 720,\n        \"num_unique_values\": 720,\n        \"samples\": [\n          341,\n          291,\n          55\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"x_1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.36568467159028667,\n        \"min\": -1.633288,\n        \"max\": 0.94902277,\n        \"num_unique_values\": 2515,\n        \"samples\": [\n          -0.9444882,\n          -0.05452956,\n          -0.5074978\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"y_1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.32674364394495836,\n        \"min\": -1.4702685,\n        \"max\": 0.9301795,\n        \"num_unique_values\": 2515,\n        \"samples\": [\n          -0.77861184,\n          -0.028642822,\n          -0.5356211\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"z_1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.359385476002467,\n        \"min\": -1.6000599,\n        \"max\": 0.75258815,\n        \"num_unique_values\": 2515,\n        \"samples\": [\n          0.5558762,\n          -0.02956701,\n          0.36386514\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"x_2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3511448468495372,\n        \"min\": -1.5673759,\n        \"max\": 0.9902104,\n        \"num_unique_values\": 2515,\n        \"samples\": [\n          -0.2799399,\n          -0.4906032,\n          0.46385393\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"y_2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3388593367137291,\n        \"min\": -1.5000172,\n        \"max\": 0.91476303,\n        \"num_unique_values\": 2515,\n        \"samples\": [\n          -0.0068503506,\n          -0.21296473,\n          0.13319777\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"z_2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3673023514057299,\n        \"min\": -1.684245,\n        \"max\": 0.8760778,\n        \"num_unique_values\": 2515,\n        \"samples\": [\n          -0.17857346,\n          -0.17060629,\n          -0.896661\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"x_3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.36782250080887974,\n        \"min\": -1.528828,\n        \"max\": 1.0449444,\n        \"num_unique_values\": 2515,\n        \"samples\": [\n          -0.73433197,\n          -0.7535171,\n          -0.17767781\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"y_3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.34135257120174095,\n        \"min\": -1.528315,\n        \"max\": 1.005121,\n        \"num_unique_values\": 2515,\n        \"samples\": [\n          0.03565054,\n          -0.61465573,\n          -0.060690355\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"z_3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.36292536476775744,\n        \"min\": -1.512563,\n        \"max\": 1.1449089,\n        \"num_unique_values\": 2515,\n        \"samples\": [\n          -1.1495109,\n          0.21803784,\n          -1.0305521\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"x_4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.34958295198699335,\n        \"min\": -1.3173977,\n        \"max\": 1.1369503,\n        \"num_unique_values\": 2515,\n        \"samples\": [\n          -0.1868991,\n          -0.12823628,\n          -0.46558386\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"y_4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3330968905484383,\n        \"min\": -1.5040565,\n        \"max\": 0.94512075,\n        \"num_unique_values\": 2514,\n        \"samples\": [\n          0.0965305,\n          -0.43437594,\n          -0.07363506\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"z_4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.36313260803977865,\n        \"min\": -1.6877078,\n        \"max\": 0.8669022,\n        \"num_unique_values\": 2515,\n        \"samples\": [\n          -0.06665061,\n          -0.17227274,\n          -0.2667337\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"x_5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3545098914959331,\n        \"min\": -1.4101677,\n        \"max\": 0.9984736,\n        \"num_unique_values\": 2515,\n        \"samples\": [\n          -0.35705903,\n          -0.7116029,\n          -0.095827654\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"y_5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3315112424026777,\n        \"min\": -1.3795812,\n        \"max\": 0.97687644,\n        \"num_unique_values\": 2515,\n        \"samples\": [\n          -0.3371794,\n          -0.2546139,\n          -0.17248239\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"z_5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3627717099255085,\n        \"min\": -1.6094112,\n        \"max\": 0.9379546,\n        \"num_unique_values\": 2515,\n        \"samples\": [\n          -0.33117285,\n          -0.45381883,\n          -0.26494703\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}